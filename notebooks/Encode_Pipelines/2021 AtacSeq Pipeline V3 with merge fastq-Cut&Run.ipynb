{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n",
    "\n",
    "\n",
    "# Atac runs Luisa 2021 Jan\n",
    "set_url = '/search/?experiments_in_set.experiment_type.display_title=ATAC-seq&experimentset_type=replicate&lab.display_title=Karen+Adelman%2C+HARVARD&status=pre-release&type=ExperimentSetReplicate'\n",
    "\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "\n",
    "run_sets = [i for i in all_sets if \"ENCODE_ATAC_Pipeline_1.1.1\"  not in i.get('completed_processes', [])]\n",
    "print(len(all_sets), 'total number of sets')\n",
    "print(len(all_sets)-len(run_sets), 'sets completed')\n",
    "print(len(run_sets), 'ready for processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_wfr = True\n",
    "add_pc = True\n",
    "add_tag = True\n",
    "pick_best = False\n",
    "\n",
    "for a_set in run_sets: \n",
    "    print()\n",
    "    print(a_set['accession'], end = \" \")\n",
    "    counter += 1\n",
    "    # some feature to extract from each set\n",
    "    control = \"\"  # True or False (True if set in scope is control)\n",
    "    control_set = \"\"  # None (if no control exp is set), or the control experiment for the one in scope\n",
    "    target_type = \"\" # Histone or TF (or None for control)\n",
    "    paired = \"\" # single or paired\n",
    "    organism = \"\"\n",
    "    \n",
    "    # pass attributions to new objects\n",
    "    attributions = None\n",
    "    \n",
    "    replicate_exps = a_set['replicate_exps']\n",
    "    replicate_exps = sorted(replicate_exps, key = lambda x: [x['bio_rep_no'], x['tec_rep_no']])\n",
    "\n",
    "    # get organism, target and control from the first replicate\n",
    "    f_exp = replicate_exps[0]['replicate_exp']['uuid']\n",
    "    f_exp_resp = ff_utils.get_metadata(f_exp, key = my_auth)\n",
    "    biosample = f_exp_resp['biosample']\n",
    "    organism = list(set([bs['individual']['organism']['name'] for bs in biosample['biosource']]))[0]\n",
    "    print(organism)\n",
    "    \n",
    "    if organism not in ['mouse', 'human']:\n",
    "        print('orgamism not ready', organism)\n",
    "        continue\n",
    "\n",
    "    attributions = get_attribution(ff_utils.get_metadata(f_exp_resp['files'][0]['uuid'], key = my_auth))\n",
    "\n",
    "    ta = []\n",
    "    # check for step1, and start if missing\n",
    "    step1_status = 'Done'\n",
    "    for an_exp in replicate_exps:\n",
    "        # are step 1 files raw or processed\n",
    "        my_source = ''\n",
    "        exp_id = an_exp['replicate_exp']['accession']\n",
    "        exp_resp = ff_utils.get_metadata(exp_id, my_auth)\n",
    "        run_name = exp_resp['accession']\n",
    "        exp_files, exp_obj, paired = get_chip_files(exp_resp, my_auth)\n",
    "        print(run_name, len(exp_files), paired, end = ' status: ')\n",
    "        \n",
    "        \n",
    "        # if too many input, merge them in step0\n",
    "        if len(exp_files) > 2:\n",
    "            my_source = 'processed'\n",
    "            merge_tag = True\n",
    "            # exp_files format [[pair1,pair2], [pair1, pair2]]  uuids\n",
    "            # exp_obj format  [[pair1,pair2], [pair1, pair2]]  accession.fileformat ('4DNFIKW8IQT2.fastq.gz')\n",
    "            input_list = []\n",
    "            if paired == 'paired':\n",
    "                # first add paired end 1s\n",
    "                input_list.append([i[0] for i in exp_files])\n",
    "                input_list.append([i[1] for i in exp_files])\n",
    "            elif paired == 'single':\n",
    "                input_list.append([i[0] for i in exp_files])      \n",
    "            merged_files = []\n",
    "            step0_status = 'complete'\n",
    "            iit = 0\n",
    "            for merge_case in input_list:\n",
    "                iit += 1\n",
    "                all_step0s = []\n",
    "                for an_input in merge_case:\n",
    "                    step0_result = get_wfr_out(an_input, 'merge-fastq', my_auth, ['v1'])\n",
    "                    all_step0s.append((step0_result['status'], step0_result.get('fastq')))\n",
    "                if len(list(set(all_step0s))) != 1:\n",
    "                    print('inconsistent step0 run for input fastq files')\n",
    "                    # this run will be repeated if add_wfr\n",
    "                    step0_result['status'] = 'inconsistent run'\n",
    "                #check if part 0 is run already, it not start the run\n",
    "                # if successful\n",
    "                if step0_result['status'] == 'complete':\n",
    "                    merged_fastq = step0_result['fastq']\n",
    "                    merged_files.append(merged_fastq)\n",
    "                # if still running\n",
    "                elif step0_result['status'] == 'running':\n",
    "                    step0_status = 'running'\n",
    "                # if run is not successful\n",
    "                else:\n",
    "                    step0_status = 'missing'\n",
    "                    if run_wfr:\n",
    "                        # RUN PART 0\n",
    "                        print('\\nstarting step0')\n",
    "                        inp_f = {'input_fastqs':merge_case}\n",
    "                        tag = exp_id + '_p' + str(iit) \n",
    "                        run_missing_wfr(step_settings('merge-fastq', organism, attributions),\n",
    "                                        inp_f, tag, my_auth, my_env)\n",
    "\n",
    "            if step0_status != 'complete':\n",
    "                print('step0', step0_status)\n",
    "                step1_status = 'not ready'\n",
    "                continue\n",
    "            \n",
    "            # if completed let's catch up with the variables used in step 1\n",
    "            # update exp_files and exp_obj\n",
    "            exp_files = [[]]\n",
    "            exp_obj = [[]]\n",
    "            for a_merged in merged_files:\n",
    "                temp_resp = ff_utils.get_metadata(a_merged, my_auth)\n",
    "                exp_files[0].append(temp_resp['uuid'])\n",
    "                exp_obj[0].append(temp_resp['display_title']) \n",
    "\n",
    "        step1_result = get_wfr_out_file(exp_files[0][0], 'encode-atacseq-aln', my_auth, ['1.1.1']) \n",
    "        print(step1_result['status'])\n",
    "        if step1_result['status'] == 'complete':\n",
    "            ta.append(step1_result['atac.first_ta'])\n",
    "            if add_pc:\n",
    "                add_preliminary_processed_files(exp_id, [step1_result['atac.first_ta']], my_auth, run_type = 'atac')\n",
    "        elif step1_result['status'] == 'running':\n",
    "            step1_status = 'Incomplete'\n",
    "        else:\n",
    "            step1_status = 'Incomplete'\n",
    "            if len(exp_files) > 2:\n",
    "                print('WARNING More then 2 seq reps in exp')\n",
    "            if run_wfr:\n",
    "                print('starting run')\n",
    "                run_missing_atac1(step_settings('encode-atacseq-aln', organism, attributions), \n",
    "                                  organism, paired, [exp_files], [exp_obj], my_env, my_auth, run_name,\n",
    "                                  source=my_source)\n",
    "\n",
    "\n",
    "    if step1_status != 'Done':\n",
    "        continue\n",
    "    # if there are controls, each experiment should have its own single control experiment\n",
    "    \n",
    "    # if there are more then 2 experiments, check the number of biological replicates\n",
    "\n",
    "    # if there is 1 Biological Replicate\n",
    "    # -pick best 2 exp\n",
    "\n",
    "    # if there are 2 Biological replicates \n",
    "    #  - run mergebed on bioreps with more then 1 technical replicate\n",
    "\n",
    "    # if there are 3 Biological replicates\n",
    "    # - if there are 3 total experiments (1 in each biological rep), pick best 2\n",
    "    # - else, run mergebed on bioreps with more then 1 technical replicate, and pick best 2 biorep\n",
    "\n",
    "    # if there are 4 or more Biolofical replicates\n",
    "    # - run mergebed on bioreps with more then 1 technical replicate, and pick best 2 biorep\n",
    "    \n",
    "    if len(ta) > 2:\n",
    "        print('more then 2')\n",
    "        \n",
    "        # if hardcoded pick best 2\n",
    "        if pick_best:\n",
    "            print('ExperimentSet has bioreps, selecting best 2')\n",
    "            ta_2 = select_best_2(ta, my_auth)\n",
    "            print(ta_2)\n",
    "            ta = ta_2\n",
    "            step2_result = get_wfr_out_file(ta[0], 'encode-atacseq-postaln', my_auth, ['1.1.1'])\n",
    "        else:\n",
    "            # merge bed stuff will come here\n",
    "            step1_5_result = get_wfr_out_file(ta[0], 'mergebed', my_auth, ['v1'])\n",
    "            if step1_5_result['status'] != 'complete':\n",
    "                print('mergebed missing')\n",
    "                continue\n",
    "            # prevent step2 running for this cases TEMP\n",
    "            ta = None\n",
    "\n",
    "            mb = step1_5_result['merged_bed']\n",
    "            step2_result = get_wfr_out_file(mb, 'encode-atacseq-postaln', my_auth, ['1.1.1'])\n",
    "        \n",
    "    else:\n",
    "        step2_result = get_wfr_out_file(ta[0], 'encode-atacseq-postaln', my_auth, ['1.1.1'])\n",
    "        \n",
    "        \n",
    "    if step2_result['status'] == 'complete':\n",
    "        print('step2 is completed')\n",
    "        if add_pc:\n",
    "            add_preliminary_processed_files(a_set['accession'], \n",
    "                                            [\n",
    "                                                step2_result['atac.optimal_peak'],\n",
    "                                                step2_result['atac.conservative_peak'],\n",
    "                                                step2_result['atac.sig_fc']\n",
    "                                            ], \n",
    "                                            my_auth, run_type = 'atac')\n",
    "        if add_tag:\n",
    "            ff_utils.patch_metadata({\"completed_processes\":[\"ENCODE_ATAC_Pipeline_1.1.1\"]}, obj_id=a_set['accession'] , key=my_auth)   \n",
    "    \n",
    "    elif step2_result['status'] == 'running':\n",
    "        print('step2 is still running')\n",
    "    else:\n",
    "        step2_status = 'Incomplete' \n",
    "        if run_wfr:\n",
    "            print('starting run')\n",
    "            run_missing_atac2(step_settings('encode-atacseq-postaln', organism, attributions), \n",
    "                              organism, paired, ta, my_env, my_auth, a_set['accession'])\n",
    "        else:\n",
    "            print('missing step2')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "move_title = 'ENCODE ATAC-Seq Pipeline - Preliminary Files'\n",
    "\n",
    "# set_url = ''\n",
    "# all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "for i in all_sets:\n",
    "    print(i['uuid'])\n",
    "ready_sets_1 = [i for i in all_sets if \"ENCODE_ATAC_Pipeline_1.1.1\" in i.get('completed_processes', [])]\n",
    "\n",
    "print(len(ready_sets_1))\n",
    "\n",
    "ready_sets_2 = []\n",
    "for a_set in ready_sets_1:\n",
    "    if a_set.get('other_processed_files'):\n",
    "        print(a_set['uuid'])\n",
    "        if move_title in [i['title'] for i in a_set['other_processed_files']]:\n",
    "            if a_set.get('processed_files'):\n",
    "                print('WARN' ,a_set['accession'], 'has items in processed files, skipping ')\n",
    "                continue\n",
    "            else:\n",
    "                ready_sets_2.append(a_set)\n",
    "print(len(ready_sets_2), 'items are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move other processed files to processed files field\n",
    "action = True\n",
    "move_title = 'ENCODE ATAC-Seq Pipeline - Preliminary Files'\n",
    "\n",
    "def move_opc_to_pc(resp, move_title, con_key, status):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        if opc:\n",
    "            print('There are files in processed_files field, expected empty', resp['accession'])\n",
    "            return False\n",
    "        else:\n",
    "            print('it is possible that move already happened, no opc but pc', resp['accession'])\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action:\n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                release_files(resp['uuid'], new_pc, con_key, status = status)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "\n",
    "print(len(ready_sets_2), 'experiment sets in scope')\n",
    "for a_set in ready_sets_2:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_auth, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_auth, 'released')\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_auth, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_auth, 'released')\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "wrangle36",
   "language": "python",
   "name": "wrangle36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
