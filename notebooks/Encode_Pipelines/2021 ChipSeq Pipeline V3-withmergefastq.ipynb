{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n",
    "\n",
    "\n",
    "# set from Luisa Jan 2021\n",
    "set_url = '/search/?experiments_in_set.experiment_type.display_title=ChIP-seq&experimentset_type=replicate&lab.display_title=Karen+Adelman%2C+HARVARD&status=pre-release&type=ExperimentSetReplicate'\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "\n",
    "run_sets = [i for i in all_sets if \"ENCODE_ChIP_Pipeline_1.1.1\"  not in i.get('completed_processes', [])]\n",
    "print(len(all_sets), 'total number of sets')\n",
    "print(len(all_sets)-len(run_sets), 'sets completed')\n",
    "print(len(run_sets), 'ready for processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### need to add bucket choice for step1 if fastqs merged\n",
    "### control and exp might be different buckets\n",
    "run_wfr = True\n",
    "add_pc = False\n",
    "add_tag = False\n",
    "\n",
    "for a_set in run_sets: \n",
    "    print()\n",
    "    print(a_set['accession'], end = \" \")\n",
    "    counter += 1\n",
    "    # some feature to extract from each set\n",
    "    control = \"\"  # True or False (True if set in scope is control)\n",
    "    control_set = \"\"  # None (if no control exp is set), or the control experiment for the one in scope\n",
    "    target_type = \"\" # Histone or TF (or None for control)\n",
    "    paired = \"\" # single or paired\n",
    "    organism = \"\"\n",
    "    \n",
    "    # pass attributions to new objects\n",
    "    attributions = None\n",
    "    \n",
    "    replicate_exps = a_set['replicate_exps']\n",
    "    replicate_exps = sorted(replicate_exps, key = lambda x: [x['bio_rep_no'], x['tec_rep_no']])\n",
    "\n",
    "    # get organism, target and control from the first replicate\n",
    "    f_exp = replicate_exps[0]['replicate_exp']['uuid']\n",
    "    f_exp_resp = ff_utils.get_metadata(f_exp, key = my_auth)\n",
    "    control, control_set, target_type, organism = get_chip_info(f_exp_resp, my_auth)\n",
    "           \n",
    "    print('ORG:', organism, \"CONT:\", control, \"TARGET:\", target_type, \"CONT_SET:\", control_set)\n",
    "    if control and target_type:\n",
    "        print('SHOULD NOT BE CONTROL')\n",
    "        continue\n",
    "\n",
    "    if organism not in ['mouse', 'human']:\n",
    "        print('orgamism not ready', organism)\n",
    "        continue\n",
    "\n",
    "    attributions = get_attribution(ff_utils.get_metadata(f_exp_resp['files'][0]['uuid'], key = my_auth))\n",
    "    \n",
    "    if not control:\n",
    "        if not target_type:\n",
    "            print('set is not control, but missing target type, skipping')\n",
    "            continue\n",
    "\n",
    "    ta = []\n",
    "    taxcor = []\n",
    "    ta_cnt = []\n",
    "    \n",
    "    # check for step1, and start if missing\n",
    "    step1_status = 'Done'\n",
    "    control_ready = True\n",
    "\n",
    "    for an_exp in replicate_exps:\n",
    "        my_source = ''\n",
    "        exp_id = an_exp['replicate_exp']['accession']\n",
    "        exp_resp = ff_utils.get_metadata(exp_id, my_auth)\n",
    "        \n",
    "        run_name = exp_resp['accession']\n",
    "        exp_files, exp_obj, paired = get_chip_files(exp_resp, my_auth)\n",
    "        print(run_name, len(exp_files), paired, end = ' status: ')\n",
    "        \n",
    "        \n",
    "        # if too many input, merge them\n",
    "        if len(exp_files) > 2:\n",
    "            print('need to implement bucket choice ')\n",
    "            continue\n",
    "            my_source = 'processed'\n",
    "            attributions = get_attribution(ff_utils.get_metadata(exp_files[0][0], key = my_auth))\n",
    "            # exp_files format [[pair1,pair2], [pair1, pair2]]  uuids\n",
    "            # exp_obj format  [[pair1,pair2], [pair1, pair2]]  accession.fileformat ('4DNFIKW8IQT2.fastq.gz')\n",
    "            input_list = []\n",
    "            if paired == 'paired':\n",
    "                # first add paired end 1s\n",
    "                input_list.append([i[0] for i in exp_files])\n",
    "                input_list.append([i[1] for i in exp_files])\n",
    "            \n",
    "            elif paired == 'single':\n",
    "                input_list.append([i[0] for i in exp_files])\n",
    "                     \n",
    "            merged_files = []\n",
    "            step0_status = 'complete'\n",
    "            iit = 0\n",
    "            for merge_case in input_list:\n",
    "                iit += 1\n",
    "                all_step0s = []\n",
    "                for an_input in merge_case:\n",
    "                    step0_result = get_wfr_out(an_input, 'merge-fastq', my_auth, ['v1'])\n",
    "                    all_step0s.append((step0_result['status'], step0_result.get('fastq')))\n",
    "                if len(list(set(all_step0s))) != 1:\n",
    "                    print('inconsistent step0 run for input fastq files')\n",
    "                    # this run will be repeated if add_wfr\n",
    "                    step0_result['status'] = 'inconsistent run'\n",
    "                #check if part 0 is run already, it not start the run\n",
    "                # if successful\n",
    "                if step0_result['status'] == 'complete':\n",
    "                    merged_fastq = step0_result['fastq']\n",
    "                    merged_files.append(merged_fastq)\n",
    "                # if still running\n",
    "                elif step0_result['status'] == 'running':\n",
    "                    step0_status = 'running'\n",
    "                # if run is not successful\n",
    "                else:\n",
    "                    step0_status = 'missing'\n",
    "                    if run_wfr:\n",
    "                        # RUN PART 0\n",
    "                        print('\\nstarting step0')\n",
    "                        inp_f = {'input_fastqs':merge_case}\n",
    "                        tag = exp_id + '_p' + str(iit) \n",
    "                        run_missing_wfr(step_settings('merge-fastq', organism, attributions),\n",
    "                                        inp_f, tag, my_auth, my_env)\n",
    "\n",
    "            if step0_status != 'complete':\n",
    "                print('step0', step0_status)\n",
    "                step1_status = 'not ready'\n",
    "                continue\n",
    "            \n",
    "            # update exp_files and exp_obj\n",
    "            exp_files = [[]]\n",
    "            exp_obj = [[]]\n",
    "            for a_merged in merged_files:\n",
    "                temp_resp = ff_utils.get_metadata(a_merged, my_auth)\n",
    "                exp_files[0].append(temp_resp['uuid'])\n",
    "                exp_obj[0].append(temp_resp['display_title'])\n",
    "   \n",
    "        if control:\n",
    "            step1_result = get_wfr_out_file(exp_files[0][0], 'encode-chipseq-aln-ctl', my_auth, ['1.1.1'], run=10000)\n",
    "            print('step1', step1_result['status'])\n",
    "            if step1_result['status'] == 'complete':\n",
    "                if add_pc:\n",
    "                    add_preliminary_processed_files(exp_id, [step1_result['chip.first_ta_ctl']], my_auth, run_type = 'chip')\n",
    "            elif step1_result['status'] == 'running':\n",
    "                step1_status = 'Incomplete'\n",
    "            else:\n",
    "                step1_status = 'Incomplete'                \n",
    "                if run_wfr:\n",
    "                    print('starting run')\n",
    "                    run_missing_chip1(control, step_settings('encode-chipseq-aln-ctl', organism, attributions), \n",
    "                                      organism, 'tf', paired, [exp_files], [exp_obj], my_env, my_auth, run_name)\n",
    "                # print(adana)\n",
    "        else:\n",
    "            # check for the status on each experiment\n",
    "            step1_result = get_wfr_out_file(exp_files[0][0], 'encode-chipseq-aln-chip', my_auth, ['1.1.1']) \n",
    "            print('step1', step1_result['status'])\n",
    "            if step1_result['status'] == 'complete':\n",
    "                ta.append(step1_result['chip.first_ta'])\n",
    "                taxcor.append(step1_result['chip.first_ta_xcor'])\n",
    "                if add_pc:\n",
    "                    add_preliminary_processed_files(exp_id, [step1_result['chip.first_ta']], my_auth, run_type = 'chip')\n",
    "            elif step1_result['status'] == 'running':\n",
    "                step1_status = 'Incomplete'\n",
    "            else:\n",
    "                step1_status = 'Incomplete'\n",
    "                if run_wfr:\n",
    "                    print('starting run')\n",
    "                    run_missing_chip1(control, step_settings('encode-chipseq-aln-chip', organism, attributions), \n",
    "                                      organism, target_type, paired, [exp_files], [exp_obj], my_env, my_auth, run_name)\n",
    "                   \n",
    "            if control_set:\n",
    "                try:\n",
    "                    exp_cnt_ids = [i['experiment'] for i in exp_resp['experiment_relation'] if i['relationship_type'] == 'controlled by']\n",
    "                    exp_cnt_ids = [i['@id'].split('/')[2] for i in exp_cnt_ids]\n",
    "                except:\n",
    "                    control_ready = False\n",
    "                    print('Control Relation has problems for this exp', exp_resp['accession'])\n",
    "                    continue\n",
    "                if len(exp_cnt_ids)!= 1:\n",
    "                    control_ready = False\n",
    "                    print('Multiple controls for this exp', exp_resp['accession'])\n",
    "                    continue\n",
    "                    \n",
    "                exp_cnt_id = exp_cnt_ids[0]\n",
    "                print('controled by set', exp_cnt_id)\n",
    "                exp_cnt_resp = ff_utils.get_metadata(exp_cnt_id, my_auth)\n",
    "                cont_file = ''\n",
    "                # check opf for control file               \n",
    "                for opf_case in exp_cnt_resp.get('other_processed_files', []):\n",
    "                    if opf_case['title'] == 'ENCODE ChIP-Seq Pipeline - Preliminary Files':\n",
    "                        opf_files = opf_case['files']\n",
    "                        assert len(opf_files) == 1\n",
    "                        cont_file = opf_files[0]['@id']\n",
    "                # if not in opf, check processed files\n",
    "                if not cont_file:\n",
    "                    pf_list = exp_cnt_resp.get('processed_files', [])\n",
    "                    if pf_list:\n",
    "                        if pf_list:\n",
    "                            assert len(pf_list) == 1\n",
    "                            cont_file = pf_list[0]['@id']\n",
    "\n",
    "                \n",
    "                if cont_file:\n",
    "                    ta_cnt.append(cont_file)\n",
    "                else:\n",
    "                    control_ready = False\n",
    "    print('control set 2', control_set)\n",
    "    if step1_status != 'Done':\n",
    "        print('Step1 not complete, skip step2:', step1_status) \n",
    "        continue\n",
    "        \n",
    "    # add pc files and tag for control\n",
    "    if control:\n",
    "        #add competed flag to set\n",
    "        if add_tag:\n",
    "            ff_utils.patch_metadata({\"completed_processes\":[\"ENCODE_ChIP_Pipeline_1.1.1\"]}, obj_id=a_set['accession'] , key=my_auth)\n",
    "        # add processed files to set\n",
    "        continue\n",
    "\n",
    "    if not control_ready:\n",
    "        print('Control not ready for second step')\n",
    "        continue\n",
    "    \n",
    "    print('set ready for part2')\n",
    "    print('ta length', len(ta))\n",
    "    if len(ta) > 3:\n",
    "        print('skipping more then 3 exp set')\n",
    "        continue\n",
    "\n",
    "    if len(ta) > 2:     \n",
    "        # only covers 3 technical or 3 biological replicates\n",
    "        ta_2 = []\n",
    "        taxcor_2 = []        \n",
    "        print('ExperimentSet has 3 experiments, selecting best 2')\n",
    "        ta_2 = select_best_2(ta, my_auth)\n",
    "        # xcor does not have qc, use ta indexes to find the correct files\n",
    "        for ta_f in ta_2:\n",
    "            taxcor_2.append(taxcor[ta.index(ta_f)])\n",
    "        ta = ta_2\n",
    "        taxcor = taxcor_2\n",
    "        ta_cnt = select_best_2(ta_cnt, my_auth)\n",
    "\n",
    "    if control_set:  \n",
    "        if len(ta_cnt) != len(ta):\n",
    "            print('Control and experiment have diferent number of bio reps, skipping')\n",
    "            continue\n",
    "    \n",
    "    step2_result = get_wfr_out_file(ta[0], 'encode-chipseq-postaln', my_auth, ['1.1.1'])\n",
    "    print(ta)\n",
    "    print(ta_cnt)\n",
    "    print(step2_result)\n",
    "    if step2_result['status'] == 'complete':\n",
    "        print('step2 is complete')\n",
    "        print('https://data.4dnucleome.org/' + step2_result['chip.sig_fc'])\n",
    "        if add_pc:\n",
    "            add_preliminary_processed_files(a_set['accession'], \n",
    "                                            [\n",
    "                                                step2_result['chip.optimal_peak'],\n",
    "                                                step2_result['chip.conservative_peak'],\n",
    "                                                step2_result['chip.sig_fc']\n",
    "                                            ], \n",
    "                                            my_auth, run_type = 'chip')\n",
    "        if add_tag:\n",
    "            ff_utils.patch_metadata({\"completed_processes\":[\"ENCODE_ChIP_Pipeline_1.1.1\"]}, obj_id=a_set['accession'] , key=my_auth)   \n",
    "    elif step2_result['status'] == 'running':\n",
    "        print('step2 is still running')\n",
    "    else:\n",
    "        step2_status = 'Incomplete'  \n",
    "        print('missing step2')\n",
    "        if run_wfr:\n",
    "            print('starting run')\n",
    "            run_ids = {'run_name': a_set['accession'],\n",
    "                       \"desc\" : a_set.get('description', ''),\n",
    "                      }\n",
    "            run_missing_chip2(control_set, step_settings('encode-chipseq-postaln', organism, attributions), \n",
    "                              organism, target_type, paired, ta, taxcor, ta_cnt, my_env, my_auth, run_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "action = False\n",
    "move_title = 'ENCODE ChIP-Seq Pipeline - Preliminary Files'\n",
    "\n",
    "# set_url = '/search/?lab.display_title=Gerd+Blobel%2C+CHOP&type=ExperimentSetReplicate&processed_files.uuid=No+value'\n",
    "# all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "print(len(all_sets))\n",
    "ready_sets_1 = [i for i in all_sets if \"ENCODE_ChIP_Pipeline_1.1.1\" in i.get('completed_processes', [])]\n",
    "print(len(ready_sets_1))\n",
    "ready_sets_2 = []\n",
    "for a_set in ready_sets_1:\n",
    "    if a_set.get('other_processed_files'):\n",
    "        print(a_set['accession'])\n",
    "        if move_title in [i['title'] for i in a_set['other_processed_files']]:\n",
    "            if a_set.get('processed_files'):\n",
    "                print('WARN' ,a_set['accession'], 'has items in processed files, skipping ')\n",
    "                continue\n",
    "            else:\n",
    "                ready_sets_2.append(a_set)\n",
    "print(len(ready_sets_2), 'items are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move other processed files to processed files field\n",
    "action = True\n",
    "def move_opc_to_pc(resp, move_title, con_key):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        if opc:\n",
    "            print('There are files in processed_files field, expected empty', resp['accession'])\n",
    "            return False\n",
    "        else:\n",
    "            print('it is possible that move already happened, no opc but pc', resp['accession'])\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action:\n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                # release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "move_title = 'ENCODE ChIP-Seq Pipeline - Preliminary Files'\n",
    "\n",
    "print(len(ready_sets_1), 'experiment sets in scope')\n",
    "for a_set in ready_sets_1:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_auth, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_auth)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_auth, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_auth)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable wfr for control files\n",
    "q = '/search/?type=FileProcessed&workflow_run_outputs.workflow.title=Chip-seq+data+processing+pipeline%2C+Align-only%2C+Control+1.1.1&disable_wfr_inputs!=true'\n",
    "control_files = ff_utils.search_metadata(q, my_auth)\n",
    "print(len(control_files))\n",
    "# for con_f in control_files:\n",
    "#     ff_utils.patch_metadata({'disable_wfr_inputs': True}, con_f['uuid'], my_auth)\n",
    "#     print('+', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "wrangle36",
   "language": "python",
   "name": "wrangle36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
