{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "* If you would like to contribute to this notebook, make changes on it in useful_notebooks folder, run \"Restart and Clear Output\" before commit.\n",
    "\n",
    "\n",
    "#### THIS SCRIPT CAN:\n",
    "1. Move experiments from an old set to a new set - see Part I section for caveats\n",
    "2. Archive any processed files and associated workflow_runs and QCs associated with the old sets\n",
    "3. CHANGE THE STATUS OF THE OLD SET TO replaced, AND ASSOCIATE IT WITH THE NEW ONE FOR REDIRECT AND ADD STATIC SECTIONS ABOUT REPLACEMENTS TO BOTH THE OLD AND NEW SETS.\n",
    "\n",
    "Each part can be run independently though once the sets are replaced experiments cannot be moved.\n",
    "\n",
    "**NOTE**: If there is a publication attached to the old set, remember to attach it manually to the new one. This script does NOT do that.\n",
    "\n",
    "**NOTE**: there is an 'action' = True/False for each part - set to True to perform updates otherwise will be dry run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "\n",
    "#Given 2 sets, one old with old biological/technical replicates\n",
    "#               second one with new biological/techinal replicates with continuing numbers (no overlap with previous one)\n",
    "# This cell will add old ones to the new one, and also if there are any processed files on the previous set, \n",
    "# files and immediate connections (wfrs, qcs, output files) will be archived\n",
    "\n",
    "# To prevent confusion, new set is in Accession format, and old set is in uuid format\n",
    "\n",
    "\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def conv_time(time_info):\n",
    "    \"\"\"Convert date_created date_modified to datetime object for time operations\"\"\"\n",
    "    time_info, zone_info = time_info.split('+')\n",
    "    assert zone_info == '00:00'\n",
    "    try:\n",
    "        time_info = datetime.strptime(time_info, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    except ValueError:  # items created at the perfect second\n",
    "        time_info = datetime.strptime(time_info, '%Y-%m-%dT%H:%M:%S')\n",
    "    return time_info\n",
    "\n",
    "def fetch_pf_associated(pf_id, my_key):\n",
    "    \"\"\"Given a file accession, find all related items\n",
    "    1) QCs\n",
    "    2) wfr producing the file, and other outputs from the same wfr\n",
    "    3) wfrs this file went as input, and all files/wfrs/qcs around it\n",
    "    The returned list might contain duplicates, uuids and display titles for qcs\"\"\"\n",
    "    file_as_list = []\n",
    "    pf_info = ff_utils.get_metadata(pf_id, my_key)\n",
    "    file_as_list.append(pf_info['uuid'])\n",
    "    if pf_info.get('quality_metric'):\n",
    "        file_as_list.append(pf_info['quality_metric']['uuid'])\n",
    "    inp_wfrs = pf_info.get('workflow_run_inputs')\n",
    "    out_wfr = pf_info.get('workflow_run_outputs')[0]\n",
    "    for inp_wfr in inp_wfrs:\n",
    "        file_as_list.extend(fetch_wfr_associated(inp_wfr['uuid'], my_key))\n",
    "    file_as_list.extend(fetch_wfr_associated(out_wfr['uuid'], my_key))\n",
    "    return list(set(file_as_list))\n",
    "        \n",
    "                \n",
    "def fetch_wfr_associated(wfr_uuid, my_key):\n",
    "    \"\"\"Given wfr_uuid, find associated output files and qcs\"\"\"\n",
    "    wfr_as_list = []\n",
    "    wfr_info = ff_utils.get_metadata(wfr_uuid, my_key)\n",
    "    wfr_as_list.append(wfr_info['uuid'])\n",
    "    if wfr_info.get('output_files'):\n",
    "        for o in wfr_info['output_files']:\n",
    "                if o.get('value'):\n",
    "                    wfr_as_list.append(o['value']['uuid'])\n",
    "                elif o.get('value_qc'):\n",
    "                    wfr_as_list.append(o['value_qc']) # this is a @id\n",
    "    if wfr_info.get('output_quality_metrics'):\n",
    "        for qc in wfr_info['output_quality_metrics']:\n",
    "            if qc.get('value'):\n",
    "                wfr_as_list.append(qc['value']['uuid'])\n",
    "    if wfr_info.get('quality_metric'):\n",
    "        wfr_as_list.append(wfr_info['quality_metric']['uuid'])\n",
    "    return wfr_as_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs \n",
    "* To prevent confusion, **new set** is *Accession*, and **old set** is *uuid*\n",
    "* also sets up connection info\n",
    "* gets es metadata for old and new set that can be (re)used in each step independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new set accession; set to be replaced uuid\n",
    "setpairs = [\n",
    "    ['4DNESWST3UBHXXX', 'f6a9adc8-ce9c-4095-889d-e25ee8b73e16']\n",
    "]\n",
    "\n",
    "my_key = get_key('andrea_data')\n",
    "\n",
    "# initial es_metadata fetching to create a list of pairs used as input to each step\n",
    "workon = []\n",
    "uid_list = [sp[1] for sp in setpairs] # grab uuids for old guys\n",
    "acc2uid = {}\n",
    "for sp in setpairs:\n",
    "    acc = sp[0]\n",
    "    try:\n",
    "        uid = ff_utils.get_metadata(acc, my_key).get('uuid')\n",
    "    except AttributeError:\n",
    "        print(\"Can't get uuid for {} - removing {} from uuid list and skipping pair\".format(acc, sp[1]))\n",
    "        uid_list.remove(sp[1])\n",
    "    else:\n",
    "        uid_list.append(uid)\n",
    "        acc2uid[acc] = uid\n",
    "        \n",
    "es_res = ff_utils.get_es_metadata(uid_list, key=my_key, is_generator=True)\n",
    "es_meta = {}\n",
    "for es in es_res:\n",
    "    es_meta[es.get('uuid')] = es\n",
    "\n",
    "for new, old in setpairs:\n",
    "    nuid = acc2uid[new]\n",
    "    workon.append([es_meta[nuid], es_meta[old]])\n",
    "\n",
    "print('HAVE {} PAIRS OF SETS TO WORKON'.format(len(workon)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - adding experiments and associated metadata from the old set to the new set\n",
    "#### NOTE: this part can be skipped if all the new links have been set up already\n",
    "##### dependencies \n",
    "* old sets are **not** already status=replaced\n",
    "* experiments being transferred to the new set do not share replicate numbers with those already in the new set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = False\n",
    "for new_set, old_set in workon:\n",
    "    # here we need want the uuids so get properties from es metadata\n",
    "    new_set_info = new_set.get('properties')\n",
    "    old_set_info = old_set.get('properties')\n",
    "    if old_set_info['status'] == 'replaced':\n",
    "        print('old set already replaced, skipping')\n",
    "        continue\n",
    "    print('Combining {} into {}'.format(old_set_info['accession'], new_set_info['accession']))\n",
    "    # assert new one is older the old one\n",
    "    assert conv_time(old_set_info['date_created']) < conv_time(new_set_info['date_created'])\n",
    "    # combine rep exps\n",
    "    new_rep = new_set_info['replicate_exps'] + old_set_info['replicate_exps']\n",
    "    new_rep = sorted(new_rep, key=lambda k: [k['bio_rep_no'],k['tec_rep_no']])\n",
    "    # assert unique bio tec reps\n",
    "    tec_bio = [str(i['bio_rep_no'])+'_'+str(i['tec_rep_no']) for i in new_rep]\n",
    "    try:\n",
    "        assert len(new_rep) == len(list(set(tec_bio)))\n",
    "    except AssertionError:\n",
    "        print('same rep numbers are used, either merged already happened, or conflicting numbers in both sets, skipping')\n",
    "        continue\n",
    "    ans = input('Continue with this rep numbers formatted b_t (y/n):\\n{}\\n'.format(tec_bio))\n",
    "    if ans != 'y':\n",
    "        break\n",
    "    # patch the new set with the new rep info\n",
    "    if action:\n",
    "        ff_utils.patch_metadata({'replicate_exps': new_rep}, new_set_info.get('accession'), my_key)\n",
    "        print(new_set_info.get('accession'), ' replicates are updated')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - determine if there are processed_files, other_processed_files, workflow_runs and QCs linked (directly or indirectly) to the old set and if so archive them\n",
    "#### Note: this can be skipped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = False\n",
    "# are there processed files/ other processed files and wfrs/qcs that need to be archived\n",
    "# will collect items on processed_files and other_processed_files fields, and their asociated items\n",
    "# (only 1 level of wfrs)\n",
    "for new_set_info, old_set_info in workon: \n",
    "    pre_set_url = \"https://data.4dnucleome.org/experiment-set-replicates/\"\n",
    "    archive_files_info = 'This Processed File has been archived because it belongs to an archived ' \\\n",
    "    'Experiment Set ([{0}]({1}{2}/)), which has been replaced by [{3}]({1}{3}/). '.format(\n",
    "        old_set_info['accession']\n",
    "        pre_set_url\n",
    "        old_set_info['uuid']\n",
    "        new_set_info['accession']\n",
    "    )\n",
    "    archive_files = []\n",
    "    if old_set_info.get('other_processed_files'):\n",
    "        for case in old_set_info['other_processed_files']:\n",
    "            archive_files.extend(case['files'])  # add all files to archive_list\n",
    "            case['type'] = 'archived'\n",
    "        ### Patch opf items type to archived\n",
    "        if action:\n",
    "            ff_utils.patch_metadata({'other_processed_files': old_set_info['other_processed_files']},\n",
    "                                    old_set_info.get('accession'),\n",
    "                                    my_key)\n",
    "    if old_set_info.get('processed_files'):\n",
    "        archive_files.extend(old_set_info['processed_files'])\n",
    "    archive_list = []\n",
    "    for ar_file in archive_files:\n",
    "        archive_list.extend(fetch_pf_associated(ar_file, my_key))\n",
    "    print(len(archive_list), 'associated items will be archived')\n",
    "    for an_item in archive_list:\n",
    "        if action:\n",
    "            item_info = ff_utils.get_metadata(an_item, my_key)\n",
    "            old_description = item_info.get('description', '')\n",
    "            new_description = archive_files_info + old_description\n",
    "            new_description = new_description.strip()\n",
    "            if not new_description.endswith('.'):\n",
    "                new_description += '.'\n",
    "            item_status = item_info['status']\n",
    "            if status == 'released':\n",
    "                ff_utils.patch_metadata({'status': 'archived', 'description': new_description}, an_item, my_key)\n",
    "            elif status == 'released to project':\n",
    "                ff_utils.patch_metadata({'status': 'archived to project', 'description': new_description}, an_item, my_key)\n",
    "            else:\n",
    "                print(an_item, 'is not released yet, consider deleting, not processed')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Create static sections, change statuses and add alternate accessions\n",
    "* Create and add static sections to old and new sets to indicate why replacement happened. This works also in case of recursive replacements (a new set replacing an old set, replacing an old set, ...).\n",
    "* Set status of old set to replaced.\n",
    "* Add the alternate accession from old to new set to set up the redirect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will perform patches/posts if set to true\n",
    "action = False\n",
    "\n",
    "# reason for replacement\n",
    "reason = 'new biological replicates were added'\n",
    "\n",
    "for new_set, old_set in workon:\n",
    "    # here we need the embedded data\n",
    "    new_set_info = new_set.get('embedded')\n",
    "    old_set_info = old_set.get('embedded')\n",
    "    old_acc = old_set_info['accession']\n",
    "    new_acc = new_set_info['accession']\n",
    "    old_status = old_set_info['status']\n",
    "    new_status = new_set_info['status']\n",
    "\n",
    "    if old_status in ['in review by lab', 'pre-release']:\n",
    "        old_status = 'draft'\n",
    "    if new_status in ['in review by lab', 'pre-release']:\n",
    "        new_status = 'draft'\n",
    "\n",
    "    # prepare the old set header\n",
    "    old_alias = \"static_header:replaced_item_{}_by_{}\".format(old_acc, new_acc)\n",
    "    pre_set_url = \"https://data.4dnucleome.org/experiment-set-replicates/\"\n",
    "    old_body_message = \"This experiment set was replaced by [{0}]({1}{0}/) because {2}.\".format(\n",
    "        new_acc, \n",
    "        pre_set_url,\n",
    "        reason\n",
    "    )\n",
    "    old_header = {\n",
    "        \"body\": old_body_message,\n",
    "        \"award\": old_set_info['award']['uuid'],\n",
    "        \"lab\": old_set_info['lab']['uuid'], \n",
    "        \"name\": \"static-header.replaced_item_{}\".format(old_acc),\n",
    "        \"section_type\": \"Item Page Header\",\n",
    "        \"options\": {\"title_icon\": \"info\", \"default_open\": True, \"filetype\": \"md\", \"collapsible\": False},\n",
    "        \"title\": \"Note: Replaced Item - {}\".format(old_acc),\n",
    "        \"status\": old_status,\n",
    "        \"aliases\": [old_alias]\n",
    "    }\n",
    "    \n",
    "    # prepare the new set header\n",
    "    new_alias = \"static_header:replacing_item_{}_old_{}\".format(new_acc, old_acc)\n",
    "    new_body_message = \"This experiment set supersedes [{0}]({1}{2}/)\".format(\n",
    "        old_acc,\n",
    "        pre_set_url,\n",
    "        old_set_info['uuid']        \n",
    "    ) \n",
    "    # Check for old set, determine if it was already replacing any other set.\n",
    "    # If yes, it will create a cascade of redirects, so include the previously replaced\n",
    "    # accession(s) in the static section.\n",
    "#     old_old_acc = []\n",
    "    if old_set_info['alternate_accessions']:\n",
    "        for alt_accession in old_set_info['alternate_accessions']:\n",
    "#             old_old_acc.append(alt_accession)\n",
    "            search_query = 'https://data.4dnucleome.org/search/?q=' + alt_accession + '&type=Item&status=replaced'\n",
    "            alt_uuid = ff_utils.search_metadata(search_query, my_key)[0].get('uuid')\n",
    "            new_body_message = \"{0} and [{1}]({2}{3}/)\".format(\n",
    "                new_body_message,\n",
    "                alt_accession,\n",
    "                pre_set_url,\n",
    "                alt_uuid\n",
    "            )\n",
    "    new_body_message += \" because \" + reason + \".\"\n",
    "    new_header = {\n",
    "      \"body\": new_body_message,\n",
    "      \"award\": new_set_info['award']['uuid'],\n",
    "      \"lab\": new_set_info['lab']['uuid'],\n",
    "      \"name\": \"static-header.replacing_item_{}\".format(new_acc),\n",
    "      \"section_type\": \"Item Page Header\",\n",
    "      \"options\": {\"title_icon\": \"info\", \"default_open\": True, \"filetype\": \"md\", \"collapsible\": False},\n",
    "      \"title\": \"Note: Superseding Item - {}\".format(new_acc),\n",
    "      \"status\": new_status,\n",
    "      \"aliases\": [new_alias]\n",
    "    }\n",
    "    \n",
    "    print('ADDING HEADER TO THE OLD SET')\n",
    "    print(old_header)\n",
    "    print('ADDING HEADER TO THE NEW SET')\n",
    "    print(new_header)\n",
    "\n",
    "    if action:\n",
    "        # post the static sections\n",
    "        try:\n",
    "            old_h_resp = ff_utils.post_metadata(old_header, 'StaticSection', my_key)['@graph'][0]\n",
    "        except:\n",
    "            print('old header already in system')\n",
    "            old_h_resp = ff_utils.get_metadata(old_alias, my_key)\n",
    "\n",
    "        try:\n",
    "            new_h_resp = ff_utils.post_metadata(new_header, 'StaticSection', my_key)['@graph'][0]\n",
    "        except:\n",
    "            print('new header already in system')\n",
    "            new_h_resp = ff_utils.get_metadata(new_alias, my_key) \n",
    "\n",
    "        #see if existing headers\n",
    "        old_header_list = []\n",
    "        new_header_list = []\n",
    "        if old_set_info.get('static_headers'):\n",
    "            old_header_list = [i['uuid'] for i in old_set_info['static_headers']]\n",
    "        if new_set_info.get('static_headers'):\n",
    "            new_header_list = [i['uuid'] for i in new_set_info['static_headers']]\n",
    "        # add new ones to the list\n",
    "        if old_h_resp['uuid'] in old_header_list:\n",
    "            pass\n",
    "        else:\n",
    "            old_header_list.append(old_h_resp['uuid'])\n",
    "        if new_h_resp['uuid'] in new_header_list:\n",
    "            pass\n",
    "        else:\n",
    "            new_header_list.append(new_h_resp['uuid'])\n",
    "\n",
    "        # set the status of old set to replaced\n",
    "        ff_utils.patch_metadata({'status':'replaced', 'static_headers': old_header_list},\n",
    "                                obj_id=old_set_info['uuid'], key=my_key)\n",
    "        # wait for indexing to take place\n",
    "        # you might need to repeat this last piece separately if indexing does not catch up\n",
    "        # new status needs to be indexed for alternate accession to be patched\n",
    "        time.sleep(60)\n",
    "        # set the alternate accession on the new set to the old one\n",
    "        alt_ac = []\n",
    "        if new_set_info.get('alternate_accessions'):\n",
    "            alt_ac = new_set_info['alternate_accessions']\n",
    "        alt_ac.append(old_acc)\n",
    "        ff_utils.patch_metadata({'alternate_accessions':alt_ac, 'static_headers': new_header_list},\n",
    "                                obj_id=new_set_info['uuid'], key=my_key)\n",
    "\n",
    "    print('DONE')\n",
    "    print('CHECK THE OLD SET', 'https://data.4dnucleome.org/experiment-set-replicates/{}/'.format(old_set_info['uuid']))\n",
    "    print('CHECK THE NEW SET', 'https://data.4dnucleome.org/experiment-set-replicates/{}/'.format(new_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
