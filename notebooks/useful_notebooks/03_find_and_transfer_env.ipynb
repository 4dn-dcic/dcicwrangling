{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON https://data.4dnucleome.org \n",
      "\n",
      "dict_keys(['file_vistrack'])\n",
      "105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "\n",
    "# get key from keypairs.json\n",
    "my_env = 'data'\n",
    "my_key = get_key('koray_data')\n",
    "schema_name = ff_utils.get_schema_names(my_key) \n",
    "print('WORKING ON', my_key['server'], '\\n')\n",
    "\n",
    "##### COLLECT ITEMS TO Release #####\n",
    "# use either a starting item to fetch all linked items\n",
    "\n",
    "# Use a starting item to find linked ones\n",
    "# starting_items = ['46db06ad-b399-4cf4-9acc-07b3e25ef132']\n",
    "#add_items = get_query_or_linked(my_key, linked=starting_items)\n",
    "\n",
    "# or a search query\n",
    "#my_query = '/search/?q=GOLD&type=Item&limit=all'\n",
    "#add_items = get_query_or_linked(my_key, query=my_query)\n",
    "\n",
    "# if you want you can dump them to separate json files (will work as test insert)\n",
    "# dump_to_json(add_items, destination folder)\n",
    "\n",
    "my_query = '/search/?type=FileVistrack'\n",
    "store = get_query_or_linked(my_key, query=my_query, linked_frame='raw')\n",
    "print(store.keys())\n",
    "print(len([i['uuid'] for key in store for i in store[key]]))\n",
    "print()\n",
    "\n",
    "# find_linked = ['48732435-5a16-4d86-a0f6-ace18dc62b6c']\n",
    "# store = get_query_or_linked(my_key, linked=find_linked, linked_frame='raw')\n",
    "# print(store.keys())\n",
    "# print(len([i['uuid'] for key in store for i in store[key]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_vistrack\n",
      "{'lab': '2319a476-9817-4c9d-9ad5-f2858c1aea50', 'last_modified': {'modified_by': '986b362f-4eb6-4a9c-8173-3ab267307e3a', 'date_modified': '2018-11-29T01:27:37.341121+00:00'}, 'project_release': '2018-11-29', 'file_classification': 'visualization', 'genome_assembly': 'GRCh38', 'file_format': 'd1311111-218e-4f61-aaf0-91f226248b2c', 'award': '8842e72e-7b38-4a2a-9557-e6b22616ecaa', 'file_type': 'fold change over control', 'filename': 'ENCFF134SMY.bigWig', 'status': 'released', 'project_lab': 'Richard Myers, HAIB', 'assay_info': 'RXRA', 'public_release': '2018-11-29', 'replicate_identifiers': ['Biorep 1 Techrep 1', 'Biorep 2 Techrep 1'], 'uuid': 'f4b9fbaf-caae-4773-93a8-58600494cee1', 'md5sum': '622678392bcef8909dcb4dd0fa160ccf', 'schema_version': '1', 'content_md5sum': '622678392bcef8909dcb4dd0fa160ccf', 'biosource': '68172441-97c4-40cc-b73f-d0f5dbc5cc05', 'submitted_by': '986b362f-4eb6-4a9c-8173-3ab267307e3a', 'dataset_type': 'ChIP-seq', 'aliases': ['encode-dcc-lab:ENCFF134SMY'], 'accession': '4DNFI3E4H24J', 'dbxrefs': ['ENC:ENCSR000BJW', 'ENC:ENCFF134SMY'], 'file_size': 365761527, 'date_created': '2018-11-20T16:50:43.029519+00:00', 'project_release_date': '2011-07-18', 'description': 'bigWig file of fold change over control for RXRA ChIP-seq on H1-hESC from Richard Myers, HAIB (merged replicates)', 'dataset_description': 'RXRA ChIP-seq protocol v041610.2 on human H1-hESC', 'higlass_uid': 'aC5J5fLhQQ-PPL_LWS7XsA'}\n"
     ]
    }
   ],
   "source": [
    "for i in store:\n",
    "    print(i)\n",
    "    print (store[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_vistrack\n",
      "105 items exist on source\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "exsiting item will be patched in second step\n",
      "105 items transfered to target\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### This part should only run once!\n",
    "\n",
    "transfer_env = 'fourfront-hotseat'\n",
    "transfer_key = ff_utils.get_authentication_with_server({}, ff_env=transfer_env)\n",
    "# reverse lookup dictionary for schema names\n",
    "\n",
    "# if the item exist in the target, should it overwrite it (will include user/award etc)\n",
    "overwrite_existing = True\n",
    "action = False\n",
    "rev_schema_name = {}\n",
    "for key, name in schema_name.items():\n",
    "    rev_schema_name[name] = schema_name[key]\n",
    "\n",
    "my_types = [i for i in ORDER if i in store.keys()]\n",
    "\n",
    "second_round_items = {}\n",
    "\n",
    "# Round I - only put the required - skip if exists already\n",
    "for a_type in my_types:\n",
    "    print(a_type)\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    # find required field\n",
    "    schema_info = ff_utils.get_metadata('/profiles/{}.json'.format(a_type), key=transfer_key)\n",
    "    req_fields = schema_info['required']\n",
    "    ids = schema_info['identifyingProperties']\n",
    "    first_fields = list(set(req_fields+ids))\n",
    "    remove_existing_items = []\n",
    "    counter=0\n",
    "    print(len(store[a_type]), 'items exist on source')\n",
    "    for an_item in store[a_type]:\n",
    "        counter += 1\n",
    "\n",
    "        # does the item exist\n",
    "        exists = False\n",
    "        try:\n",
    "            # TODO check with all identifiers\n",
    "            existing = ff_utils.get_metadata(an_item['uuid'], key=transfer_key)\n",
    "            exists = True\n",
    "        except:\n",
    "            exists = False\n",
    "        \n",
    "        # skip the items that exists\n",
    "        if exists and existing:\n",
    "            if overwrite_existing:\n",
    "                # patch in second step will update the item\n",
    "                print('exsiting item will be patched in second step')\n",
    "                continue\n",
    "            else:\n",
    "                remove_existing_items.append(an_item['uuid'])\n",
    "                print(\"{} {} can not post item\".format(obj_type, an_item['uuid']))\n",
    "                continue\n",
    "        post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "        print('posting')\n",
    "        if action:\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "   \n",
    "    second_round_items[a_type] = [i for i in store[a_type] if i['uuid'] not in remove_existing_items]\n",
    "    print(len(second_round_items[a_type]), 'items transfered to target')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round II - patch the rest of the metadata\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    if not second_round_items[a_type]:\n",
    "        continue \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        counter += 1\n",
    "        ff_utils.patch_metadata(an_item, obj_id = an_item['uuid'], key = transfer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round III - move attachments\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round III - move attachments\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    for an_item in second_round_items[a_type]:\n",
    "        if 'attachment' in an_item.keys():\n",
    "            at_key = an_item['attachment']['blob_id']\n",
    "            copy_source = {'Bucket': source_att, 'Key': at_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_att, at_key)\n",
    "            except:\n",
    "                print('Can not find attachment on source', an_item['uuid'])\n",
    "                continue\n",
    "            print('attachment copied')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round IV - move files\n",
    "for a_type in my_types:\n",
    "    if a_type in ['file_processed', 'file_vistrack']:\n",
    "        source_file_bucket = source_pf\n",
    "        target_file_bucket = target_pf\n",
    "    elif a_type in ['file_reference', 'file_fastq', 'file_microscopy', 'file_fasta', 'file_calibration']:\n",
    "        source_file_bucket = source_raw\n",
    "        target_file_bucket = target_raw\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        # accumulate all keys from a file object to be uploaded\n",
    "        files_to_upload = []\n",
    "        file_resp = ff_utils.get_metadata(an_item['uuid'], key = my_key)\n",
    "        # add extra file keys\n",
    "        if file_resp.get('extra_files', []):\n",
    "            for an_extra_file in file_resp['extra_files']:\n",
    "                files_to_upload.append(an_extra_file['upload_key'])\n",
    "        # add main file key\n",
    "        files_to_upload.append(file_resp['upload_key'])\n",
    "        \n",
    "        for file_key in files_to_upload:\n",
    "            copy_source = {'Bucket': source_file_bucket, 'Key': file_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_file_bucket, file_key)\n",
    "            except:\n",
    "                print('Can not find file on source', file_key)\n",
    "                continue\n",
    "            print('file copied')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
