{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE COPY NOTEBOOK TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "\n",
    "#### This notebook can be used to copy metadata from one environment and post or patch items (if they already exist) into a different environment\n",
    "\n",
    "#### *This notebook uses ES functions so needs to be run in a personal EC2*\n",
    "\n",
    "This cell sets up the auth for the environment that you are retrieving metadata from and from a list of starting items provided via some ID or returned from a search will retrieve all linked items.\n",
    "\n",
    "**Note:** some items types can be excluded by modifying the parameters to `expand_es_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "import time\n",
    "\n",
    "# get auth from keypairs.json\n",
    "my_auth = get_key('andyprod')\n",
    "\n",
    "# or alternatively from env name\n",
    "# transfer_from_env = 'fourfront-production-green'\n",
    "# my_auth = ff_utils.get_authentication_with_server({}, ff_env=transfer_from_env)\n",
    "\n",
    "schema_name = get_schema_names(my_auth) \n",
    "print('WORKING ON', my_auth['server'], '\\n')\n",
    "\n",
    "# use either a list of IDS of starting items or search that retrieves them\n",
    "items_in_scope = ['4DNESSNWXHXK'] # ['4DNACCCC', '4DNACCCCC']\n",
    "\n",
    "#search_url  = '/search/?award.project=4DN&experiments_in_set.experiment_type=dilution+Hi-C&experimentset_type=replicate&lab.display_title=Bing+Ren%2C+UCSD&status=pre-release&type=ExperimentSetReplicate'\n",
    "\n",
    "time1 = time.time()\n",
    "if items_in_scope:\n",
    "    starting_item_uuids = [ff_utils.get_metadata(i, my_auth)['uuid'] for i in items_in_scope]\n",
    "elif search_url:\n",
    "    starting_item_uuids = [i['uuid'] for i in ff_utils.search_metadata(search_url, my_auth)]\n",
    "\n",
    "store={}\n",
    "item_uuids=[]\n",
    "store, item_uuids = ff_utils.expand_es_metadata(starting_item_uuids, my_auth, store_frame='raw',add_pc_wfr=True, ignore_field = [])\n",
    "\n",
    "print(len(store['experiment_set_replicate']), 'exp sets for status change')\n",
    "print(len(item_uuids), 'items collected')\n",
    "time2 = time.time()\n",
    "print(round((time2-time1), 1), 'sec for collection')\n",
    "for itype, found in store.items():\n",
    "    print(\"{}\\t{}\".format(itype, len(found)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### In this section you specify the environment to which you wish to transfer the metadata\n",
    "\n",
    "**NOTE:** here is where you should set parameters:\n",
    "\n",
    "- if you want to actually do the updates in the specified env - `action=True` otherwise dry run\n",
    "- if you want to overwrite existing items via patching then `overwrite_existing=True` otherwise they won't be touched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "transfer_env = 'fourfront-webdev'\n",
    "transfer_key = ff_utils.get_authentication_with_server({}, ff_env=transfer_env)\n",
    "\n",
    "# if the item exist in the target, should it overwrite it (will include user/award etc)\n",
    "overwrite_existing = False\n",
    "action = False  # set True to post/patch in the indicated env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### The following cells should generally only be run once but adjusting parameters can change the behavior so items are patched if they already exist or not over-written - nonetheless care should be taken\n",
    "\n",
    "#### **WARNING - running first with `action=False` is recommended but if a conflict is encountered this will not be picked up by the dry run and you may end up with only partially posted items.\n",
    "\n",
    "#### This does initial posting of items that do not yet exist in the target environment\n",
    "\n",
    "##### NOTE: only required fields are posted for new items so if you stop here you will end up with partially posted items with only a few properties.  \n",
    "\n",
    "### Round I - only post the required fields for new items - skip if exists already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rev_schema_name = {}\n",
    "for key, name in schema_name.items():\n",
    "    rev_schema_name[name] = schema_name[key]\n",
    "\n",
    "my_types = [i for i in ORDER if i in store.keys()]\n",
    "\n",
    "second_round_items = {}\n",
    "\n",
    "for a_type in my_types:\n",
    "    print(a_type)\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    # find required field\n",
    "    schema_info = ff_utils.get_metadata('/profiles/{}.json'.format(a_type), key=transfer_key)\n",
    "    req_fields = schema_info['required']\n",
    "    ids = schema_info['identifyingProperties']\n",
    "    first_fields = list(set(req_fields+ids))\n",
    "    remove_existing_items = []\n",
    "    different_uuids = []\n",
    "    counter=0\n",
    "    print(len(store[a_type]), 'items exist on source')\n",
    "    for idx, an_item in enumerate(store[a_type]):\n",
    "        counter += 1\n",
    "\n",
    "        # does the item exist\n",
    "        exists = False\n",
    "        try:\n",
    "            existing = ff_utils.get_metadata(an_item['uuid'], key=transfer_key)\n",
    "            exists = True\n",
    "        except:\n",
    "            for id2chk in ids:\n",
    "                try:\n",
    "                    existing = ff_utils.get_metadata(id2chk, key=transfer_key)\n",
    "                    exists = True\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    if exists:\n",
    "                        print(\"WARNING! uuid mismatch: {}\\t{}\".format(an_item.get('uuid'), existing.get('uuid')))\n",
    "                        print(\"Existence on {} based on retrieval with {} ID\".format(tranfer_key.get('server'), id2chk))\n",
    "                        break\n",
    "                \n",
    "        # skip the items that exists\n",
    "        if exists and existing:\n",
    "            if overwrite_existing:\n",
    "                if existing.get('uuid') != an_item.get('uuid'):\n",
    "                    print('WARNING - mismatched uuids: will patch {}!'.format(existing.get('uuid')))\n",
    "                    # here is where we need to swap info somehow so right thing can be updated\n",
    "                    # how to transfer the properties or swap uuids?\n",
    "                    remove_existing_items.append(an_item['uuid'])  # first make sure we don't try to post with the other uuid and get a conflict\n",
    "                    an_item['uuid'] = existing.get('uuid')\n",
    "                    print(\"uuid swapped\")\n",
    "                    print(an_item)\n",
    "                    store[a_type][idx] = an_item\n",
    "                                                                              \n",
    "                # patch in second step will update the item\n",
    "                print('existing item will be patched in second step')\n",
    "                continue\n",
    "            else:\n",
    "                remove_existing_items.append(an_item['uuid'])\n",
    "                print(\"{} {} can not post item\".format(obj_type, an_item['uuid']))\n",
    "                continue\n",
    "        post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "        print('posting')\n",
    "        if action:\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "   \n",
    "    second_round_items[a_type] = [i for i in store[a_type] if i['uuid'] not in remove_existing_items]\n",
    "    print(len(second_round_items[a_type]), 'items transfered to target')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round II - patch the rest of the metadata updating pre-existing items if `overwrite_existing=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itype, found in second_round_items.items():\n",
    "    print(\"{}\\t{}\".format(itype, len(found)))\n",
    "counter = 0\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    if not second_round_items[a_type]:\n",
    "        continue \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        counter += 1\n",
    "        if action:\n",
    "            ff_utils.patch_metadata(an_item, obj_id = an_item['uuid'], key = transfer_key)\n",
    "print(\"{} items patched in second round\".format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING!\n",
    "\n",
    "### Nothing below here has been updated or tested so may or may not work!!!\n",
    "\n",
    "### Round III - move attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round III - move attachments\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    for an_item in second_round_items[a_type]:\n",
    "        if 'attachment' in an_item.keys():\n",
    "            at_key = an_item['attachment']['blob_id']\n",
    "            copy_source = {'Bucket': source_att, 'Key': at_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_att, at_key)\n",
    "            except:\n",
    "                print('Can not find attachment on source', an_item['uuid'])\n",
    "                continue\n",
    "            print('attachment copied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - not tested\n",
    "\n",
    "### Round IV - move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "\n",
    "for a_type in my_types:\n",
    "    if a_type in ['file_processed', 'file_vistrack']:\n",
    "        source_file_bucket = source_pf\n",
    "        target_file_bucket = target_pf\n",
    "    elif a_type in ['file_reference', 'file_fastq', 'file_microscopy', 'file_fasta', 'file_calibration']:\n",
    "        source_file_bucket = source_raw\n",
    "        target_file_bucket = target_raw\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        # accumulate all keys from a file object to be uploaded\n",
    "        files_to_upload = []\n",
    "        file_resp = ff_utils.get_metadata(an_item['uuid'], key = my_key)\n",
    "        # add extra file keys\n",
    "        if file_resp.get('extra_files', []):\n",
    "            for an_extra_file in file_resp['extra_files']:\n",
    "                files_to_upload.append(an_extra_file['upload_key'])\n",
    "        # add main file key\n",
    "        files_to_upload.append(file_resp['upload_key'])\n",
    "        \n",
    "        for file_key in files_to_upload:\n",
    "            copy_source = {'Bucket': source_file_bucket, 'Key': file_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_file_bucket, file_key)\n",
    "            except:\n",
    "                print('Can not find file on source', file_key)\n",
    "                continue\n",
    "            print('file copied')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
