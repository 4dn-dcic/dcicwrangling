{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE COPY NOTEBOOK TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "\n",
    "#### This notebook can be used to copy metadata from one environment and post or patch items (if they already exist) into a different environment\n",
    "\n",
    "#### *This notebook uses ES functions so needs to be run in a personal EC2*\n",
    "\n",
    "This cell sets up the auth for the environment that you are retrieving metadata from and from a list of starting items provided via some ID or returned from a search will retrieve all linked items.\n",
    "\n",
    "**Note:** some items types can be excluded by modifying the parameters to `expand_es_metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "import time\n",
    "\n",
    "# options you may want to change\n",
    "overwrite_existing = True # if the item exist in the target, should it overwrite it\n",
    "action = True  # set True to post/patch in the indicated env\n",
    "\n",
    "# auth for source\n",
    "# get auth from keypairs.json\n",
    "src_auth = get_key('andyprod')\n",
    "\n",
    "# or alternatively from env name\n",
    "# source_env = 'fourfront-production-green'\n",
    "# src_auth = ff_utils.get_authentication_with_server({}, ff_env=source_env)\n",
    "\n",
    "# auth for target\n",
    "# get auth from keypairs.json\n",
    "# target_auth = get_key('andywebdev')\n",
    "\n",
    "# or alternatively from env name\n",
    "target_env = 'fourfront-webdev'\n",
    "target_auth = ff_utils.get_authentication_with_server({}, ff_env=target_env)\n",
    "\n",
    "print('TRANSFERRING envs:\\nFROM: {}\\nTO:{}\\n'.format(src_auth['server'], target_auth['server']))\n",
    "\n",
    "schema_name = get_schema_names(src_auth) \n",
    "\n",
    "# use either a list of IDS of starting items or search that retrieves them\n",
    "items_in_scope = ['4DNESSNWXHXK'] # ['4DNACCCC', '4DNACCCCC']\n",
    "\n",
    "#search_url  = '/search/?award.project=4DN&experiments_in_set.experiment_type=dilution+Hi-C&experimentset_type=replicate&lab.display_title=Bing+Ren%2C+UCSD&status=pre-release&type=ExperimentSetReplicate'\n",
    "\n",
    "time1 = time.time()\n",
    "if items_in_scope:\n",
    "    starting_item_uuids = [ff_utils.get_metadata(i, src_auth)['uuid'] for i in items_in_scope]\n",
    "elif search_url:\n",
    "    starting_item_uuids = [i['uuid'] for i in ff_utils.search_metadata(search_url, src_auth)]\n",
    "\n",
    "store={}\n",
    "item_uuids=[]\n",
    "store, item_uuids = ff_utils.expand_es_metadata(starting_item_uuids, src_auth, store_frame='raw',add_pc_wfr=True, ignore_field = [])\n",
    "\n",
    "print(len(starting_item_uuids), 'parent item(s) to transfer')\n",
    "print(len(item_uuids), 'items collected')\n",
    "time2 = time.time()\n",
    "print(round((time2-time1), 1), 'sec for collection')\n",
    "# new_store = {}\n",
    "for itype, found in store.items():\n",
    "    print(\"{}\\t{}\".format(itype, len(found)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Set up some globalish variables\n",
    "#### And helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up some generally global variables\n",
    "rev_schema_name = {}\n",
    "for key, name in schema_name.items():\n",
    "    rev_schema_name[name] = schema_name[key]\n",
    "missing_types = [i for i in store.keys() if i not in ORDER]\n",
    "print(\"MISSING FROM ORDER BUT IN STORE:\")\n",
    "print(missing_types)\n",
    "my_types = [i for i in ORDER if i in store.keys()]\n",
    "second_round_items = {}\n",
    "id_swappers = {}\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def camel_case(name):\n",
    "    return ''.join(x for x in name.title() if not x == '_')\n",
    "\n",
    "def search_for_existing(id_list, item, itype, transfer_auth):\n",
    "    ''' if the uuid is not found tries to find the item by other identifying properties\n",
    "        and if found will return it in raw frame\n",
    "    '''\n",
    "    base_query = 'search/?type={}&{}={}&frame=object'\n",
    "    itype = camel_case(itype)\n",
    "    for id2chk in id_list:\n",
    "        if id2chk == 'uuid':\n",
    "            continue\n",
    "        val2chk = item.get(id2chk)\n",
    "        if not val2chk:\n",
    "            continue\n",
    "        query = base_query.format(itype, id2chk, val2chk)\n",
    "        try:\n",
    "            srch_res = ff_utils.search_metadata(query, transfer_auth)\n",
    "            # print(srch_res)\n",
    "            if len(srch_res) == 1:\n",
    "                return ff_utils.get_metadata(srch_res[0].get('uuid'), transfer_auth, add_on='frame=raw')\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def check_for_swaps(props_to_check, swaps):\n",
    "    ''' replaces uuids that have swapped with the value to use taken from the swaps dict \n",
    "    '''\n",
    "    checked_props = {}\n",
    "    for prop, val in props_to_check.items():\n",
    "        if isinstance(val, list):\n",
    "            ulist = []\n",
    "            for v in val:\n",
    "                if isinstance(v, str):\n",
    "                    if v in swaps:\n",
    "                        v = swaps.get(v)\n",
    "                elif isinstance(v, dict):\n",
    "                    v = check_for_swaps(v, swaps)\n",
    "                else:\n",
    "                    print(\"WARNING - unexpected data structure!\")\n",
    "                ulist.append(v)\n",
    "            checked_props[prop] = ulist\n",
    "        elif isinstance(val, dict):\n",
    "            udict = {}\n",
    "            for k, v in val.items():\n",
    "                if v in swaps:\n",
    "                    v = swaps.get(v)\n",
    "                udict[k] = v\n",
    "            checked_props[prop] = udict\n",
    "        elif val in swaps:\n",
    "            checked_props[prop] = swaps.get(val)\n",
    "        else:\n",
    "            checked_props[prop] = val\n",
    "    return checked_props\n",
    "\n",
    "\n",
    "def check_for_swaps(props_to_check, swaps):\n",
    "    ''' replaces uuids that have swapped with the value to use taken from the swaps dict \n",
    "    '''\n",
    "    checked_props = None\n",
    "    if isinstance(props_to_check, dict):\n",
    "        checked_props = {}\n",
    "        for prop, val in props_to_check.items():\n",
    "            res = check_for_swaps(val, swaps)\n",
    "            checked_props[prop] = res\n",
    "    elif isinstance(props_to_check, list):\n",
    "        checked_props = []\n",
    "        for val in props_to_check:\n",
    "            res = check_for_swaps(val, swaps)\n",
    "            checked_props.append(res)\n",
    "    elif isinstance(props_to_check, str):\n",
    "        checked_props = ''\n",
    "        if props_to_check in swaps:\n",
    "            checked_props = swaps.get(props_to_check)\n",
    "        else:\n",
    "            checked_props = props_to_check\n",
    "    \n",
    "    else:\n",
    "        checked_props = props_to_check\n",
    "    return checked_props\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The following cells should generally only be run once but adjusting parameters can change the behavior so items are patched if they already exist or not over-written - nonetheless care should be taken\n",
    "\n",
    "#### **WARNING - running first with `action=False` is recommended but if a conflict is encountered this will not be picked up by the dry run and you may end up with only partially posted items.\n",
    "\n",
    "#### This does initial posting of items that do not yet exist in the target environment\n",
    "\n",
    "##### NOTE: only required fields are posted for new items so if you stop here you will end up with partially posted items with only a few properties.  \n",
    "\n",
    "### Round I - only post the required fields for new items - skip if exists already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for a_type in my_types:\n",
    "    print(a_type)\n",
    "    # if a_type != 'experiment_type':\n",
    "    #    continue\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    # find required field\n",
    "    schema_info = ff_utils.get_metadata('/profiles/{}.json'.format(a_type), key=target_auth)\n",
    "    req_fields = schema_info['required']\n",
    "    ids = schema_info['identifyingProperties']\n",
    "    first_fields = list(set(req_fields+ids))\n",
    "    remove_existing_items = []\n",
    "    counter=0\n",
    "    print(len(store[a_type]), 'items exist on source')\n",
    "    for idx, an_item in enumerate(store[a_type]):\n",
    "        counter += 1\n",
    "\n",
    "        # does the item exist\n",
    "        exists = False\n",
    "        try:\n",
    "            existing = ff_utils.get_metadata(an_item['uuid'], key=target_auth, add_on='frame=raw')\n",
    "            exists = True\n",
    "        except:\n",
    "            existing = search_for_existing(ids, an_item, a_type, target_auth)\n",
    "            if existing:\n",
    "                exists = True\n",
    "                \n",
    "        \n",
    "        if existing:\n",
    "            if (existing == an_item):\n",
    "                print(\"No updates needed\")\n",
    "                continue\n",
    "                  \n",
    "        # skip the items that exists\n",
    "        if exists and existing:\n",
    "            if overwrite_existing:\n",
    "                if existing.get('uuid') != an_item.get('uuid'):\n",
    "                    print('WARNING - mismatched uuids: will patch {}!'.format(existing.get('uuid')))\n",
    "                    # here is where we need to swap info somehow so right thing can be updated\n",
    "                    # how to transfer the properties or swap uuids and scan items for use of these in linkTos?\n",
    "                    id_swappers[an_item.get('uuid')] = existing.get('uuid')\n",
    "                    remove_existing_items.append(an_item['uuid'])  # first make sure we don't try to post with the other uuid and get a conflict\n",
    "                    an_item['uuid'] = existing.get('uuid')\n",
    "                    print(\"uuid swapped\")\n",
    "                    # print(an_item)\n",
    "                    store[a_type][idx] = an_item\n",
    "                                                                              \n",
    "                # patch in second step will update the item\n",
    "                print('existing item will be patched in second step')\n",
    "                continue\n",
    "            else:\n",
    "                remove_existing_items.append(an_item['uuid'])\n",
    "                print(\"{} {} can not post item\".format(obj_type, an_item['uuid']))\n",
    "                continue\n",
    "        post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "        # for posting of required fields we need to assume ordering is respected so can just scan the post_first fields\n",
    "        # for any swapped uuids\n",
    "        post_first = check_for_swaps(post_first, id_swappers)\n",
    "        print('posting')\n",
    "        # if post_first.get('uuid') == '3ef3bbe0-dcc3-4301-87fb-fd40514866ae':\n",
    "        #    import pdb; pdb.set_trace()\n",
    "        print(post_first)\n",
    "        if action:\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = target_auth)\n",
    "   \n",
    "    second_round_items[a_type] = [i for i in store[a_type] if i['uuid'] not in remove_existing_items]\n",
    "    print(len(second_round_items[a_type]), 'items transfered to target')\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round II - patch the rest of the metadata updating pre-existing items if `overwrite_existing=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itype, found in second_round_items.items():\n",
    "    print(\"{}\\t{}\".format(itype, len(found)))\n",
    "counter = 0\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    if not second_round_items[a_type]:\n",
    "        continue \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        counter += 1\n",
    "        an_item = check_for_swaps(an_item, id_swappers)\n",
    "        if action:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            \n",
    "            res = ff_utils.patch_metadata(an_item, obj_id = an_item['uuid'], key = target_auth)\n",
    "            print(res.get('status'))\n",
    "print(\"{} items patched in second round\".format(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING!\n",
    "\n",
    "### Nothing below here has been updated or tested so may or may not work!!!\n",
    "\n",
    "### Round III - move attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round III - move attachments\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    for an_item in second_round_items[a_type]:\n",
    "        if 'attachment' in an_item.keys():\n",
    "            at_key = an_item['attachment']['blob_id']\n",
    "            copy_source = {'Bucket': source_att, 'Key': at_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_att, at_key)\n",
    "            except:\n",
    "                print('Can not find attachment on source', an_item['uuid'])\n",
    "                continue\n",
    "            print('attachment copied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - not tested\n",
    "\n",
    "### Round IV - move files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "\n",
    "for a_type in my_types:\n",
    "    if a_type in ['file_processed', 'file_vistrack']:\n",
    "        source_file_bucket = source_pf\n",
    "        target_file_bucket = target_pf\n",
    "    elif a_type in ['file_reference', 'file_fastq', 'file_microscopy', 'file_fasta', 'file_calibration']:\n",
    "        source_file_bucket = source_raw\n",
    "        target_file_bucket = target_raw\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        # accumulate all keys from a file object to be uploaded\n",
    "        files_to_upload = []\n",
    "        file_resp = ff_utils.get_metadata(an_item['uuid'], key = my_key)\n",
    "        # add extra file keys\n",
    "        if file_resp.get('extra_files', []):\n",
    "            for an_extra_file in file_resp['extra_files']:\n",
    "                files_to_upload.append(an_extra_file['upload_key'])\n",
    "        # add main file key\n",
    "        files_to_upload.append(file_resp['upload_key'])\n",
    "        \n",
    "        for file_key in files_to_upload:\n",
    "            copy_source = {'Bucket': source_file_bucket, 'Key': file_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_file_bucket, file_key)\n",
    "            except:\n",
    "                print('Can not find file on source', file_key)\n",
    "                continue\n",
    "            print('file copied')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
