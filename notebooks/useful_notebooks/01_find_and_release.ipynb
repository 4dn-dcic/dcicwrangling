{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "* If you would like to contribute to this notebook, make changes on it in useful_notebooks folder, run \"Restart and Clear Output\" before commit.\n",
    "\n",
    "#### Unexpected Expansion Cases\n",
    "###### Experiment set fetches unrealated labs and awards\n",
    "Sometimes a fetch will get some unrelated labs and awards, this is because of the multiple awards a lab can have. This multiple awards are visited, which have users linked to them. This users also have labs, so here you go. Hopefully all are released/current already.\n",
    "\n",
    "###### Experiment set fetches unrealted biosmaple/experiment/set\n",
    "This was so far because of the experiment and biosample relation field, or the references field that links to a publication. If you ignore these fields, if should be fine.\n",
    "`ignore_field = ['experiment_relation', 'biosample_relation', 'references']`\n",
    "\n",
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.cleanup import *\n",
    "import time\n",
    "\n",
    "# status mapping for ordering purposes\n",
    "STATUS_LEVEL = {\n",
    "    # standard_status\n",
    "    \"released\": 10, \"current\": 10, \"restricted\": 10,\n",
    "    \"released to project\": 9,\n",
    "    \"pre-release\": 8,\n",
    "    \"planned\": 6, \"submission in progress\": 6,\n",
    "    \"in review by lab\": 4,\n",
    "    \"revoked\": 0, \"archived\": 0,\"deleted\": 0, \"obsolete\": 0, \"replaced\": 0, \"archived to project\": 0,\n",
    "    # additional file statuses\n",
    "    'to be uploaded by workflow': 4, 'uploading': 4, 'uploaded': 4, 'upload failed': 4, 'draft': 4, 'released to lab': 4}\n",
    "\n",
    "\n",
    "my_auth = get_key('andyprod', keyfile='~/keypairs.json')\n",
    "\n",
    "# Which status to change\n",
    "change_status = 'released'\n",
    "change_level = STATUS_LEVEL.get(change_status, 1)\n",
    "additional_changes = None\n",
    "\n",
    "\n",
    "## helper function to deal with extra_files of file items\n",
    "def check_extra_files(extra_files):\n",
    "    ef_fields = ['href', 'md5sum', 'file_size', 'use_for', 'status']\n",
    "    extras = []\n",
    "    chgcnt = 0\n",
    "    for ef in extra_files:\n",
    "        extra = {pname: ef.get(pname) for pname in ef_fields if pname in ef}\n",
    "        efformat = ef.get('file_format').get('uuid')\n",
    "        extra['file_format'] = efformat\n",
    "        \n",
    "        estatus = ef.get('status')\n",
    "        estatus_level = STATUS_LEVEL.get(estatus)\n",
    "        if estatus_level < 4 or estatus_level >= change_level:\n",
    "            # we don't want to change this extra_file at all\n",
    "            continue\n",
    "        else:\n",
    "            extra['status'] = change_status\n",
    "            chgcnt += 1\n",
    "        extras.append(extra)\n",
    "    \n",
    "    return (chgcnt, extras)  # returning the count of extras needing to be changed along with list of all extras\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Do you want to patch other properties, besides status?\n",
    "Examples: add/remove viewing groups, tags, contributing labs, patch lab/award"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use validate_change() to add each change to the list additional_changes\n",
    "additional_changes = []\n",
    "\n",
    "# add_viewing_group = validate_change(\n",
    "#     key='viewable_by', value=['IWG'], verb='add', level_min=6)\n",
    "# additional_changes.append(add_viewing_group)\n",
    "\n",
    "# remove_viewing_group = validate_change(\n",
    "#     key='viewable_by', value=['IWG'], verb='remove', level_min=9, level_max=10)\n",
    "# additional_changes.append(remove_viewing_group)\n",
    "\n",
    "# tag_experiments = validate_change(\n",
    "#     key='tags', value=['my_tag'], verb='add',  types=['experiment_hi_c'], level_max=10)\n",
    "# additional_changes.append(tag_experiments)\n",
    "\n",
    "# add_contributing_lab = validate_change(\n",
    "#     key='contributing_labs', value=['828cd4fe-ebb0-4b36-a94a-d2e3a36cc989'], verb='add',\n",
    "#     types=['file_fastq', 'file_processed'])\n",
    "# additional_changes.append(add_contributing_lab)\n",
    "\n",
    "# patch_award = validate_change(\n",
    "#     key='award', value='71171a4e-dca1-44cb-8375-fafd896c6923', verb='patch',\n",
    "#     level_min=4, level_max=4)\n",
    "# additional_changes.append(patch_award)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the ExperimentSetReplicate Items to release along with all their linked Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_in_scope = ['4DNES19URNAH'] # ['4DNACCCC', '4DNACCCCC']\n",
    "\n",
    "#search_url  = '/search/?award.project=4DN&experiments_in_set.experiment_type=dilution+Hi-C&experimentset_type=replicate&lab.display_title=Bing+Ren%2C+UCSD&status=pre-release&type=ExperimentSetReplicate'\n",
    "\n",
    "time1 = time.time()\n",
    "if sets_in_scope:\n",
    "    set_to_release = [ff_utils.get_metadata(i, my_auth)['uuid'] for i in sets_in_scope]\n",
    "elif search_url:\n",
    "    set_to_release = [i['uuid'] for i in ff_utils.search_metadata(search_url, my_auth)]\n",
    "\n",
    "store={}\n",
    "item_uuids=[]\n",
    "store, uuids = ff_utils.expand_es_metadata(set_to_release, my_auth, store_frame='embedded',add_pc_wfr=True, ignore_field = ['experiment_relation', 'biosample_relation', 'references', 'experiment_type'])\n",
    "\n",
    "print(len(store['experiment_set_replicate']), 'exp sets for status change')\n",
    "print(len(uuids), 'items collected')\n",
    "time2 = time.time()\n",
    "print(round((time2-time1), 1), 'sec for collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Some QC checks on several of the expected and most important item types\n",
    "\n",
    "### NOTE: if check_wfrs is True QC will be done on workflows - in some cases for may not care to check this when pre-releasing but generally it's a good idea to leave True\n",
    "### When delete_problematic is set to True then problematic workflow_runs will have their status changed to 'deleted' \n",
    "\n",
    "- this cleans up failed duplicate runs and errored runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Check audits\n",
    "\n",
    "# Please Modify the following accordingly \n",
    "# do you want to check for duplicate/problematic runs on files?\n",
    "# it will take some time\n",
    "check_wfrs = True\n",
    "# if any are found do you want to remove them?\n",
    "delete_problematic = False\n",
    "\n",
    "\n",
    "\n",
    "# create stash of wfrs to pass to delete_wfrs\n",
    "stash = store.get('workflow_run_sbg', []) + store.get('workflow_run_awsem', [])\n",
    "\n",
    "\n",
    "# check expsets\n",
    "print('EXPSET CHECK')\n",
    "for a_set in store['experiment_set_replicate']:\n",
    "    if not a_set.get('completed_processes'):\n",
    "        print(a_set['accession'], 'missing processing tag', a_set['description'][:50])\n",
    "\n",
    "# check exps \n",
    "print('\\nEXP CHECK')\n",
    "# check for experiment numbers\n",
    "exp_names = [i for i in store if i.startswith('experiment') and not i.startswith('experiment_set')]\n",
    "all_exps_on_sets = [a for i in store['experiment_set_replicate'] for a in i['experiments_in_set']]\n",
    "all_exps = [a['uuid'] for i in store.keys() for a in store[i] if i in exp_names]\n",
    "if len(all_exps_on_sets) != len(all_exps):\n",
    "    print('Number of experiments is not same as experiments associated with sets')\n",
    "    print('# of exps: {}. # of exps on sets: {}'.format(len(all_exps), len(all_exps_on_sets)))\n",
    "\n",
    "hela_exps = []  # a list of exp['uuid'] with biosource from HeLa individual\n",
    "hela_exps_unsure = []\n",
    "for exp_type in exp_names:\n",
    "    for exp in store[exp_type]:\n",
    "        biosource_is_hela = 0\n",
    "        for bs in exp['biosample']['biosource']:\n",
    "            if bs.get('individual') and bs['individual'].get('display_title') == '4DNINEL8T2GK':  # the HeLa individual\n",
    "                biosource_is_hela += 1\n",
    "        if biosource_is_hela == len(exp['biosample']['biosource']):  # all biosources are HeLa: exp is HeLa\n",
    "            hela_exps.append(exp['uuid'])\n",
    "        elif biosource_is_hela > 0:  # some but not all biosources are HeLa: unsure\n",
    "            hela_exps_unsure.append(exp['uuid'])\n",
    "if hela_exps_unsure and change_level >= 9:\n",
    "    release_hela = input('Experiments with multiple Biosources found, some of which are HeLa. ' +\n",
    "                         'Sequence files associated with these experiments will be restricted. ' +\n",
    "                         'Do you want to release them, instead? (yes/no)')\n",
    "    if release_hela == 'no':\n",
    "        hela_exps.extend(hela_exps_unsure)\n",
    "    elif release_hela != 'yes':\n",
    "        raise ValueError('Invalid response')\n",
    "\n",
    "print('\\nFILE FASTQ CHECK')\n",
    "for a_file in store['file_fastq']:\n",
    "    if not a_file.get('quality_metric'):\n",
    "        print(a_file['accession'], 'missing fastqc')\n",
    "    if not a_file.get('content_md5sum'):\n",
    "        print(a_file['accession'], 'missing content md5 sum')\n",
    "    if not a_file.get('md5sum'):\n",
    "        print(a_file['accession'], 'md5 was not calculated during upload, missing md5sum')\n",
    "    if check_wfrs:\n",
    "        dw = delete_wfrs(a_file, my_auth, delete=delete_problematic, stash=stash)\n",
    "\n",
    "# check processed files\n",
    "print('\\nFILE PROCESSED CHECK')\n",
    "if store.get('file_processed'):\n",
    "    for a_file in store['file_processed']:\n",
    "        # in select cases fastq can be a processed file\n",
    "        if a_file['file_format']['file_format'] == '/file-formats/fastq/':\n",
    "            if not a_file.get('quality_metric'):\n",
    "                print(a_file['accession'], 'missing Fastqc for FastQ processed file')\n",
    "        if a_file['file_format']['file_format'] == '/file-formats/pairs/':\n",
    "            if not a_file.get('quality_metric'):\n",
    "                print(a_file['accession'], 'missing Pairsqc')\n",
    "        if a_file['file_format']['file_format'] == '/file-formats/bam/':\n",
    "            if not a_file.get('quality_metric'):\n",
    "                print(a_file['accession'], 'missing BAMqc')\n",
    "        if a_file['file_format']['file_format'] == '/file-formats/bed/':\n",
    "            if a_file.get('track_and_facet_info').get('experiment_type') in ['ChIP-seq', 'ATAC-seq', 'CUT&RUN']:\n",
    "                if not a_file.get('quality_metric'):\n",
    "                    print(a_file['accession'], 'missing QualityMetric')\n",
    "        if a_file['file_format']['file_format'] == '/file-formats/tsv/':\n",
    "            if a_file.get('track_and_facet_info').get('experiment_type') == 'RNA-seq':\n",
    "                qc_info = a_file.get('quality_metric')\n",
    "                if not qc_info:\n",
    "                    print(a_file['accession'], 'missing RNAseq QualityMetric info')\n",
    "                elif len(qc_info) < 2:\n",
    "                    print(a_file['accession'], 'missing some RNAseq QualityMetric info')\n",
    "                    \n",
    "        if not a_file.get('source_experiments'):\n",
    "            print(a_file['accession'], 'user submitted or produced by sbg runs')\n",
    "        if check_wfrs:\n",
    "            dw = delete_wfrs(a_file, my_auth, delete=delete_problematic, stash=stash)   \n",
    "\n",
    "# check wfrs\n",
    "print('\\nWFR CHECK')\n",
    "# list all wf types found\n",
    "print('  Following run types are found:')\n",
    "for wf in set([i['display_title'].split(' run')[0] for i in store.get('workflow_run_awsem')]):\n",
    "           print('    ' + wf)\n",
    "if store.get('workflow_run_awsem'):\n",
    "    for wfr in store['workflow_run_awsem']:\n",
    "        if wfr['run_status'] != 'complete':\n",
    "            print('problematic wfr', wfr['uuid'], wfr['run_status'])\n",
    "        \n",
    "# check for weird status\n",
    "print('\\nREPORT NUMBERS AND CHECK STATUS')\n",
    "for i in store:\n",
    "    print(i, len(store[i]))\n",
    "    weird = [[i, x['uuid'], x['status']] for x in store[i] if STATUS_LEVEL.get(x['status']) == 0]\n",
    "    if weird:\n",
    "        for case in weird:\n",
    "            print(case)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## This cell checks each item against the requested status change and let's you know what will happen\n",
    "\n",
    "#### print_each = True will provide info on every Item that will be changed - it is useful to check this output to get an idea of what will happen in the next cell but if you are confident and want shorter output you can set this to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check status\n",
    "print_each = True\n",
    "\n",
    "for a_type in store:\n",
    "    total = len(store[a_type])\n",
    "    change = 0\n",
    "    matching = 0\n",
    "    unusual = 0\n",
    "    skipping = 0\n",
    "    for raw_data in store[a_type]:\n",
    "        item_level = STATUS_LEVEL.get(raw_data['status'])\n",
    "        patch_data = {}\n",
    "        if item_level > change_level:\n",
    "            skipping += 1\n",
    "            msg = ('{} {} ITEM HAS STATUS {} HIGHER THAN {} - SKIPPING'.format(a_type, raw_data['uuid'], raw_data['status'], change_status))\n",
    "        elif item_level == change_level:\n",
    "            matching += 1\n",
    "            msg = ('MATCHING ACCESS STATUS', a_type, raw_data['uuid'], raw_data['status'])\n",
    "        elif item_level == 0:\n",
    "            unusual += 1\n",
    "            msg = ('SKIP UNUSUAL STATUS   ', a_type, raw_data['uuid'], raw_data['status'])\n",
    "        else:\n",
    "            change += 1\n",
    "            msg = ('        CHANGE        ', a_type, raw_data['uuid'], raw_data['status'])\n",
    "\n",
    "            # Special case: HeLa sequences (FASTQ and BAM files) are not released but restricted\n",
    "            if change_level >= 9 and hela_exps and a_type in ['file_fastq', 'file_processed'] and raw_data['file_format']['file_format'] in ['fastq', 'bam']:\n",
    "                is_hela = file_in_exp(raw_data, hela_exps)\n",
    "                if is_hela:\n",
    "                    msg = ('       RESTRICT       ', a_type, raw_data['uuid'], raw_data['status'])\n",
    "                elif is_hela is None:\n",
    "                    change -= 1\n",
    "                    skipping += 1\n",
    "                    print('\\nERROR! SKIPPING {} {} Impossible to determine whether is HeLa'.format(a_type, raw_data['uuid']))\n",
    "\n",
    "        if additional_changes:\n",
    "            patch_data = change_additional_fields(patch_data, raw_data, a_type, item_level, change_level, additional_changes)\n",
    "            if patch_data:\n",
    "                msg += tuple(k + ': ' + v for k, v in patch_data.items())\n",
    "\n",
    "        if print_each:\n",
    "            if a_type.startswith('file_'):\n",
    "                # check to see if there are any extra_files and report how many will be patched\n",
    "                if 'extra_files' in raw_data:\n",
    "                    cnt, extras = check_extra_files(raw_data.get('extra_files'))\n",
    "                    msg = msg + ('\\n\\textra_file patch - update {} of {}'.format(cnt, len(extras)),)\n",
    "            print(msg)\n",
    "            \n",
    "    print('{:<25} Out of {t}, {r} skipped, {m} matching, {u} unusual, and {c} needs change'.format(a_type, t=total, r=skipping, m=matching, u=unusual, c=change))\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And finally do the patching if all looks good\n",
    "\n",
    "- don't forget wrangler review - you'll be asked\n",
    "- set action = True to effect the patch otherwise it will just report what will happen\n",
    "- generally print_each=False is fine in this cell but can be changed if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to patch the status, change action to True\n",
    "action = False\n",
    "print_each = False\n",
    "\n",
    "reviewed = \"\"\n",
    "reviewed = input('Did another wrangler review this release? (y/n):')\n",
    "if reviewed != 'y':\n",
    "    raise KeyError('A key step is missing!')\n",
    "\n",
    "for a_type in store:\n",
    "    total = len(store[a_type])\n",
    "    change = 0\n",
    "    matching = 0\n",
    "    unusual = 0\n",
    "    skipping = 0\n",
    "    for raw_data in store[a_type]:\n",
    "        item_level = STATUS_LEVEL.get(raw_data['status'])\n",
    "        patch_data = {}\n",
    "\n",
    "        # Status change\n",
    "        if item_level > change_level:\n",
    "            skipping += 1\n",
    "            msg = ('{} {} ITEM HAS STATUS {} HIGHER THAN {} - SKIPPING'.format(a_type, raw_data['uuid'], raw_data['status'], change_status))\n",
    "        elif item_level == change_level:\n",
    "            matching += 1\n",
    "            msg = ('MATCHING ACCESS STATUS', a_type, raw_data['uuid'], raw_data['status'])\n",
    "        elif item_level == 0:\n",
    "            unusual += 1\n",
    "            msg = ('SKIP UNUSUAL STATUS   ', a_type, raw_data['uuid'], raw_data['status'])\n",
    "        else:\n",
    "            # Normal case\n",
    "            change += 1\n",
    "            msg = ('        CHANGE        ', a_type, raw_data['uuid'], raw_data['status'])\n",
    "            patch_data['status'] = change_status\n",
    "\n",
    "            # Special case: publication\n",
    "            if change_status == 'released' and a_type in ['publication']:\n",
    "                patch_data['status'] = 'current'\n",
    "\n",
    "            # Special case: HeLa sequences (FASTQ and BAM files) are not released but restricted\n",
    "            if change_level >= 9 and hela_exps and a_type in ['file_fastq', 'file_processed'] and raw_data['file_format']['file_format'] in ['fastq', 'bam']:\n",
    "                is_hela = file_in_exp(raw_data, hela_exps)\n",
    "                if is_hela:\n",
    "                    msg = ('       RESTRICT       ', a_type, raw_data['uuid'], raw_data['status'])\n",
    "                    patch_data['status'] = 'restricted'\n",
    "                elif is_hela is None:\n",
    "                    change -= 1\n",
    "                    skipping += 1\n",
    "                    print('\\nERROR! SKIPPING {} {} Impossible to determine whether is HeLa'.format(a_type, raw_data['uuid']))\n",
    "                    continue\n",
    "                    \n",
    "            # Special handling of extra_files\n",
    "            if a_type.startswith('file_'):\n",
    "                # check to see if there are any extra_files and report how many will be patched\n",
    "                if 'extra_files' in raw_data:\n",
    "                    cnt, extras = check_extra_files(raw_data.get('extra_files'))\n",
    "                    if cnt:\n",
    "                        patch_data['extra_files'] = extras\n",
    "\n",
    "        # Additional changes\n",
    "        if additional_changes:\n",
    "            patch_data = change_additional_fields(patch_data, raw_data, a_type, item_level, change_level, additional_changes)\n",
    "            if [k for k in patch_data if k not in ['status', 'extra_files']]:\n",
    "                msg += tuple(k + ': ' + v for k, v in patch_data.items() if k not in ['status', 'extra_files'])\n",
    "\n",
    "        # Do the patch\n",
    "        if patch_data and action:\n",
    "            ff_utils.patch_metadata(patch_data, obj_id=raw_data['uuid'],key=my_auth)\n",
    "\n",
    "        if print_each:\n",
    "            print(msg)\n",
    "    print('{:<25} Out of {t}, {r} skipped, {m} matching, {u} unusual, and {c} UPDATED with status'.format(a_type, t=total, r=skipping, m=matching, u=unusual, c=change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
