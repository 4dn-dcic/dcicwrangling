{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON https://data.4dnucleome.org \n",
      "\n",
      "organism 1\n",
      "award 8\n",
      "target 4\n",
      "biosample_cell_culture 2\n",
      "workflow_run_awsem 44\n",
      "workflow 6\n",
      "static_section 1\n",
      "lab 9\n",
      "file_processed 23\n",
      "file_reference 3\n",
      "experiment_seq 10\n",
      "file_format 7\n",
      "vendor 4\n",
      "experiment_set_replicate 1\n",
      "individual_human 1\n",
      "biosample 2\n",
      "file_fastq 20\n",
      "biosource 1\n",
      "protocol 2\n",
      "antibody 4\n",
      "quality_metric_fastqc 16\n",
      "ontology 2\n",
      "quality_metric_chipseq 11\n",
      "ontology_term 10\n",
      "software 12\n",
      "user 17\n",
      "221\n"
     ]
    }
   ],
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "\n",
    "### This notebook will copy items from given environment to your local build\n",
    "### it expects to have 'local' key in keypairs.json, if not, please handle your keys accordingly\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "\n",
    "# get key from keypairs.json\n",
    "my_env = 'data'\n",
    "my_key = get_key('koray_data')\n",
    "schema_name = get_schema_names(my_key) \n",
    "print('WORKING ON', my_key['server'], '\\n')\n",
    "\n",
    "find_linked = ['11f207be-ebc4-4622-8b42-02e7841d17db']\n",
    "store, uuids = ff_utils.expand_es_metadata(find_linked, key = my_key, add_pc_wfr = True, ignore_field=['references', 'attachments'])\n",
    "\n",
    "for key in store:\n",
    "    print(key, len(store[key]))\n",
    "print(len([i['uuid'] for key in store for i in store[key]]))\n",
    "\n",
    "# replace workflows from inserts\n",
    "insert_f = json.load(open('/Users/koray/Github/fourfront/src/encoded/tests/data/inserts/workflow.json'))\n",
    "wf_uuids = [i['uuid'] for i in store['workflow']]\n",
    "updated_wf = [i for i in insert_f if i['uuid'] in wf_uuids]\n",
    "store['workflow'] = updated_wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "17 items exist on source\n",
      "11 items posted, 6 existing items skipped\n",
      "17 items will be patched in second round\n",
      "\n",
      "award\n",
      "8 items exist on source\n",
      "6 items posted, 2 existing items skipped\n",
      "8 items will be patched in second round\n",
      "\n",
      "lab\n",
      "9 items exist on source\n",
      "8 items posted, 1 existing items skipped\n",
      "9 items will be patched in second round\n",
      "\n",
      "static_section\n",
      "1 items exist on source\n",
      "0 items posted, 1 existing items skipped\n",
      "1 items will be patched in second round\n",
      "\n",
      "ontology\n",
      "2 items exist on source\n",
      "0 items posted, 2 existing items skipped\n",
      "2 items will be patched in second round\n",
      "\n",
      "ontology_term\n",
      "10 items exist on source\n",
      "3 items posted, 7 existing items skipped\n",
      "10 items will be patched in second round\n",
      "\n",
      "file_format\n",
      "7 items exist on source\n",
      "0 items posted, 7 existing items skipped\n",
      "7 items will be patched in second round\n",
      "\n",
      "organism\n",
      "1 items exist on source\n",
      "0 items posted, 1 existing items skipped\n",
      "1 items will be patched in second round\n",
      "\n",
      "target\n",
      "4 items exist on source\n",
      "4 items posted, 0 existing items skipped\n",
      "4 items will be patched in second round\n",
      "\n",
      "vendor\n",
      "4 items exist on source\n",
      "4 items posted, 0 existing items skipped\n",
      "4 items will be patched in second round\n",
      "\n",
      "protocol\n",
      "2 items exist on source\n",
      "2 items posted, 0 existing items skipped\n",
      "2 items will be patched in second round\n",
      "\n",
      "biosample_cell_culture\n",
      "2 items exist on source\n",
      "2 items posted, 0 existing items skipped\n",
      "2 items will be patched in second round\n",
      "\n",
      "individual_human\n",
      "1 items exist on source\n",
      "1 items posted, 0 existing items skipped\n",
      "1 items will be patched in second round\n",
      "\n",
      "biosource\n",
      "1 items exist on source\n",
      "1 items posted, 0 existing items skipped\n",
      "1 items will be patched in second round\n",
      "\n",
      "antibody\n",
      "4 items exist on source\n",
      "4 items posted, 0 existing items skipped\n",
      "4 items will be patched in second round\n",
      "\n",
      "biosample\n",
      "2 items exist on source\n",
      "2 items posted, 0 existing items skipped\n",
      "2 items will be patched in second round\n",
      "\n",
      "quality_metric_fastqc\n",
      "16 items exist on source\n",
      "16 items posted, 0 existing items skipped\n",
      "16 items will be patched in second round\n",
      "\n",
      "quality_metric_chipseq\n",
      "11 items exist on source\n",
      "11 items posted, 0 existing items skipped\n",
      "11 items will be patched in second round\n",
      "\n",
      "file_fastq\n",
      "20 items exist on source\n",
      "20 items posted, 0 existing items skipped\n",
      "20 items will be patched in second round\n",
      "\n",
      "file_processed\n",
      "23 items exist on source\n",
      "23 items posted, 0 existing items skipped\n",
      "23 items will be patched in second round\n",
      "\n",
      "file_reference\n",
      "3 items exist on source\n",
      "3 items posted, 0 existing items skipped\n",
      "3 items will be patched in second round\n",
      "\n",
      "experiment_seq\n",
      "10 items exist on source\n",
      "10 items posted, 0 existing items skipped\n",
      "10 items will be patched in second round\n",
      "\n",
      "experiment_set_replicate\n",
      "1 items exist on source\n",
      "1 items posted, 0 existing items skipped\n",
      "1 items will be patched in second round\n",
      "\n",
      "software\n",
      "12 items exist on source\n",
      "0 items posted, 12 existing items skipped\n",
      "12 items will be patched in second round\n",
      "\n",
      "workflow\n",
      "6 items exist on source\n",
      "0 items posted, 6 existing items skipped\n",
      "6 items will be patched in second round\n",
      "\n",
      "workflow_run_awsem\n",
      "44 items exist on source\n",
      "44 items posted, 0 existing items skipped\n",
      "44 items will be patched in second round\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### This part should only run once!\n",
    "\n",
    "transfer_env = 'local'\n",
    "transfer_key = get_key('local')\n",
    "\n",
    "#### This part should only run once!\n",
    "\n",
    "# if the item exist in the target, should it overwrite it (will include user/award etc)\n",
    "overwrite_existing = True\n",
    "\n",
    "# reverse lookup dictionary for schema names\n",
    "rev_schema_name = {}\n",
    "for key, name in schema_name.items():\n",
    "    rev_schema_name[name] = schema_name[key]\n",
    "\n",
    "my_types = [i for i in ORDER if i in store.keys()]\n",
    "\n",
    "second_round_items = {}\n",
    "\n",
    "# Round I - only put the required - skip if exists already\n",
    "for a_type in my_types:\n",
    "    print(a_type)\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    # find required field\n",
    "    schema_info = ff_utils.get_metadata('/profiles/{}.json'.format(a_type), key=transfer_key)\n",
    "    req_fields = schema_info['required']\n",
    "    ids = schema_info['identifyingProperties']\n",
    "    first_fields = list(set(req_fields+ids))\n",
    "    remove_existing_items = []\n",
    "    \n",
    "    print(len(store[a_type]), 'items exist on source')\n",
    "    posted = 0\n",
    "    skip_exist = 0\n",
    "    for an_item in store[a_type]:\n",
    "        exists = False\n",
    "        try:\n",
    "            # TODO check with all identifiers\n",
    "            existing = ff_utils.get_metadata(an_item['uuid'], key=transfer_key)\n",
    "            exists = True\n",
    "        except:\n",
    "            exists = False\n",
    "        # skip the items that exists, if overwrite is not allowed, they them out from patch list\n",
    "        if exists and existing:\n",
    "            skip_exist += 1\n",
    "            if not overwrite_existing:\n",
    "                remove_existing_items.append(an_item['uuid'])\n",
    "            # print(\"{} {} can not post existing item\".format(obj_type, an_item['uuid']))\n",
    "            continue\n",
    "        posted += 1\n",
    "        post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "        ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "   \n",
    "    second_round_items[a_type] = [i for i in store[a_type] if i['uuid'] not in remove_existing_items]\n",
    "    print(posted, 'items posted,', skip_exist, 'existing items skipped')\n",
    "    print(len(second_round_items[a_type]), 'items will be patched in second round')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user - patched\n",
      "award - patched\n",
      "lab - patched\n",
      "static_section - patched\n",
      "ontology - patched\n",
      "ontology_term - patched\n",
      "file_format - patched\n",
      "organism - patched\n",
      "target - patched\n",
      "vendor - patched\n",
      "protocol - patched\n",
      "biosample_cell_culture - patched\n",
      "individual_human - patched\n",
      "biosource - patched\n",
      "antibody - patched\n",
      "biosample - patched\n",
      "quality_metric_fastqc - patched\n",
      "quality_metric_chipseq - patched\n",
      "file_fastq - patched\n",
      "file_processed - patched\n",
      "file_reference - patched\n",
      "experiment_seq - patched\n",
      "experiment_set_replicate - patched\n",
      "software - patched\n",
      "workflow - patched\n",
      "workflow_run_awsem - patched\n"
     ]
    }
   ],
   "source": [
    "# Round II - patch the rest of the metadata\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    if not second_round_items[a_type]:\n",
    "        print(a_type, '- no items to patch')\n",
    "        continue \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        if a_type == 'file_fastq':\n",
    "            if 'extra_files' in an_item:\n",
    "                del an_item['extra_files']\n",
    "        ff_utils.patch_metadata(an_item, obj_id = an_item['uuid'], key = transfer_key)\n",
    "    print(a_type, '- patched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 experiment sets in scope\n",
      "1 4DNES9WNNK52\n",
      "4DNES9WNNK52 files will move\n",
      "4DNES9WNNK52 moved to pc\n",
      "4DNEX1UOUWPM files will move\n",
      "4DNEX1UOUWPM moved to pc\n",
      "4DNEXD6Z69AC files will move\n",
      "4DNEXD6Z69AC moved to pc\n",
      "\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_key = get_key('local')\n",
    "run_sets = [ff_utils.get_metadata('11f207be-ebc4-4622-8b42-02e7841d17db', my_key)]\n",
    "\n",
    "\n",
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "action = True\n",
    "\n",
    "\n",
    "# move other processed files to processed files field\n",
    "def move_opc_to_pc(resp, move_title, con_key):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        print('There are files in processed_files field, expected empty')\n",
    "        return False\n",
    "    # are there files in opc\n",
    "    if not opc:\n",
    "        print('there are no other processed files, skipping')\n",
    "        return False\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action: \n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "    \n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "#move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "move_title = \"ENCODE ChIP-Seq Pipeline - Preliminary Files\"\n",
    "\n",
    "print(len(run_sets), 'experiment sets in scope')\n",
    "for a_set in run_sets:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_key, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_key)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_key, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_key)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
