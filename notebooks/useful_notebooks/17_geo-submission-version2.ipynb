{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29e1cd-dfd0-4acb-bb65-7db2461b2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GEO submission update (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aceeca6c-ecc6-4156-b74b-0ef4b98bdb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.cleanup import get_workflow_details, delete_wfrs\n",
    "import time\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf65ed2-c0ab-404c-9991-d09785ae3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add key and blank GEO submission template\n",
    "\n",
    "my_auth = get_key('default', keyfile='~/keypairs.json')  #add_key\n",
    "GEO_metadata_template_file = '/Users/rahinavelkar/Library/CloudStorage/OneDrive-HarvardUniversity/harvard/geo_submission_v2/seq_template.xlsx' #https://www.ncbi.nlm.nih.gov/geo/info/seq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3f76e3-8a17-4ea0-88c1-e921ad8249cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples 12\n"
     ]
    }
   ],
   "source": [
    "#Add datasets to be uploaded. All datasets should be from one publication or study\n",
    "\n",
    "sets_list = [] \n",
    "\n",
    "search_url  = '/browse/?type=ExperimentSetReplicate&experimentset_type=replicate&produced_in_pub.display_title=Gholamalamdari+O+et+al.+%282024%29+PMID%3A38712201&sort=experiments_in_set.experiment_type.display_title&sort=dataset_label&sort=condition'\n",
    "\n",
    "if sets_list:\n",
    "    esets = [ff_utils.get_metadata(i, my_auth) for i in sets_list]\n",
    "elif search_url:\n",
    "    esets = [i for i in ff_utils.search_metadata(search_url, my_auth)]\n",
    "    \n",
    "print(\"No. of samples {}\".format(len(esets))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de52e67-4c20-4090-be4c-d3a8819451d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting STUDY SECTION \n",
    "\n",
    "for eset in esets[0:1]:\n",
    "    if eset.get(\"produced_in_pub\") == None:\n",
    "        print(\"No publication present, add manually\")\n",
    "    else:\n",
    "        pub_details = eset.get(\"produced_in_pub\")\n",
    "        pub_title = pub_details.get(\"title\")\n",
    "        summary = pub_details.get(\"abstract\")\n",
    "        authors = pub_details.get(\"authors\")\n",
    "        \n",
    "full_name = []\n",
    "\n",
    "for author in authors:\n",
    "    name = author.split(\" \")\n",
    "    surname = name[0]\n",
    "    if len(name) > 2:\n",
    "        firstname = name[-1]\n",
    "    else:\n",
    "        firstname = name[1]\n",
    "    author_name = firstname + \", \" + surname\n",
    "    full_name.append([author_name])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c2089a4-e559-4e34-93e3-57b343f39920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized counts\n",
      "4DNFIMTKDNJW.bw\n",
      "normalized counts\n",
      "4DNFINZYRIU6.bw\n",
      "normalized counts\n",
      "4DNFIKWPQKXH.bw\n",
      "normalized counts\n",
      "4DNFIX693RNR.bw\n",
      "normalized counts\n",
      "4DNFIWRT1RIZ.bw\n",
      "normalized counts\n",
      "4DNFIKZDFYQ8.bw\n",
      "normalized counts\n",
      "4DNFIPHWZ5B4.bw\n",
      "normalized counts\n",
      "4DNFIMI5G3HW.bw\n",
      "normalized counts\n",
      "4DNFI625PP2A.bw\n",
      "normalized counts\n",
      "4DNFIFKMOD1L.bw\n",
      "normalized counts\n",
      "4DNFIBY8G6RZ.bw\n",
      "normalized counts\n",
      "4DNFI8HIU45G.bw\n",
      "normalized counts\n",
      "4DNFIO4EE1OU.bw\n",
      "normalized counts\n",
      "4DNFI6FTPH5V.bw\n",
      "normalized counts\n",
      "4DNFIVZSO9RI.bw\n",
      "other\n"
     ]
    }
   ],
   "source": [
    "##Extracting SAMPLES details\n",
    "\n",
    "processed_files = [\"normalized counts\"] #add file type\n",
    "supplementary_file_type = [] #add file type\n",
    "molecule = \"genomic DNA\"  #add extracted molecule choices [\"polyA RNA\", \"total RNA\", \"nuclear RNA\", \"cytoplasmic RNA\", \"genomic DNA\", \"protein\", \"other\"]\n",
    "\n",
    "portal_url = \"https://data.4dnucleome.org/\"\n",
    "replicate_desc = {}\n",
    "replicate_desc_short = {}\n",
    "md5sums_raw = {}\n",
    "md5sums_proc = {}\n",
    "paired_raw_files = {}\n",
    "all_raw_files = {}\n",
    "all_proc_files = {}\n",
    "total_supplementary_files = []\n",
    "\n",
    "dataset_labels = []\n",
    "conditions = []\n",
    "organisms = []\n",
    "\n",
    "tissue_name = \"\"\n",
    "cell_line_name = \"\"\n",
    "cell_type = \"\"\n",
    "treatment = \"\"\n",
    "batch = \"\"\n",
    "\n",
    "samples_rows = []\n",
    "\n",
    "for eset in esets:\n",
    "    supplementary_files = eset.get(\"other_processed_files\")\n",
    "    for sup_file_info in supplementary_files:\n",
    "        s_files = sup_file_info.get(\"files\")\n",
    "        for sfile in s_files:\n",
    "            s_file_type = sfile.get(\"file_type\")\n",
    "            if s_file_type in supplementary_file_type:\n",
    "                total_supplementary_files.append(sfile.get('display_title'))\n",
    "    if eset.get(\"dataset_label\") not in dataset_labels:\n",
    "        dataset_label = eset.get(\"dataset_label\")\n",
    "        dataset_labels.append(dataset_label)\n",
    "    condition = eset.get(\"condition\")\n",
    "    genotype = condition\n",
    "    if genotype not in conditions:\n",
    "        conditions.append(genotype)\n",
    "    replicate_info = eset.get(\"replicate_exps\")\n",
    "    for reps in replicate_info:\n",
    "            biorep = reps.get(\"bio_rep_no\")     \n",
    "            techrep = reps.get(\"tec_rep_no\")\n",
    "            rep_description = \"Biological replicate \" +  str(biorep) + \", Technical replicate \" + str(techrep)\n",
    "            rep_description_short = \"B\" +  str(biorep) + \" T\" + str(techrep)\n",
    "            rep_info = reps.get(\"replicate_exp\")\n",
    "            rep_info_acc = rep_info.get(\"accession\")\n",
    "            replicate_desc[rep_info_acc] = rep_description\n",
    "            replicate_desc_short[rep_info_acc] = rep_description_short\n",
    "    exps = eset.get('experiments_in_set')\n",
    "    for exp in exps:\n",
    "        raw_files_per_exp = []\n",
    "        proc_files_per_exp = []\n",
    "        exp_id = exp.get(\"@id\")\n",
    "        exp_url = portal_url + exp_id\n",
    "        exp_acc = exp.get(\"accession\")\n",
    "        library_name = exp.get(\"display_title\")\n",
    "        exp_details = exp.get(\"experiment_type\")\n",
    "        library_strategy = exp_details.get('display_title')\n",
    "        files = exp.get(\"files\")\n",
    "        for file in files:\n",
    "            file_acc = file.get(\"accession\")\n",
    "            file_type = file.get(\"file_type\")\n",
    "            file_name = file.get(\"display_title\")\n",
    "            if file_type == \"reads\":\n",
    "                raw_files_per_exp.append(file.get('display_title'))\n",
    "                file_metadata = ff_utils.get_metadata(file_acc, my_auth)\n",
    "                instrument = file_metadata.get('instrument')\n",
    "                md5sum = file_metadata.get('md5sum')\n",
    "                md5sums_raw[file_name] = md5sum\n",
    "                if file.get(\"paired_end\"):\n",
    "                    paired = \"paired-end\"\n",
    "                    related_files = file.get(\"related_files\")\n",
    "                    for rf in related_files:\n",
    "                        if rf.get(\"relationship_type\") == \"paired with\":\n",
    "                            paired_acc = rf.get(\"accession\")\n",
    "                            paired_raw_files[file_acc] = paired_acc\n",
    "                else:\n",
    "                    paired = \"single\"\n",
    "        if exp.get(\"processed_files\"):\n",
    "            proc_files = exp.get(\"processed_files\")\n",
    "            for pfile in proc_files:\n",
    "                file_acc = pfile.get(\"accession\")\n",
    "                file_type = pfile.get(\"file_type\")\n",
    "                file_name = pfile.get(\"display_title\")\n",
    "                if file_type in processed_files:\n",
    "                    proc_files_per_exp.append(file_name)\n",
    "                    pfile_metadata = ff_utils.get_metadata(file_acc, my_auth)\n",
    "                    pmd5sum = pfile_metadata.get('md5sum')\n",
    "                    md5sums_proc[file_name] = pmd5sum\n",
    "        else:\n",
    "            proc_files_per_exp.append('')\n",
    "        biosample = exp.get(\"biosample\")\n",
    "        biosample_id = biosample.get(\"@id\")\n",
    "        biosample_url = portal_url + biosample_id\n",
    "        biosample_type = biosample.get(\"biosample_type\")\n",
    "        biosources = biosample.get(\"biosource\")\n",
    "        for biosource in biosources:\n",
    "            organism = biosource.get(\"organism\")\n",
    "            organism_name = organism.get(\"display_title\")\n",
    "            if organism_name not in organisms:\n",
    "                organisms.append(organism_name)\n",
    "            biosource_type = biosource.get(\"biosource_type\")\n",
    "            if biosource_type == \"tissue\":\n",
    "                tissue = biosource.get('tissue')\n",
    "                tissue_name = tissue.get('term_name') \n",
    "            else:\n",
    "                cell_type = biosource_type\n",
    "                cell_line = biosource.get(\"cell_line\")\n",
    "                cell_line_name = cell_line.get(\"term_name\")\n",
    "        title =  exp_acc + \", \" + organism_name + \" - \" + condition + \", \" + replicate_desc_short[exp_acc]      \n",
    "        description = library_strategy + \" in \" + organism_name + \", \" + condition + \", \" + replicate_desc[exp_acc] + \", 4DN experiment: \" + exp_url + \" ,4DN Biosample: \" + biosample_url\n",
    "        samples_rows.append([exp_acc,title,library_strategy,organism_name,tissue_name,cell_line_name, cell_type, genotype, treatment, batch, molecule, paired,instrument,description])\n",
    "        all_raw_files[exp_acc] = raw_files_per_exp\n",
    "        all_proc_files[exp_acc] = proc_files_per_exp\n",
    "\n",
    "#Add experimental design        \n",
    "for label in dataset_labels:\n",
    "    genotypes = ', '.join(conditions)\n",
    "    all_organisms = ', '.join(organisms)\n",
    "    experiment_design = '{} in tissues/genotype: {}'.format(label, genotypes)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fe3e83d-f6f8-4fb1-a09d-1aadce202b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahinavelkar/.pyenv/versions/3.8.12/envs/4dn_dcicwrangling/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Populate STUDY and SAMPLES section in template file\n",
    "\n",
    "workbook = openpyxl.load_workbook(GEO_metadata_template_file)\n",
    "sheet = workbook['Metadata']\n",
    "\n",
    "#STUDY section\n",
    "\n",
    "sheet[\"B12\"] = pub_title\n",
    "sheet[\"B13\"] = summary\n",
    "sheet[\"B14\"] = experiment_design\n",
    "\n",
    "#author names\n",
    "if len(full_name) > 7:\n",
    "    sheet.insert_rows(22, amount=len(full_name)-6)\n",
    "    start_row = 39 + len(full_name)-6\n",
    "    \n",
    "    for row in range(22, 22+(len(full_name)-6)):\n",
    "        sheet[f\"A{row}\"] = 'contributor'\n",
    "else:\n",
    "    start_row = 39\n",
    "    \n",
    "for i, row_data in enumerate(full_name, start=15):\n",
    "    for j, value in enumerate(row_data, start=2):\n",
    "        sheet.cell(row=i, column=j, value=value)\n",
    "        \n",
    "#SAMPLES section\n",
    "\n",
    "if len(samples_rows) > 16:\n",
    "    add_no_rows = len(samples_rows) - 16    \n",
    "    sheet.insert_rows(start_row + 16, add_no_rows + 1)\n",
    "        \n",
    "        \n",
    "for i, row_data in enumerate(samples_rows, start=start_row):\n",
    "    for j, value in enumerate(row_data, start=1):\n",
    "        sheet.cell(row=i, column=j, value=value)\n",
    "\n",
    "# SAMPLES section - files metadata        \n",
    "edit_file_headers = False\n",
    "\n",
    "no_of_raw_files = []\n",
    "no_of_processed_files = []\n",
    "raw_files = []\n",
    "proc_files = []\n",
    "        \n",
    "for key, value in all_proc_files.items():\n",
    "    no_of_processed_files.append(len(value))\n",
    "    proc_files.append(value)\n",
    "    \n",
    "for key, value in all_raw_files.items():\n",
    "    no_of_raw_files.append(len(value))\n",
    "    raw_files.append(value)    \n",
    "    \n",
    "max_raw = max(no_of_raw_files)\n",
    "max_proc = max(no_of_processed_files)\n",
    "\n",
    "if max_proc > 2 or max_raw > 5:\n",
    "    print(\"Updating column headers for files\")\n",
    "    edit_file_headers = True\n",
    "\n",
    "if edit_file_headers == True:\n",
    "    file_headers =  []\n",
    "    for i in range(max_proc):\n",
    "        file_headers.append('processed data file')\n",
    "    file_headers.append('*raw file')    \n",
    "    for i in range(max_raw-1):\n",
    "        file_headers.append('raw file')  \n",
    "\n",
    "    start_row = start_row - 1\n",
    "    for col_num, value in enumerate(file_headers, start=15):\n",
    "        sheet.cell(row=file_headers_row, column=col_num, value=value)\n",
    "        \n",
    "    start_row_files = start_row\n",
    "    start_column_proc = 15\n",
    "    \n",
    "    for i, row_data in enumerate(proc_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc):\n",
    "            sheet.cell(row=i, column=j, value=value)\n",
    "    \n",
    "    for i, row_data in enumerate(raw_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc + max_proc):\n",
    "            sheet.cell(row=i, column=j, value=value)   \n",
    "\n",
    "else:\n",
    "    start_row_files = start_row\n",
    "    start_column_proc = 15\n",
    "    \n",
    "    for i, row_data in enumerate(proc_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc):\n",
    "            sheet.cell(row=i, column=j, value=value)\n",
    "    \n",
    "    for i, row_data in enumerate(raw_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc + 2):\n",
    "            sheet.cell(row=i, column=j, value=value)\n",
    "                        \n",
    "\n",
    "workbook.save(GEO_metadata_template_file)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3049c22-231d-486a-994b-202f1b7f2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate paired-end\n",
    "\n",
    "paired_end_start_row = start_row + len(samples_rows) + 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d394cef3-2a2e-48cb-a5f9-44d269faf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate MD5 Checksums sheet.\n",
    "\n",
    "add_md5sum_raw = []\n",
    "add_md5sum_proc = []\n",
    "\n",
    "for filename, md5sum in md5sums_raw.items():\n",
    "    add_md5sum_raw.append([filename,md5sum ])\n",
    "\n",
    "add_md5sum_proc = []\n",
    "\n",
    "for filename, md5sum in md5sums_proc.items():\n",
    "    add_md5sum_proc.append([filename,md5sum ])\n",
    "\n",
    "\n",
    "workbook = openpyxl.load_workbook(GEO_metadata_template_file)\n",
    "sheet = workbook['MD5 Checksums']\n",
    "\n",
    "start_row = 9\n",
    "for i, row_data in enumerate(add_md5sum_raw, start=start_row):\n",
    "    for j, value in enumerate(row_data, start=1):\n",
    "        sheet.cell(row=i, column=j, value=value)\n",
    "        \n",
    "for i, row_data in enumerate(add_md5sum_proc, start=start_row):\n",
    "    for j, value in enumerate(row_data, start=6):\n",
    "        sheet.cell(row=i, column=j, value=value)        \n",
    "        \n",
    "workbook.save(GEO_metadata_template_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da41901-5669-40f2-b514-803bdfe02ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
