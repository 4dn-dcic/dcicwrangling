{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29e1cd-dfd0-4acb-bb65-7db2461b2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GEO submission update (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aceeca6c-ecc6-4156-b74b-0ef4b98bdb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.cleanup import get_workflow_details, delete_wfrs\n",
    "import time\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf65ed2-c0ab-404c-9991-d09785ae3de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add key and blank GEO submission template\n",
    "\n",
    "my_auth = get_key('default', keyfile='~/keypairs.json')  #add_key\n",
    "GEO_metadata_template_file = '' #https://www.ncbi.nlm.nih.gov/geo/info/seq.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac3f76e3-8a17-4ea0-88c1-e921ad8249cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples 1\n"
     ]
    }
   ],
   "source": [
    "#Add datasets to be uploaded. All datasets should be from one publication or study and filter for one organism at a time.\n",
    "\n",
    "sets_list = [] \n",
    "\n",
    "search_url  = ''\n",
    "    esets = [ff_utils.get_metadata(i, my_auth) for i in sets_list]\n",
    "elif search_url:\n",
    "    esets = [i for i in ff_utils.search_metadata(search_url, my_auth)]\n",
    "    \n",
    "print(\"No. of samples {}\".format(len(esets))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de52e67-4c20-4090-be4c-d3a8819451d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No publication present, add manually\n"
     ]
    }
   ],
   "source": [
    "## Extracting STUDY SECTION \n",
    "user_input = []\n",
    "\n",
    "for eset in esets[0:1]:\n",
    "    if eset.get(\"produced_in_pub\") == None:\n",
    "        print(\"No publication present, add manually\")\n",
    "        has_pub = False\n",
    "        user_input.append(\"title\")\n",
    "        user_input.append(\"summary\")\n",
    "        user_input.append(\"author list\")\n",
    "    else:\n",
    "        has_pub = True\n",
    "        pub_details = eset.get(\"produced_in_pub\")\n",
    "        pub_title = pub_details.get(\"title\")\n",
    "        summary = pub_details.get(\"abstract\")\n",
    "        authors = pub_details.get(\"authors\")\n",
    "        full_name = []\n",
    "\n",
    "        for author in authors:\n",
    "            name = author.split(\" \")\n",
    "            surname = name[0]\n",
    "            if len(name) > 2:\n",
    "                firstname = name[-1]\n",
    "            else:\n",
    "                firstname = name[1]\n",
    "            author_name = firstname + \", \" + surname\n",
    "            full_name.append([author_name])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c2089a4-e559-4e34-93e3-57b343f39920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "##Extracting SAMPLES details\n",
    "\n",
    "##User input\n",
    "\n",
    "#add file type\n",
    "processed_files = []\n",
    "supplementary_file_type = [\"gene-cell expression matrix\", \"cell annotation\", \"gene annotation\"]\n",
    "\n",
    "#If you would like to add supplementary files from 4DN as processed files for GEO\n",
    "add_supp_as_proc = True  \n",
    "molecule = \"nuclear RNA\"  #add extracted molecule choices [\"polyA RNA\", \"total RNA\", \"nuclear RNA\", \"cytoplasmic RNA\", \"genomic DNA\", \"protein\", \"other\"]\n",
    "\n",
    "\n",
    "#Don't edit below\n",
    "portal_url = \"https://data.4dnucleome.org/\"\n",
    "replicate_desc = {}\n",
    "replicate_desc_short = {}\n",
    "md5sums_raw = {}\n",
    "md5sums_proc = {}\n",
    "paired_raw_files = {}\n",
    "all_raw_files = {}\n",
    "all_proc_files = {}\n",
    "total_supplementary_files = []\n",
    "proc_file_description = {}\n",
    "\n",
    "dataset_labels = []\n",
    "conditions = []\n",
    "organisms = []\n",
    "all_tissues = []\n",
    "\n",
    "tissue_name = \"\"\n",
    "cell_line_name = \"\"\n",
    "cell_type = \"\"\n",
    "treatment = \"\"\n",
    "batch = \"\"\n",
    "\n",
    "samples_rows = []\n",
    "\n",
    "for eset in esets:\n",
    "    if add_supp_as_proc:\n",
    "                    total_supplementary_files = []\n",
    "    supplementary_files = eset.get(\"other_processed_files\")\n",
    "    for sup_file_info in supplementary_files:\n",
    "        s_files = sup_file_info.get(\"files\")\n",
    "        for sfile in s_files:\n",
    "            s_file_type = sfile.get(\"file_type\")\n",
    "            if s_file_type in supplementary_file_type:\n",
    "                sfile_acc = sfile.get(\"accession\")\n",
    "                sfile_name = sfile.get('display_title')\n",
    "                sfile_md5sum = sfile.get(\"md5sum\")\n",
    "                proc_file_description[s_file_type] = sfile_name.split(\".\", 1)[1]\n",
    "                total_supplementary_files.append(sfile_name)\n",
    "                if add_supp_as_proc:\n",
    "                    md5sums_proc[sfile_name] = sfile_md5sum\n",
    "    if eset.get(\"dataset_label\") not in dataset_labels:\n",
    "        dataset_label = eset.get(\"dataset_label\")\n",
    "        dataset_labels.append(dataset_label)\n",
    "    condition = eset.get(\"condition\")\n",
    "    genotype = condition\n",
    "    if genotype not in conditions:\n",
    "        conditions.append(genotype)\n",
    "    replicate_info = eset.get(\"replicate_exps\")\n",
    "    for reps in replicate_info:\n",
    "            biorep = reps.get(\"bio_rep_no\")     \n",
    "            techrep = reps.get(\"tec_rep_no\")\n",
    "            rep_description = \"Biological replicate \" +  str(biorep) + \", Technical replicate \" + str(techrep)\n",
    "            rep_description_short = \"B\" +  str(biorep) + \" T\" + str(techrep)\n",
    "            rep_info = reps.get(\"replicate_exp\")\n",
    "            rep_info_acc = rep_info.get(\"accession\")\n",
    "            replicate_desc[rep_info_acc] = rep_description\n",
    "            replicate_desc_short[rep_info_acc] = rep_description_short\n",
    "    exps = eset.get('experiments_in_set')\n",
    "    for exp in exps:\n",
    "        raw_files_per_exp = []\n",
    "        proc_files_per_exp = []\n",
    "        exp_id = exp.get(\"@id\")\n",
    "        exp_url = portal_url + exp_id\n",
    "        exp_acc = exp.get(\"accession\")\n",
    "        library_name = exp.get(\"display_title\")\n",
    "        exp_details = exp.get(\"experiment_type\")\n",
    "        library_strategy = exp_details.get('display_title')\n",
    "        files = exp.get(\"files\")\n",
    "        for file in files:\n",
    "            file_acc = file.get(\"accession\")\n",
    "            file_type = file.get(\"file_type\")\n",
    "            file_name = file.get(\"display_title\")\n",
    "            if file_type == \"reads\":\n",
    "                raw_files_per_exp.append(file.get('display_title'))\n",
    "                file_metadata = ff_utils.get_metadata(file_acc, my_auth)\n",
    "                instrument = file_metadata.get('instrument')\n",
    "                md5sum = file_metadata.get('md5sum')\n",
    "                md5sums_raw[file_name] = md5sum\n",
    "                if file.get(\"paired_end\"):\n",
    "                    paired = \"paired-end\"\n",
    "                    related_files = file.get(\"related_files\")\n",
    "                    for rf in related_files:\n",
    "                        if rf.get(\"relationship_type\") == \"paired with\":\n",
    "                            paired_acc = rf.get(\"accession\")\n",
    "                            paired_raw_files[file_acc] = paired_acc\n",
    "                else:\n",
    "                    paired = \"single\"\n",
    "        if exp.get(\"processed_files\"):\n",
    "            proc_files = exp.get(\"processed_files\")\n",
    "            for pfile in proc_files:\n",
    "                file_acc = pfile.get(\"accession\")\n",
    "                file_type = pfile.get(\"file_type\")\n",
    "                file_name = pfile.get(\"display_title\")\n",
    "                if file_type in processed_files:\n",
    "                    proc_files_per_exp.append(file_name)\n",
    "                    pfile_metadata = ff_utils.get_metadata(file_acc, my_auth)\n",
    "                    pmd5sum = pfile_metadata.get('md5sum')\n",
    "                    md5sums_proc[file_name] = pmd5sum\n",
    "        else:    \n",
    "            proc_files_per_exp.append('')\n",
    "        biosample = exp.get(\"biosample\")\n",
    "        biosample_id = biosample.get(\"@id\")\n",
    "        biosample_url = portal_url + biosample_id\n",
    "        biosample_type = biosample.get(\"biosample_type\")\n",
    "        biosources = biosample.get(\"biosource\")\n",
    "        for biosource in biosources:\n",
    "            organism = biosource.get(\"organism\")\n",
    "            organism_name = organism.get(\"display_title\")\n",
    "            organism_uuid = organism.get(\"uuid\")\n",
    "            organism_metadata = ff_utils.get_metadata(organism_uuid, my_auth)\n",
    "            if organism_metadata.get(\"genome_assembly\"):\n",
    "                 organism_genome_assembly = organism_metadata.get(\"genome_assembly\")\n",
    "            else:\n",
    "                 user_input.append(\"organism_genome_assembly\")\n",
    "            if organism_name not in organisms:\n",
    "                organisms.append(organism_name)\n",
    "            biosource_type = biosource.get(\"biosource_type\")\n",
    "            if biosource_type == \"tissue\":\n",
    "                tissue = biosource.get('tissue')\n",
    "                tissue_name = tissue.get('term_name')\n",
    "                all_tissues.append(tissue_name)\n",
    "            else:\n",
    "                cell_type = biosource_type\n",
    "                cell_line = biosource.get(\"cell_line\")\n",
    "                cell_line_name = cell_line.get(\"term_name\")\n",
    "        title =  exp_acc + \", \" + organism_name + \" - \" + condition + \", \" + replicate_desc_short[exp_acc]      \n",
    "        description = library_strategy + \" in \" + organism_name + \", \" + condition + \", \" + replicate_desc[exp_acc] + \", 4DN experiment: \" + exp_url + \" ,4DN Biosample: \" + biosample_url\n",
    "        samples_rows.append([exp_acc,title,library_strategy,organism_name,tissue_name,cell_line_name, cell_type, genotype, treatment, batch, molecule, paired,instrument,description])\n",
    "        all_raw_files[exp_acc] = raw_files_per_exp\n",
    "        if add_supp_as_proc:\n",
    "            all_proc_files[exp_acc] = total_supplementary_files\n",
    "        else:    \n",
    "            all_proc_files[exp_acc] = proc_files_per_exp\n",
    "\n",
    "#Add experimental design        \n",
    "genotypes = ', '.join(conditions)\n",
    "experiment_design = 'Total {} samples generated using {} in {} on {}'.format(len(samples_rows),library_strategy,organism_name, genotypes)\n",
    "\n",
    "#Add processed data files format and content\n",
    "proc_descriptions = []\n",
    "for file_type, ext in proc_file_description.items():\n",
    "    desc = ext + \":\" + file_type\n",
    "    proc_descriptions.append(desc)\n",
    "processed_data_files_format_content  = ', '.join(proc_descriptions)\n",
    "\n",
    "#Warnings                                         \n",
    "if len(all_proc_files) == 0:\n",
    "    print(\"No processed files for any samples - cannot submit to GEO\")\n",
    "if len(organisms) > 1:\n",
    "    print(\"Samples for more than one organisms added, please filter the search query for one organism.\")\n",
    "                                         \n",
    "                                         \n",
    "print(\"done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "218636cf-a4ee-4744-8328-3ee637dde560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add the following manually: title, summary, author list, organism_genome_assembly, organism_genome_assembly\n"
     ]
    }
   ],
   "source": [
    "#mnaullay add missing data\n",
    "\n",
    "missing = ', '.join(user_input)\n",
    "print(\"Add the following manually: {}\".format(missing))\n",
    "\n",
    "if 'organism_genome_assembly' not in user_input:\n",
    "    print(\"Genome assembly added {}\".format(organism_genome_assembly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bad03dc-be9a-47cd-90fa-1768fbebf93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "override_genome_assembly = True\n",
    "\n",
    "if has_pub == False:\n",
    "    pub_title = \"Cross-species imputation and comparison of single-cell transcriptomic profiles\"\n",
    "    summary = \"Cross-species comparison and prediction of gene expression profiles are important to understand regulatory changes during evolution and to transfer knowledge learned from model organisms to humans. Single-cell RNA-seq (scRNA-seq) profiles enable us to capture gene expression profiles with respect to variations among indi- vidual cells; however, cross-species comparison of scRNA-seq profiles is challenging because of data sparsity, batch effects, and the lack of one-to-one cell matching across species. Moreover, single-cell profiles are challenging to obtain in certain biological contexts, limiting the scope of hypothesis generation. Here we developed Icebear, a neural network framework that decomposes single-cell measurements into factors representing cell identity, species, and batch factors. Icebear enables accurate prediction of single-cell gene expression profiles across species, thereby pro- viding high-resolution cell type and disease profiles in under-characterized contexts. Icebear also facilitates direct cross-species comparison of single-cell expression profiles for conserved genes that are located on the X chromosome in eutherian mammals but on autosomes in chicken. This comparison, for the first time, revealed evolutionary and diverse adaptations of X-chromosome upregulation in mammals\"\n",
    "    full_name = [[\"Ran, Zhang\"] ,[\"Mu, Yang\"], [\"Jacob, Schreiber\"], [\"Diana, O’Day\"], [\"James, Turner\"], [\"Jay, Shendure\"], [\"William, Noble\"], [\"Christine, Disteche\"], [\"Xinxian, Deng\"]]\n",
    "\n",
    "if override_genome_assembly:\n",
    "    organism_genome_assembly = \"ASM229v1\"\n",
    "    \n",
    "extract_protocol = \"Adult brain and heart from both male mouse and chicken were purchased from BioChemed Services, and male opossum adult brain was provided by J. Turner (MRC, UK).\"\n",
    "library_construction_protocol = \"The data were collected by indexing cells from each species by reverse transcriptase barcoding and then processing them jointly, in which case the species identity of each cell was known based on the sequence barcode.\"\n",
    "data_processing_step = \"1. For a given sample, creating a multi-species reference genome by concatenating the reference genomes of all the species used in that sample. 2. Mapping all of the reads to the multi-species reference, retaining only reads that map uniquely using the STAR aligner with the following parameters: –out- SAMtype BAM Unsorted –outSAMmultNmax 1 –outSAMstrandField intronMotif –outFilterMultimapNmax 1. 3. Removing PCR duplicates. 4. Eliminating any read that maps to an unassembled scaffold, mitochondrial DNA, orany locus that is marked as a repeat element by RepeatMasker (http://www.repea tmasker.org). The repeat elements by RepeatMasker were retrieved from UCSC genome browser, with the exception of opossum, where we ran RepeatMasker to generate them. Repeat elements were removed using BEDtools. 5. For each cell, counting the total number of remaining reads that map to each of the three species. 6. If the sum of the second- and third-largest counts is greater than 20% of all counts, then mark the cell as a species-doublet and eliminate it. 7. Labelling the remaining cells according to their generating species.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe3e83d-f6f8-4fb1-a09d-1aadce202b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahinavelkar/.pyenv/versions/3.8.12/envs/4dn_dcicwrangling/lib/python3.8/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating column headers for files\n"
     ]
    }
   ],
   "source": [
    "#Populate STUDY, SAMPLES and PROTOCOLS section in template file\n",
    "\n",
    "workbook = openpyxl.load_workbook(GEO_metadata_template_file)\n",
    "sheet = workbook['Metadata']\n",
    "\n",
    "#STUDY section\n",
    "\n",
    "start_row = 39\n",
    "sheet[\"B12\"] = pub_title\n",
    "sheet[\"B13\"] = summary\n",
    "sheet[\"B14\"] = experiment_design\n",
    "\n",
    "#author names\n",
    "if len(full_name) > 7:\n",
    "    sheet.insert_rows(22, amount=len(full_name)-6)\n",
    "    start_row = 39 + len(full_name) - 6\n",
    "\n",
    "    for row in range(22, 22+(len(full_name)-6)):\n",
    "        sheet[f\"A{row}\"] = 'contributor'\n",
    "\n",
    "for i, row_data in enumerate(full_name, start=15):\n",
    "    for j, value in enumerate(row_data, start=2):\n",
    "        sheet.cell(row=i, column=j, value=value)\n",
    "        \n",
    "\n",
    "#SAMPLES section\n",
    "\n",
    "if len(samples_rows) > 16:\n",
    "    add_no_rows = len(samples_rows) - 16    \n",
    "    sheet.insert_rows(start_row + 16, add_no_rows + 1)\n",
    "                \n",
    "for i, row_data in enumerate(samples_rows, start=start_row):\n",
    "    for j, value in enumerate(row_data, start=1):\n",
    "        sheet.cell(row=i, column=j, value=value)\n",
    "\n",
    "# SAMPLES section - files metadata        \n",
    "edit_file_headers = False\n",
    "\n",
    "no_of_raw_files = []\n",
    "no_of_processed_files = []\n",
    "raw_files = []\n",
    "proc_files = []\n",
    "        \n",
    "for key, value in all_proc_files.items():\n",
    "    no_of_processed_files.append(len(value))\n",
    "    proc_files.append(value)\n",
    "    \n",
    "for key, value in all_raw_files.items():\n",
    "    no_of_raw_files.append(len(value))\n",
    "    raw_files.append(value)    \n",
    "    \n",
    "max_raw = max(no_of_raw_files)\n",
    "max_proc = max(no_of_processed_files)\n",
    "\n",
    "if max_proc > 2 or max_raw > 5:\n",
    "    print(\"Updating column headers for files\")\n",
    "    edit_file_headers = True\n",
    "\n",
    "if edit_file_headers:\n",
    "    file_headers =  []\n",
    "    for i in range(max_proc):\n",
    "        file_headers.append('processed data file')\n",
    "    file_headers.append('*raw file')    \n",
    "    for i in range(max_raw-1):\n",
    "        file_headers.append('raw file')  \n",
    "\n",
    "    start_row_files = start_row - 1\n",
    "    for col_num, value in enumerate(file_headers, start=15):\n",
    "        sheet.cell(row=start_row_files, column=col_num, value=value)\n",
    "        \n",
    "    start_row_files = start_row\n",
    "    start_column_proc = 15\n",
    "    \n",
    "    for i, row_data in enumerate(proc_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc):\n",
    "            sheet.cell(row=i, column=j, value=value)\n",
    "    \n",
    "    for i, row_data in enumerate(raw_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc + max_proc):\n",
    "            sheet.cell(row=i, column=j, value=value)   \n",
    "\n",
    "else:\n",
    "    start_row_files = start_row\n",
    "    start_column_proc = 15\n",
    "    \n",
    "    for i, row_data in enumerate(proc_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc):\n",
    "            sheet.cell(row=i, column=j, value=value)\n",
    "    \n",
    "    for i, row_data in enumerate(raw_files, start=start_row_files):\n",
    "        for j, value in enumerate(row_data, start=start_column_proc + 2):\n",
    "            sheet.cell(row=i, column=j, value=value)\n",
    "            \n",
    "            \n",
    "#Populate protocol section                \n",
    "for row in sheet.iter_rows():\n",
    "    for cell in row:\n",
    "        if cell.value == \"*extract protocol\":\n",
    "            sheet[f\"B{cell.row}\"] = extract_protocol\n",
    "        if cell.value == \"*library construction protocol\":\n",
    "            sheet[f\"B{cell.row}\"] = library_construction_protocol\n",
    "        if cell.value == \"*data processing step\":\n",
    "            sheet[f\"B{cell.row}\"] = data_processing_step\n",
    "        if cell.value == \"*genome build/assembly\":\n",
    "            sheet[f\"B{cell.row}\"] = organism_genome_assembly\n",
    "        if cell.value == \"*processed data files format and content\":\n",
    "            sheet[f\"B{cell.row}\"] = processed_data_files_format_content\n",
    "            \n",
    "#Populate Paired-end experiment    \n",
    "if len(paired_raw_files) > 0:\n",
    "    for row in sheet.iter_rows():\n",
    "        for cell in row:\n",
    "            if cell.value == \"file name 1\":\n",
    "                paired_files_start_row = cell.row\n",
    "    \n",
    "    paired_per_exp = []\n",
    "    for r1, r2 in paired_raw_files.items():\n",
    "        paired_per_exp.append([r1, r2])\n",
    "\n",
    "    for i, row_data in enumerate(paired_per_exp, start=paired_files_start_row+1):\n",
    "        for j, value in enumerate(row_data, start=1):\n",
    "            sheet.cell(row=i, column=j, value=value)                        \n",
    "workbook.save(GEO_metadata_template_file)  \n",
    "\n",
    "print(\"Metadata sheet updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d394cef3-2a2e-48cb-a5f9-44d269faf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate MD5 Checksums sheet.\n",
    "\n",
    "add_md5sum_raw = []\n",
    "add_md5sum_proc = []\n",
    "\n",
    "for filename, md5sum in md5sums_raw.items():\n",
    "    add_md5sum_raw.append([filename,md5sum ])\n",
    "\n",
    "add_md5sum_proc = []\n",
    "\n",
    "for filename, md5sum in md5sums_proc.items():\n",
    "    add_md5sum_proc.append([filename,md5sum ])\n",
    "\n",
    "\n",
    "workbook = openpyxl.load_workbook(GEO_metadata_template_file)\n",
    "sheet = workbook['MD5 Checksums']\n",
    "\n",
    "start_row = 9\n",
    "for i, row_data in enumerate(add_md5sum_raw, start=start_row):\n",
    "    for j, value in enumerate(row_data, start=1):\n",
    "        sheet.cell(row=i, column=j, value=value)\n",
    "        \n",
    "for i, row_data in enumerate(add_md5sum_proc, start=start_row):\n",
    "    for j, value in enumerate(row_data, start=6):\n",
    "        sheet.cell(row=i, column=j, value=value)        \n",
    "        \n",
    "workbook.save(GEO_metadata_template_file)\n",
    "\n",
    "print(\"MD5sum sheet updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8341b-6267-4d1e-a28c-94d39bdf95fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
