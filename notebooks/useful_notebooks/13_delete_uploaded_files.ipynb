{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6e347c-6d64-40cf-bb68-f8f058a1b227",
   "metadata": {},
   "source": [
    "### Replace uploaded files\n",
    "Use this notebook when you want to replace a file that had not yet been released. Replacement consists in setting the correct file status, deleting some properties on the File item that refer to the previously uploaded file, and delete wfr items and related quality metrics that the previously uploaded file was input for.\n",
    "\n",
    "The new file upload needs to be handled separately.\n",
    "\n",
    "The notebook checks the status of items before patching.\n",
    "\n",
    "**NOTE 1:** only use this when a file is not yet released.\n",
    "\n",
    "**NOTE 2:** you need to delete files from S3 if you don't immediately proceed with uploading a new file, since the hourly md5 check reverts the effect of running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7989d-0373-4ce2-b242-ce38a8145207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "\n",
    "my_auth = get_key('andrea_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ddcd0-3420-468e-bc49-759fb82a21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate files to reset\n",
    "files = '''\n",
    "4DNFIXXXXXX1\n",
    "4DNFIXXXXXX2\n",
    "'''\n",
    "\n",
    "files = [f for f in files.split('\\n') if f]\n",
    "print(len(files), 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691ba6a-10e1-4333-86cf-9ea2eae9c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "def find_attached_items(file):\n",
    "    \"\"\"Given a File, list @id and status of all wfr inputs and linked quality metrics\"\"\"\n",
    "    def _get_id_status(item):\n",
    "        return item['@id'], item['status']\n",
    "\n",
    "    item_ids = []\n",
    "    wfr_inputs = file.get('workflow_run_inputs', [])\n",
    "    # exception for File types that do not have workflow_run_inputs\n",
    "    if file['@type'][0] not in ['FileFastq', 'FileProcessed']:\n",
    "        wfr_inputs = search_wfrs_with_input_file(file)\n",
    "    \n",
    "    for wfr_in in wfr_inputs:\n",
    "        # append wfr that has the file as input\n",
    "        item_ids.append(_get_id_status(wfr_in))\n",
    "        wfr_in_object = ff_utils.get_metadata(wfr_in['@id'], key=my_auth)\n",
    "        # append any qc of the wfr\n",
    "        if wfr_in_object.get('quality_metric'):\n",
    "            item_ids.append(_get_id_status(wfr_in_object['quality_metric']))\n",
    "\n",
    "    return item_ids\n",
    "\n",
    "\n",
    "def search_wfrs_with_input_file(file):\n",
    "    \"\"\"Returns wfr items that have a given file as input.\n",
    "    This is helpful when handling FileReference or others that do not have workflow_run_inputs\"\"\"\n",
    "    query = '/search/?type=WorkflowRunAwsem&input_files.value.accession=' + file['accession']\n",
    "    wfr_items = ff_utils.search_metadata(query, key=my_auth)\n",
    "    return wfr_items\n",
    "\n",
    "\n",
    "def delete_item(item_id, status):\n",
    "    \"\"\"Delete item if status allows\"\"\"\n",
    "    assert status in ['in review by lab', 'pre-release'], \"Item status is incompatible with this change\"\n",
    "    if ACTION:\n",
    "        res = ff_utils.patch_metadata({\"status\": \"deleted\"}, item_id, key=my_auth)\n",
    "        if res.get('status') == 'success':\n",
    "            print(f\"Deleted {item_id}\")\n",
    "    else:\n",
    "        print(f\"{item_id} will be deleted\")\n",
    "    return\n",
    "\n",
    "\n",
    "def delete_file_fields(file):\n",
    "    \"\"\"Reset file to uploading if status allows\"\"\"\n",
    "    assert file['status'] in ['uploaded', 'pre-release'], \"File status is incompatible with this change\"\n",
    "    fields_to_be_removed = ['filename', 'md5sum', 'content_md5sum', 'file_size']\n",
    "    del_add_on = 'delete_fields=' + ','.join(fields_to_be_removed)\n",
    "    if ACTION:\n",
    "        res = ff_utils.patch_metadata({\"status\": \"uploading\"}, file['@id'], key=my_auth, add_on=del_add_on)\n",
    "        if res.get('status') == 'success':\n",
    "            print(f\"Reset {file['@id']} to uploading\")\n",
    "    else:\n",
    "        print(f\"{file['@id']} will be reset to uploading\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcada955-e0f1-4d6f-bb17-b703e22082e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "# set action to True to patch items, otherwise a dry run will be executed\n",
    "ACTION = False\n",
    "\n",
    "for file_id in files:\n",
    "    f = ff_utils.get_metadata(file_id, key=my_auth)\n",
    "    additional_items = find_attached_items(f)\n",
    "    for (item_id, status) in additional_items:\n",
    "        delete_item(item_id, status)\n",
    "    delete_file_fields(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fbb96-094c-4761-8c8e-444f5b91cb25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
