{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digest data sets to simplified json objects for GEO submission\n",
    "\n",
    "This notebook exports ExperimentSets and related Experiments and Biosamples as json files, compatible with GEO submission. Important metadata from other item types is recorded in the relevant file (e.g. Biosource info is condensed in the exported Biosample json).\n",
    "\n",
    "* Part 0. Initialize all functions.\n",
    "* Part 1. List all sets to export. **IMPORTANT**: make sure that the sets do not have **restricted files** (e.g. from HeLa cells).\n",
    "* Part 2. Generate simplified dictionaries for each ExpSet, Experiment and Biosample.\n",
    "* Part 3. Save dictionaries as json files.\n",
    "* Part 4. Patch items with date of export for external submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import get_key\n",
    "from functions.geo_minimization import *\n",
    "\n",
    "my_auth = get_key('andrea_data')\n",
    "URL = 'https://data.4dnucleome.org'\n",
    "DB = 'GEO'\n",
    "FORMATS = ['fastq', 'mcool', 'pairs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boildown_protocol(protocol_object):\n",
    "    '''used for both protocol and document items'''\n",
    "    if protocol_object.get('attachment') is None:\n",
    "        protocol_object = ff_utils.get_metadata(protocol_object['uuid'], key=my_auth)\n",
    "    output_dict = {}\n",
    "    protocol_simple_interesting_values = ['protocol_type', 'description', 'url']\n",
    "    for key in protocol_object:\n",
    "        if key == 'attachment':\n",
    "            output_dict['download'] = URL + protocol_object['@id'] + protocol_object['attachment']['href']\n",
    "        elif key in protocol_simple_interesting_values:\n",
    "            output_dict[key] = protocol_object[key]\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def boildown_protocols(protocols):\n",
    "    return [boildown_protocol(p) for p in protocols]\n",
    "\n",
    "\n",
    "def boildown_experimental_protocol(protocol, protocol_variations):\n",
    "    ''' Combine experimental protocol and protocol variations.\n",
    "    Return a list of protocol objects'''\n",
    "    experimental_protocols = []\n",
    "    if protocol:\n",
    "        experimental_protocols.append(protocol)\n",
    "    if protocol_variations:\n",
    "        experimental_protocols.extend(protocol_variations)\n",
    "    return boildown_protocols(experimental_protocols)\n",
    "\n",
    "\n",
    "def boildown_cell_culture_details(cell_culture_details, biosources):\n",
    "    ''' get SOP_cell_culture from Biosource (not embedded in Biosample) and\n",
    "    'protocols_additional' and 'authentication_protocols' from BCC (not embedded in Biosample)\n",
    "    '''\n",
    "    protocols = []\n",
    "    for bio in biosources:\n",
    "        biosource = ff_utils.get_metadata(bio['@id'], key=my_auth)\n",
    "        if biosource.get('SOP_cell_line'):\n",
    "            protocols.append(biosource['SOP_cell_line'])\n",
    "    for cell_culture in cell_culture_details:\n",
    "        bcc = ff_utils.get_metadata(cell_culture['uuid'], key=my_auth)\n",
    "        protocols.extend(bcc.get('protocols_additional', []))\n",
    "        protocols.extend(bcc.get('authentication_protocols', []))\n",
    "    return boildown_protocols(protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize files. Uses get_metadata, works for raw and processed files.\n",
    "\n",
    "file_quality_metric_interesting_values = [\n",
    "    'Sequence length',\n",
    "    'Total Sequences'\n",
    "]\n",
    "\n",
    "file_interesting_values = [\n",
    "    'paired_end',\n",
    "    'accession',\n",
    "    'display_title',\n",
    "    'file_type',\n",
    "#     'file_type_detailed',  # has also file_format['display_title']\n",
    "#     'href',\n",
    "    'file_size',\n",
    "    'file_classification',\n",
    "    'filename',  # is it necessary? not embedded in expset\n",
    "    'instrument',  # is it necessary? not embedded in expset\n",
    "    'public_release',  # is it necessary? not embedded in expset\n",
    "    'genome_assembly',\n",
    "    'md5sum',\n",
    "#     'content_md5sum',\n",
    "]\n",
    "\n",
    "\n",
    "def boildown_files(files_list):\n",
    "    '''Takes list of raw files and produces list of runs.\n",
    "    Each run is a pair of files (if paired end) or a single file (if single end).'''\n",
    "    files = [boildown_file(f) for f in files_list]\n",
    "    runs = []\n",
    "    for a_file in files:\n",
    "        if a_file.get('paired_end') is None:\n",
    "            runs.append([a_file])\n",
    "        elif a_file.get('paired_end') == '1':\n",
    "            pe1 = a_file\n",
    "            for another_file in files:\n",
    "                if another_file['accession'] == a_file['related_files']:\n",
    "                    pe2 = another_file\n",
    "                    break\n",
    "            runs.append([pe1, pe2])\n",
    "    return runs\n",
    "\n",
    "\n",
    "def boildown_file(file_object):\n",
    "    '''get_metadata to get all info'''\n",
    "    file_dictionary = {}\n",
    "    file = ff_utils.get_metadata(file_object['accession'], key=my_auth)\n",
    "    for key, value in file.items():\n",
    "        if key in file_interesting_values:\n",
    "            if isinstance(value, list):\n",
    "                if len(value) > 0:\n",
    "                    file_dictionary[key] = ', '.join(value)\n",
    "            else:\n",
    "                file_dictionary[key] = str(value)\n",
    "        elif key == 'file_format':\n",
    "            file_dictionary[key] = boildown_title(value)\n",
    "        elif key == 'quality_metric':\n",
    "            for k in value.keys():\n",
    "                if k in file_quality_metric_interesting_values:\n",
    "                    file_dictionary[k] = str(value[k])\n",
    "        elif key == 'related_files':\n",
    "            file_dictionary[key] = boildown_related_files(value)\n",
    "        elif key == 'workflow_run_outputs' and len(value) > 0:\n",
    "            wfr = value[0]  # file derives only from the first wfr in the list\n",
    "            file_dictionary['workflow_run'] = URL + wfr['@id']\n",
    "#             file_dictionary['workflow'] = wfr['workflow']['display_title']\n",
    "            file_dictionary['derived_from'] = \", \".join([\n",
    "                file['value']['display_title'] for file in wfr['input_files']\n",
    "            ])\n",
    "    return file_dictionary\n",
    "\n",
    "\n",
    "def boildown_processed_files(processed_files_list):\n",
    "    '''Specific for HiC pipeline'''\n",
    "    output_list = []\n",
    "    for pf in processed_files_list:\n",
    "        if pf['file_format']['display_title'] in ['pairs', 'mcool']:\n",
    "            file_dict = boildown_file(pf)\n",
    "            file_dict['data_processing'] = 'https://data.4dnucleome.org/resources/data-analysis/hi_c-processing-pipeline'\n",
    "            output_list.append(file_dict)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expset_function_dispatch = {\n",
    "    'accession': same,\n",
    "    'lab': boildown_title,\n",
    "    'contributing_labs': boildown_list_to_titles,\n",
    "    'submitted_by': boildown_title,\n",
    "    'award': boildown_award,\n",
    "    'description': same,\n",
    "    'dataset_label': same,\n",
    "    'condition': same,\n",
    "    'last_modified': boildown_date_modified,\n",
    "    'public_release': same,\n",
    "    'experiments_in_set': boildown_experiments_in_set,\n",
    "    'documents': boildown_protocols,  # get_metadata for each protocol\n",
    "    'number_of_experiments': same,\n",
    "    'replicate_exps': boildown_replicate_exps,\n",
    "    'processed_files': boildown_processed_files,\n",
    "    'external_references': boildown_external_references,  # use instead of dbxrefs because it is validated\n",
    "    'produced_in_pub': boildown_publication,\n",
    "}\n",
    "\n",
    "\n",
    "def simplify_expset(expset_accession, my_auth):\n",
    "    ''' Keys are explicitly declared in the function dispatch dictionary.\n",
    "    They will be added in the same order. Keys not present are ignored.'''\n",
    "    expset_object = ff_utils.get_metadata(expset_accession, my_auth)\n",
    "    expset_dict = {}\n",
    "    for key in expset_function_dispatch.keys():\n",
    "        if expset_object.get(key):\n",
    "            value = expset_object[key]\n",
    "            result = expset_function_dispatch[key](value)\n",
    "            if isinstance(result, dict):\n",
    "                for k, v in result.items():\n",
    "                    expset_dict[k] = v\n",
    "            else:\n",
    "                expset_dict[key] = result\n",
    "        else:  # add empty fields, just to have an overview of all keys\n",
    "            expset_dict[key] = ''\n",
    "    if not expset_dict.get('produced_in_pub'):\n",
    "        data_usage_section = ff_utils.get_metadata(\"621e8359-3885-40ce-965d-91894aa7b758\", key=my_auth)\n",
    "        expset_dict['produced_in_pub'] = data_usage_section['content']\n",
    "    return expset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific for ExperimentHiC\n",
    "experiment_ignore = [\n",
    "    'filesets', 'notes', 'aliases', '@type', 'badges', 'date_created',\n",
    "    'experiment_relation', 'dbxrefs', 'references', 'follows_sop',\n",
    "    'library_preparation_date', 'other_processed_files', 'project_release',\n",
    "    'publications_of_exp', 'quality_metric_flags', 'reference_files',\n",
    "    'sop_mapping', 'static_content', 'static_headers', 'status', 'tags',\n",
    "    'schema_version', '@id', 'uuid', 'principals_allowed', '@context',\n",
    "    'actions', 'aggregated-items', 'validation-errors', 'authentication_docs',\n",
    "    'biosample_quantity_units',  # combined with biosample quantity\n",
    "    'alternate_accessions', 'produced_in_pub', 'experiment_sets'\n",
    "]\n",
    "\n",
    "experiment_function_dispatch = {\n",
    "    'accession': same,\n",
    "    'lab': boildown_title,\n",
    "    'contributing_labs': boildown_list_to_titles,\n",
    "    'submitted_by': boildown_title,\n",
    "    'award': boildown_award,\n",
    "    'display_title': boildown_exp_display_title,\n",
    "    'description': same,\n",
    "    'last_modified': boildown_date_modified,\n",
    "    'public_release': same,\n",
    "    'experiment_type': boildown_title,\n",
    "    'biological_replicate_number': '',\n",
    "    'technical_replicate_number': '',\n",
    "    'experiment_categorizer': boildown_exp_categorizer,  # 'combined' key (enzyme or target) + value\n",
    "    'biosample': boildown_title, # boildown_biosample,\n",
    "    'biosample_quantity': boildown_biosample_quantity,\n",
    "    'experiment_summary': same,\n",
    "    'digestion_enzyme': boildown_title,\n",
    "    'digestion_temperature': same,\n",
    "    'digestion_time': same,\n",
    "    'ligation_temperature': same,\n",
    "    'ligation_time': same,\n",
    "    'ligation_volume': same,\n",
    "    'tagging_method': same,\n",
    "    'average_fragment_size': same,\n",
    "    'biotin_removed': same,\n",
    "    'crosslinking_method': same,\n",
    "    'crosslinking_temperature': same,\n",
    "    'crosslinking_time': same,\n",
    "    'enzyme_lot_number': same,\n",
    "    'fragment_size_range': same,\n",
    "    'fragment_size_selection_method': same,\n",
    "    'fragmentation_method': same,\n",
    "    'library_prep_kit': same,\n",
    "    'pcr_cycles': same,\n",
    "    'protocol': boildown_experimental_protocol,  # includes protocol_variation\n",
    "#     'protocol_variation': boildown_protocols,\n",
    "    'documents': boildown_protocols,  # get_metadata for each protocol\n",
    "    'external_references': boildown_external_references,\n",
    "    'files': boildown_files,  # get_metadata for each file\n",
    "    'processed_files': boildown_processed_files,  # do not want these files\n",
    "}\n",
    "\n",
    "def simplify_experiment(experiment_accession, my_auth, biorep, tecrep):\n",
    "    ''' Keys are explicitly declared in the function dispatch dictionary.\n",
    "    They will be added in the same order. Keys not present are ignored.'''\n",
    "    experiment_object = ff_utils.get_metadata(experiment_accession, my_auth)\n",
    "    experiment_dict = {}\n",
    "    for key in experiment_function_dispatch.keys():\n",
    "        # start exceptions\n",
    "        if key == 'biosample_quantity':\n",
    "            experiment_dict[key] = experiment_function_dispatch[key](\n",
    "                experiment_object[key], experiment_object['biosample_quantity_units'])\n",
    "        elif key == 'biological_replicate_number':\n",
    "            experiment_dict[key] = biorep\n",
    "        elif key == 'technical_replicate_number':\n",
    "            experiment_dict[key] = tecrep\n",
    "        elif key == 'protocol':\n",
    "            experiment_dict['experimental_protocol'] = experiment_function_dispatch[key](\n",
    "                experiment_object.get(key), experiment_object.get('protocol_variation'))\n",
    "        # end exceptions\n",
    "        \n",
    "        elif experiment_object.get(key):\n",
    "            value = experiment_object[key]\n",
    "            result = experiment_function_dispatch[key](value)\n",
    "#             if isinstance(result, dict):\n",
    "#                 for k, v in result.items():\n",
    "#                     experiment_dict[k] = v\n",
    "#             else:\n",
    "            experiment_dict[key] = result\n",
    "        else:  # add empty fields, just to have an overview of all keys\n",
    "            experiment_dict[key] = ''\n",
    "\n",
    "    return experiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boildown_individual(individual_embedded):\n",
    "    '''GET metadata from individual id and return all keys in interesting values'''\n",
    "    individual_interesting_values = [\n",
    "        'age', 'age_units', 'sex', 'life_stage', 'mouse_life_stage',\n",
    "        'mouse_strain', 'ethnicity', 'health_status',\n",
    "    ]\n",
    "    individual_dict = {}\n",
    "    individual = ff_utils.get_metadata(individual_embedded['@id'], key=my_auth)\n",
    "    for key, value in individual.items():\n",
    "        if key in individual_interesting_values:\n",
    "            individual_dict[key] = value\n",
    "        elif key == 'organism':\n",
    "            individual_dict['organism_id'] = get_organism_id(individual)\n",
    "    return individual_dict\n",
    "\n",
    "\n",
    "biosource_function_dispatch = {\n",
    "#     'display_title': same,  # do not need that\n",
    "    'biosource_type': same,\n",
    "    'biosource_vendor': boildown_title,\n",
    "    'individual': boildown_individual,\n",
    "#     'SOP_cell_line': boildown_sop_cell_line,  # already retrieved from Biosample\n",
    "}\n",
    "\n",
    "\n",
    "def minimize_biosource(biosources_list):\n",
    "    ''' Biosources list is obtained from biosample.\n",
    "    Often (always?) there is just one Biosource. Return list of accessions otherwise.\n",
    "    Most of the interesting values are embedded fields, apart from SOP_cell_line\n",
    "    which requires to get_metadata.'''\n",
    "    if len(biosources_list) > 1:\n",
    "        return ', '.join([bios['accession'] for bios in biosources_list])\n",
    "    # most cases have only 1 biosource\n",
    "    biosource = biosources_list[0]\n",
    "    biosource_dict = {}\n",
    "    for key in biosource_function_dispatch.keys():\n",
    "        if biosource.get(key):\n",
    "            value = biosource[key]\n",
    "            result = biosource_function_dispatch[key](value)\n",
    "            if isinstance(result, dict):  # here only individual\n",
    "                for k, v in result.items():\n",
    "                    biosource_dict[k] = v\n",
    "            else:\n",
    "                biosource_dict[key] = result\n",
    "        else:  # add empty fields, just to have an overview of all keys\n",
    "            biosource_dict[key] = ''\n",
    "    return biosource_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biosample_function_dispatch = {\n",
    "    'accession': same,\n",
    "    'biosource_summary': same,  # calcprop (biosource_name), OK also with multiple biosources\n",
    "    'biosample_type': same,  # calcprop (biosource_type or mixed biosources if many)\n",
    "    'tissue_organ_info': boildown_tissue_organ_info,  # OK also with multiple biosources\n",
    "    'description': same,  # is it needed? not embedded in expset\n",
    "    'modifications_summary': same,  # calcprop\n",
    "    'treatments_summary': same,  # calcprop\n",
    "    'biosource': minimize_biosource,  # dict\n",
    "    'biosample_protocols': boildown_protocols,  # get_metadata for each protocol\n",
    "    'cell_culture_details': boildown_cell_culture_details,  # get_metadata for each protocol and also SOP\n",
    "    'last_modified': boildown_date_modified,\n",
    "    'documents': boildown_protocols,\n",
    "    'external_references': boildown_external_references,  # dbxrefs. is it needed?\n",
    "}\n",
    "\n",
    "\n",
    "def simplify_biosample(biosample_accession, my_auth):\n",
    "    ''' Keys are explicitly declared in the function dispatch dictionary.\n",
    "    They will be added in the same order. Keys not present are ignored.'''\n",
    "    biosample_object = ff_utils.get_metadata(biosample_accession, my_auth)\n",
    "    biosample_dict = {}\n",
    "    for key in biosample_function_dispatch.keys():\n",
    "        # start exceptions\n",
    "        if key == 'cell_culture_details':\n",
    "            biosample_dict['cell_culture_protocols'] = biosample_function_dispatch[key](\n",
    "                biosample_object.get(key), biosample_object['biosource'])\n",
    "        # end exceptions\n",
    "\n",
    "        elif biosample_object.get(key):\n",
    "            value = biosample_object[key]\n",
    "            result = biosample_function_dispatch[key](value)\n",
    "            if isinstance(result, dict):\n",
    "                for k, v in result.items():\n",
    "                    biosample_dict[k] = v\n",
    "            else:\n",
    "                biosample_dict[key] = result\n",
    "        else:  # add empty fields, just to have an overview of all keys\n",
    "            biosample_dict[key] = ''\n",
    "\n",
    "    return biosample_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List ExpSets (or a search query) to export for GEO submission\n",
    "\n",
    "sets_list = ['4DNESRJ8KV4Q'] # ['4DNESACCCCCC', '4DNESACCCCCC']\n",
    "search_url = ''  # '/search/?type=ExperimentSetReplicate&condition=Enzyme%20DpnII%20-%20in%20situ%20Hi-C%20on%20cells%20cultured%20prior%20to%204DN%20SOP'  # '/search/?type=ExperimentSetReplicate&award.project=4DN&status=pre-release&dataset_label=Micro-C+on+JM8.N4+Cells'\n",
    "\n",
    "sets_to_submit = []\n",
    "if sets_list:\n",
    "    sets_to_submit.extend([ff_utils.get_metadata(set_id, key=my_auth) for set_id in sets_list])\n",
    "elif search_url:\n",
    "    sets_to_submit = ff_utils.search_metadata(search_url, my_auth)\n",
    "\n",
    "print(len(sets_to_submit), 'Experiment Sets to export for GEO submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the items\n",
    "store = {}  # key is @id, value is the object with frame=embedded\n",
    "\n",
    "def _get_if_not_in_store(item):\n",
    "    at_id = item['@id']\n",
    "    if at_id not in store:\n",
    "        store[at_id] = ff_utils.get_metadata(at_id, key=my_auth)\n",
    "    return\n",
    "\n",
    "time1 = time.time()\n",
    "for expset in sets_to_submit:\n",
    "    store[expset['@id']] = expset\n",
    "    \n",
    "    # Experiment, Biosample\n",
    "    for experiment in expset.get('experiments_in_set'):\n",
    "        _get_if_not_in_store(experiment)\n",
    "        _get_if_not_in_store(experiment['biosample'])\n",
    "\n",
    "        # BiosampleCellCulture, Protocols\n",
    "        for cell_culture in experiment['biosample'].get('cell_culture_details'):\n",
    "            _get_if_not_in_store(cell_culture)\n",
    "            for protocol in (cell_culture.get('protocols_additional', [])\n",
    "                             + cell_culture.get('authentication_protocols', [])):\n",
    "                _get_if_not_in_store(protocol)\n",
    "\n",
    "        # Biosource, Individual, Organism\n",
    "        for biosource in experiment['biosample']['biosource']:\n",
    "            _get_if_not_in_store(biosource)\n",
    "            _get_if_not_in_store(biosource['individual'])\n",
    "            # insert somewhere here some control on HeLa or other restricted files\n",
    "            _get_if_not_in_store(biosource['individual']['organism'])\n",
    "#             if biosource.get('SOP_cell_line'):  # already embedded in biosource\n",
    "#                 _get_if_not_in_store(protocol)\n",
    "\n",
    "        # Exp. Protocol + variations\n",
    "        for protocol in ([(experiment.get('protocol', '')]\n",
    "                           + experiment.get('protocol_variations', [])):\n",
    "            _get_if_not_in_store(protocol)\n",
    "\n",
    "        # Raw and Processed Files (in experiment)\n",
    "        for file in experiment.get('files', []) + experiment.get('processed_files', []):\n",
    "            if file['file_format']['display_title'] in FORMATS:\n",
    "                _get_if_not_in_store(file)\n",
    "\n",
    "    # Processed Files (in experiment set)\n",
    "    for file in expset.get('processed_files', []):\n",
    "        if file['file_format']['display_title'] in FORMATS:\n",
    "            _get_if_not_in_store(file)\n",
    "\n",
    "time2 = time.time()\n",
    "print(round((time2-time1), 1), 'sec for collection')\n",
    "for item in store:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export simplified dictionaries for each ExpSet, Experiment, Biosample\n",
    "\n",
    "date_dict = {}  # record date of export\n",
    "\n",
    "def get_date_exported(database):\n",
    "    '''get date of metadata export (to be reported in the external_submissions)'''\n",
    "    date = datetime.now(timezone.utc).isoformat()\n",
    "    return {'date_exported': date, 'database': database}\n",
    "\n",
    "# Experiment Sets\n",
    "expset_dicts = []\n",
    "experiments_to_submit = {}\n",
    "for accession in sets_to_submit:\n",
    "    expset_dictionary = simplify_expset(accession, my_auth)\n",
    "    expset_dicts.append(expset_dictionary)\n",
    "    date_dict[accession] = get_date_exported(DB)\n",
    "    \n",
    "    # get Experiments and replicate structure from the exported ExpSet\n",
    "    for rep in expset_dictionary['replicate_exps']:\n",
    "        experiments_to_submit[rep['replicate']] = {'bio_rep_no': rep['biological_replicate_number'],\n",
    "                                                   'tec_rep_no': rep['technical_replicate_number']}\n",
    "\n",
    "# Experiments\n",
    "exp_dicts = []\n",
    "biosamples_to_submit = []\n",
    "for accession, rep in experiments_to_submit.items():\n",
    "    exp_dictionary = simplify_experiment(accession, my_auth, rep['bio_rep_no'], rep['tec_rep_no'])\n",
    "    exp_dicts.append(exp_dictionary)\n",
    "    date_dict[accession] = get_date_exported(DB)\n",
    "    \n",
    "    # get Biosamples from the exported Experiments\n",
    "    biosamples_to_submit.append(exp_dictionary['biosample'])\n",
    "\n",
    "# Biosamples\n",
    "bio_dicts = []\n",
    "for accession in list(set(biosamples_to_submit)):\n",
    "    bio_dictionary = simplify_biosample(accession, my_auth)\n",
    "    bio_dicts.append(bio_dictionary)\n",
    "    date_dict[accession] = get_date_exported(DB)\n",
    "\n",
    "print('Exp Sets exported:\\t', sets_to_submit)\n",
    "print('Experiments exported:\\t', [k for k in experiments_to_submit.keys()])\n",
    "print('Biosamples exported:\\t', list(set(biosamples_to_submit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save json files\n",
    "\n",
    "directory = Path(\"~/Documents/GEO/submissions\").expanduser()\n",
    "overwrite = False  # overwrites existing files if set to True\n",
    "\n",
    "for dictionary in (expset_dicts + exp_dicts + bio_dicts):\n",
    "    file_name = dictionary['accession'] + '.json'\n",
    "    full_path = Path.joinpath(directory, file_name)\n",
    "    if full_path.exists() and not overwrite:\n",
    "        print(file_name, 'already exists in the folder', directory)\n",
    "    else:\n",
    "        with open(full_path, 'w') as fp:\n",
    "            json.dump(dictionary, fp, indent=4)\n",
    "        print(file_name, 'file saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. patch export date for each item in external_submission\n",
    "action = False\n",
    "\n",
    "count = 0\n",
    "for accession, patch_body in date_dict.items():\n",
    "    if action:\n",
    "        res = ff_utils.patch_metadata({\"external_submission\": patch_body}, accession, key=my_auth)\n",
    "        if res['status'] != 'success':\n",
    "            print(res)\n",
    "            break\n",
    "        else:\n",
    "            count += 1\n",
    "    else:\n",
    "        print(accession, patch_body)\n",
    "print('patched {} items'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
