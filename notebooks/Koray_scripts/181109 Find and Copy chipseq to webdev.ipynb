{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING ON https://data.4dnucleome.org \n",
      "\n",
      "dict_keys(['individual_mouse', 'biosample', 'user', 'file_fastq', 'vendor', 'experiment_set_replicate', 'genomic_region', 'quality_metric_fastqc', 'modification', 'antibody', 'file_format', 'lab', 'target', 'award', 'protocol', 'experiment_seq', 'ontology_term', 'organism', 'biosample_cell_culture', 'ontology', 'biosource'])\n",
      "individual_mouse 1\n",
      "biosample 4\n",
      "user 15\n",
      "file_fastq 12\n",
      "vendor 2\n",
      "experiment_set_replicate 3\n",
      "genomic_region 1\n",
      "quality_metric_fastqc 12\n",
      "modification 2\n",
      "antibody 1\n",
      "file_format 1\n",
      "lab 10\n",
      "target 3\n",
      "award 5\n",
      "protocol 2\n",
      "experiment_seq 6\n",
      "ontology_term 46\n",
      "organism 1\n",
      "biosample_cell_culture 4\n",
      "ontology 3\n",
      "biosource 1\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "\n",
    "# get key from keypairs.json\n",
    "my_env = 'data'\n",
    "my_key = get_key('koray_data')\n",
    "schema_name = get_schema_names(my_key) \n",
    "print('WORKING ON', my_key['server'], '\\n')\n",
    "\n",
    "##### COLLECT ITEMS TO Release #####\n",
    "# use either a starting item to fetch all linked items\n",
    "\n",
    "# Use a starting item to find linked ones\n",
    "# starting_items = ['46db06ad-b399-4cf4-9acc-07b3e25ef132']\n",
    "#add_items = get_query_or_linked(my_key, linked=starting_items)\n",
    "\n",
    "# or a search query\n",
    "#my_query = '/search/?q=GOLD&type=Item&limit=all'\n",
    "#add_items = get_query_or_linked(my_key, query=my_query)\n",
    "\n",
    "# if you want you can dump them to separate json files (will work as test insert)\n",
    "# dump_to_json(add_items, destination folder)\n",
    "\n",
    "# my_query = '/search/?biosample.biosource.individual.organism.name=mouse&biosample.biosource_summary=ES-E14&experiment_type=in%20situ%20Hi-C&type=ExperimentHiC'\n",
    "# store = get_query_or_linked(my_key, query=my_query)\n",
    "# print(store.keys())\n",
    "# print(len([i['uuid'] for key in store for i in store[key]]))\n",
    "# print()\n",
    "\n",
    "find_linked = ['8a776401-d264-4196-9369-5de9a8c83361', '56cd3d0c-607c-46f5-b16f-fb35d0660cda', 'd07a3074-c8e1-4112-8db4-d46db5e439a8']\n",
    "store = get_query_or_linked(my_key, linked=find_linked, linked_frame='raw', ignore_field=['references'])\n",
    "print(store.keys())\n",
    "for key in store:\n",
    "    print(key, len(store[key]))\n",
    "print(len([i['uuid'] for key in store for i in store[key]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "15 items exist on source\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267307e3a can not post item\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267227777 can not post item\n",
      "user fb287a31-e765-41c5-8c1d-665f8e9f025b can not post item\n",
      "user e2324f87-0625-4bbc-803b-d47677aebe08 can not post item\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267228139 can not post item\n",
      "user 986b362f-4eb6-4a9c-8173-3ab267111888 can not post item\n",
      "user 68b09572-0c3e-48c7-90a9-7bad21d6bac1 can not post item\n",
      "user d8ac229e-ec08-4411-be38-dc779520ea62 can not post item\n",
      "7 items transfered to target\n",
      "\n",
      "award\n",
      "5 items exist on source\n",
      "award b0b9c607-f8b4-4f02-93f4-9895b461334b can not post item\n",
      "award 12a92962-8265-4fc0-b2f8-cf14f05db58b can not post item\n",
      "award 029a2578-43dc-4343-8f41-694518cce304 can not post item\n",
      "2 items transfered to target\n",
      "\n",
      "lab\n",
      "10 items exist on source\n",
      "lab 828cd4fe-ebb0-4b36-a94a-d2e3a36cc989 can not post item\n",
      "lab ce54a7f7-af8a-4505-8327-e430634f494b can not post item\n",
      "lab 6e1ac73e-bdb7-4995-b003-2c31f538049a can not post item\n",
      "lab abd48785-b0e5-4453-be14-30d37a516bf3 can not post item\n",
      "6 items transfered to target\n",
      "\n",
      "ontology\n",
      "3 items exist on source\n",
      "ontology 530016bc-8535-4448-903e-854af460b254 can not post item\n",
      "ontology 530006bc-8535-4448-903e-854af460b254 can not post item\n",
      "ontology 530026bc-8535-4448-903e-854af460b254 can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "ontology_term\n",
      "46 items exist on source\n",
      "ontology_term 111167bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111189bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111130bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 3a3aca22-7824-4179-ba02-9fdc68abcc2f can not post item\n",
      "ontology_term 111116bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111115bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111114bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111113bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111112bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111111bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term a27ad2c3-cd3b-46fe-b69b-9673eacdd9b0 can not post item\n",
      "ontology_term 111120bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111118bc-8535-4448-903e-854af460a233 can not post item\n",
      "ontology_term 111117bc-8535-4448-903e-854af460a233 can not post item\n",
      "32 items transfered to target\n",
      "\n",
      "organism\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "file_format\n",
      "1 items exist on source\n",
      "file_format c13d06cf-218e-4f61-aaf0-91f226248b2c can not post item\n",
      "0 items transfered to target\n",
      "\n",
      "genomic_region\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "target\n",
      "3 items exist on source\n",
      "3 items transfered to target\n",
      "\n",
      "vendor\n",
      "2 items exist on source\n",
      "2 items transfered to target\n",
      "\n",
      "modification\n",
      "2 items exist on source\n",
      "2 items transfered to target\n",
      "\n",
      "protocol\n",
      "2 items exist on source\n",
      "2 items transfered to target\n",
      "\n",
      "biosample_cell_culture\n",
      "4 items exist on source\n",
      "4 items transfered to target\n",
      "\n",
      "individual_mouse\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "biosource\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "antibody\n",
      "1 items exist on source\n",
      "1 items transfered to target\n",
      "\n",
      "biosample\n",
      "4 items exist on source\n",
      "4 items transfered to target\n",
      "\n",
      "quality_metric_fastqc\n",
      "12 items exist on source\n",
      "12 items transfered to target\n",
      "\n",
      "file_fastq\n",
      "12 items exist on source\n",
      "12 items transfered to target\n",
      "\n",
      "experiment_seq\n",
      "6 items exist on source\n",
      "6 items transfered to target\n",
      "\n",
      "experiment_set_replicate\n",
      "3 items exist on source\n",
      "3 items transfered to target\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### This part should only run once!\n",
    "\n",
    "transfer_env = 'fourfront-webdev'\n",
    "transfer_key = ff_utils.get_authentication_with_server({}, ff_env=transfer_env)\n",
    "\n",
    "#### This part should only run once!\n",
    "\n",
    "# if the item exist in the target, should it overwrite it (will include user/award etc)\n",
    "overwrite_existing = False\n",
    "\n",
    "# reverse lookup dictionary for schema names\n",
    "rev_schema_name = {}\n",
    "for key, name in schema_name.items():\n",
    "    rev_schema_name[name] = schema_name[key]\n",
    "\n",
    "my_types = [i for i in ORDER if i in store.keys()]\n",
    "\n",
    "second_round_items = {}\n",
    "\n",
    "# Round I - only put the required - skip if exists already\n",
    "for a_type in my_types:\n",
    "    print(a_type)\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    # find required field\n",
    "    schema_info = ff_utils.get_metadata('/profiles/{}.json'.format(a_type), key=transfer_key)\n",
    "    req_fields = schema_info['required']\n",
    "    ids = schema_info['identifyingProperties']\n",
    "    first_fields = list(set(req_fields+ids))\n",
    "    remove_existing_items = []\n",
    "    counter=0\n",
    "    print(len(store[a_type]), 'items exist on source')\n",
    "    for an_item in store[a_type]:\n",
    "        counter += 1\n",
    "        \n",
    "        if overwrite_existing:\n",
    "            post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "        else:\n",
    "            # does the item exist\n",
    "            exists = False\n",
    "            try:\n",
    "                # TODO check with all identifiers\n",
    "                existing = ff_utils.get_metadata(an_item['uuid'], key=transfer_key)\n",
    "                exists = True\n",
    "            except:\n",
    "                exists = False\n",
    "            # skip the items that exists\n",
    "            if exists and existing:\n",
    "                remove_existing_items.append(an_item['uuid'])\n",
    "                print(\"{} {} can not post item\".format(obj_type, an_item['uuid']))\n",
    "                continue\n",
    "            post_first = {key:value for (key,value) in an_item.items() if key in first_fields}\n",
    "            ff_utils.post_metadata(post_first, obj_type, key = transfer_key)\n",
    "   \n",
    "    second_round_items[a_type] = [i for i in store[a_type] if i['uuid'] not in remove_existing_items]\n",
    "    print(len(second_round_items[a_type]), 'items transfered to target')\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round II - patch the rest of the metadata\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    if not second_round_items[a_type]:\n",
    "        continue \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        counter += 1\n",
    "        if a_type == 'file_fastq':\n",
    "            if 'extra_files' in an_item:\n",
    "                del an_item['extra_files']\n",
    "        ff_utils.patch_metadata(an_item, obj_id = an_item['uuid'], key = transfer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attachment copied\n"
     ]
    }
   ],
   "source": [
    "# Round III - move attachments\n",
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#source_addresses\n",
    "source_health = ff_utils.get_metadata('/health', key = my_key)\n",
    "source_raw = source_health['file_upload_bucket'] \n",
    "source_pf = source_health['processed_file_bucket'] \n",
    "source_att = source_health['blob_bucket']\n",
    "\n",
    "#target_addresses\n",
    "target_health = ff_utils.get_metadata('/health', key = transfer_key)\n",
    "target_raw = target_health['file_upload_bucket'] \n",
    "target_pf = target_health['processed_file_bucket'] \n",
    "target_att = target_health['blob_bucket'] \n",
    "\n",
    "# Round III - move attachments\n",
    "for a_type in my_types:\n",
    "    obj_type = rev_schema_name[a_type]\n",
    "    for an_item in second_round_items[a_type]:\n",
    "        if 'attachment' in an_item.keys():\n",
    "            at_key = an_item['attachment']['blob_id']\n",
    "            copy_source = {'Bucket': source_att, 'Key': at_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_att, at_key)\n",
    "            except:\n",
    "                print('Can not find attachment on source', an_item['uuid'])\n",
    "                continue\n",
    "            print('attachment copied')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n",
      "file copied\n"
     ]
    }
   ],
   "source": [
    "# Round IV - move files\n",
    "for a_type in my_types:\n",
    "    if a_type in ['file_processed']:\n",
    "        source_file_bucket = source_pf\n",
    "        target_file_bucket = target_pf\n",
    "    elif a_type in ['file_reference', 'file_fastq', 'file_microscopy', 'file_fasta', 'file_calibration']:\n",
    "        source_file_bucket = source_raw\n",
    "        target_file_bucket = target_raw\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    for an_item in second_round_items[a_type]:\n",
    "        # accumulate all keys from a file object to be uploaded\n",
    "        files_to_upload = []\n",
    "        file_resp = ff_utils.get_metadata(an_item['uuid'], key = my_key)\n",
    "        # add extra file keys\n",
    "        if file_resp.get('extra_files', []):\n",
    "            for an_extra_file in file_resp['extra_files']:\n",
    "                files_to_upload.append(an_extra_file['upload_key'])\n",
    "        # add main file key\n",
    "        files_to_upload.append(file_resp['upload_key'])\n",
    "        \n",
    "        for file_key in files_to_upload:\n",
    "            copy_source = {'Bucket': source_file_bucket, 'Key': file_key}\n",
    "            try:\n",
    "                s3.meta.client.copy(copy_source, target_file_bucket, file_key)\n",
    "            except:\n",
    "                print('Can not find file on source', file_key)\n",
    "                continue\n",
    "            print('file copied')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
