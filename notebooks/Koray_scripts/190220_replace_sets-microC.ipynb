{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining 4DNESCZJD9KK into 4DNESYTWHUH6\n",
      "same rep numbers are used, either merged already happened, or conflicting numbers in both sets, skipping\n",
      "\n",
      "Combining 4DNESRCFT5AI into 4DNES21D8SP8\n",
      "Continue with this rep numbers formatted b_t (y/n):\n",
      "['1_1', '1_2', '1_3', '1_4', '2_1', '2_2', '2_3', '2_4', '2_5']\n",
      "y\n",
      "4DNES21D8SP8  replicates are updated\n",
      "16 associated items will be archived\n"
     ]
    }
   ],
   "source": [
    "# Dekker lab biological replicate addition to micro-C experiments\n",
    "# old and new set lists\n",
    "# first updated the descriptions of the new sets, and deleted an additional empty set Ankita submitted\n",
    "# All sets have the correct replicate number, so I just need to combine them\n",
    "\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "workon = [['4DNESYTWHUH6', '51d9ac15-e55e-4f64-85e5-01998e349cf1'], # hff6 ones\n",
    "          ['4DNES21D8SP8', '998573fb-384e-4792-a621-254d705d4ec9']  # h1 ones\n",
    "         ]\n",
    "\n",
    "action = True\n",
    "\n",
    "def conv_time(time_info):\n",
    "    \"\"\"Convert date_created date_modified to datetime object for time operations\"\"\"\n",
    "    time_info, zone_info = time_info.split('+')\n",
    "    assert zone_info == '00:00'\n",
    "    try:\n",
    "        time_info = datetime.strptime(time_info, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "    except ValueError:  # items created at the perfect second\n",
    "        time_info = datetime.strptime(time_info, '%Y-%m-%dT%H:%M:%S')\n",
    "    return time_info\n",
    "\n",
    "def fetch_pf_associated(pf_id, my_key):\n",
    "    \"\"\"Given a file accession, find all related items\n",
    "    1) QCs\n",
    "    2) wfr producing the file, and other outputs from the same wfr\n",
    "    3) wfrs this file went as input, and all files/wfrs/qcs around it\n",
    "    The returned list might contain duplicates, uuids and display titles for qcs\"\"\"\n",
    "    file_as_list = []\n",
    "    pf_info = ff_utils.get_metadata(pf_id, my_key)\n",
    "    file_as_list.append(pf_info['uuid'])\n",
    "    if pf_info.get('quality_metric'):\n",
    "        file_as_list.append(pf_info['quality_metric']['uuid'])\n",
    "    inp_wfrs = pf_info.get('workflow_run_inputs')\n",
    "    out_wfr = pf_info.get('workflow_run_outputs')[0]\n",
    "    for inp_wfr in inp_wfrs:\n",
    "        file_as_list.extend(fetch_wfr_associated(inp_wfr['uuid'], my_key))\n",
    "    file_as_list.extend(fetch_wfr_associated(out_wfr['uuid'], my_key))\n",
    "    return list(set(file_as_list))\n",
    "        \n",
    "                \n",
    "def fetch_wfr_associated(wfr_uuid, my_key):\n",
    "    \"\"\"Given wfr_uuid, find associated output files and qcs\"\"\"\n",
    "    wfr_as_list = []\n",
    "    wfr_info = ff_utils.get_metadata(wfr_uuid, my_key)\n",
    "    wfr_as_list.append(wfr_info['uuid'])\n",
    "    if wfr_info.get('output_files'):\n",
    "        for o in wfr_info['output_files']:\n",
    "                if o.get('value'):\n",
    "                    wfr_as_list.append(o['value']['uuid'])\n",
    "                elif o.get('value_qc'):\n",
    "                    wfr_as_list.append(o['value_qc']) # this is a @id\n",
    "    if wfr_info.get('output_quality_metrics'):\n",
    "        for qc in wfr_info['output_quality_metrics']:\n",
    "            if qc.get('value'):\n",
    "                wfr_as_list.append(qc['value']['uuid'])\n",
    "    return wfr_as_list\n",
    "\n",
    "\n",
    "my_key = get_key('koray_data')\n",
    "for new_set, old_set in workon:\n",
    "    print()\n",
    "    old_set_info = ff_utils.get_metadata(old_set, my_key, add_on='frame=raw')\n",
    "    new_set_info = ff_utils.get_metadata(new_set, my_key, add_on='frame=raw')\n",
    "    if old_set_info['status'] == 'replaced':\n",
    "        print('old set already replaced, skipping')\n",
    "    print('Combining {} into {}'.format(old_set_info['accession'], new_set_info['accession']))\n",
    "    # assert new one is older the old one\n",
    "    assert conv_time(old_set_info['date_created']) < conv_time(new_set_info['date_created'])\n",
    "    # combine rep exps\n",
    "    new_rep = new_set_info['replicate_exps'] + old_set_info['replicate_exps']\n",
    "    new_rep = sorted(new_rep, key=lambda k: [k['bio_rep_no'],k['tec_rep_no']])\n",
    "    # assert unique bio tec reps\n",
    "    tec_bio = [str(i['bio_rep_no'])+'_'+str(i['tec_rep_no']) for i in new_rep]\n",
    "    try:\n",
    "        assert len(new_rep) == len(list(set(tec_bio)))\n",
    "    except AssertionError:\n",
    "        print('same rep numbers are used, either merged already happened, or conflicting numbers in both sets, skipping')\n",
    "        continue\n",
    "    ans = input('Continue with this rep numbers formatted b_t (y/n):\\n{}\\n'.format(tec_bio))\n",
    "    if ans != 'y':\n",
    "        break\n",
    "    # patch the new set with the new rep info\n",
    "    if action:\n",
    "        ff_utils.patch_metadata({'replicate_exps': new_rep}, new_set, my_key)\n",
    "        print(new_set, ' replicates are updated')\n",
    "    \n",
    "    # are there processed files/ other processed files and wfrs/qcs that need to be archived\n",
    "    # will collect items on processed_files and other_processed_files fields, and their asociated items\n",
    "    # (only 1 level of wfrs)\n",
    "    archive_files = []\n",
    "    if old_set_info.get('other_processed_files'):\n",
    "        for case in old_set_info['other_processed_files']:\n",
    "            archive_files.extend(case['files'])  # add all files to archive_list\n",
    "            case['type'] = 'archived'\n",
    "        if action:\n",
    "            ff_utils.patch_metadata({'other_processed_files': old_set_info['other_processed_files']}, old_set\n",
    "                                    , my_key)\n",
    "    if old_set_info.get('processed_files'):\n",
    "        archive_files.extend(old_set_info['processed_files'])\n",
    "    archive_list = []\n",
    "    for ar_file in archive_files:\n",
    "        archive_list.extend(fetch_pf_associated(ar_file, my_key))\n",
    "    print(len(archive_list), 'associated items will be archived')\n",
    "    for an_item in archive_list:\n",
    "        if action:\n",
    "            ff_utils.patch_metadata({'status': 'archived'}, an_item, my_key)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old header already in system\n",
      "new header already in system\n",
      "DONE\n",
      "CHECK THE OLD SET https://data.4dnucleome.org/experiment-set-replicates/51d9ac15-e55e-4f64-85e5-01998e349cf1/\n",
      "CHECK THE NEW SET https://data.4dnucleome.org/experiment-set-replicates/4DNESYTWHUH6/\n",
      "old header already in system\n",
      "new header already in system\n",
      "DONE\n",
      "CHECK THE OLD SET https://data.4dnucleome.org/experiment-set-replicates/998573fb-384e-4792-a621-254d705d4ec9/\n",
      "CHECK THE NEW SET https://data.4dnucleome.org/experiment-set-replicates/4DNES21D8SP8/\n"
     ]
    }
   ],
   "source": [
    "### PLEASE COPY NOTEBOOKS TO YOUR FOLDERS TO PREVENT COMMIT CONFLICTS\n",
    "\n",
    "# will perform patches/posts if set to true\n",
    "action = True\n",
    "# reason for replacement\n",
    "reason = 'new biological replicates were added'\n",
    "\n",
    "for new_set, old_set in workon:\n",
    "    old_set_info = ff_utils.get_metadata(old_set, my_key)\n",
    "    new_set_info = ff_utils.get_metadata(new_set, my_key)\n",
    "    old_acc = old_set_info['accession']\n",
    "    new_acc = new_set_info['accession']\n",
    "    old_status = old_set_info['status']\n",
    "    new_status = new_set_info['status']\n",
    "\n",
    "    # convert status for the static section\n",
    "    if old_status in ['in review by lab' , 'submission in proggress', 'pre-release']:\n",
    "        old_status = 'draft'\n",
    "    if new_status in ['in review by lab' , 'submission in proggress', 'pre-release']:\n",
    "        new_status = 'draft'\n",
    "\n",
    "    # add headers to old and new set\n",
    "    old_alias = \"static_header:replaced_item_{}_by_{}\".format(old_acc, new_acc)\n",
    "    old_header = {\n",
    "      \"body\": \"This experiment set was replaced by [{0}](https://data.4dnucleome.org/experiment-set-replicates/{0}/) because {1}.\".format(new_acc, reason),\n",
    "      \"award\": old_set_info['award']['uuid'],\n",
    "      \"lab\": old_set_info['lab']['uuid'],            \n",
    "      \"name\": \"static-header.replaced_item_{}\".format(old_acc),\n",
    "      \"section_type\": \"Item Page Header\",\n",
    "      \"options\": {\"title_icon\": \"info\", \"default_open\": True, \"filetype\": \"md\", \"collapsible\": False},\n",
    "      \"title\": \"Note: Replaced Item - {}\".format(old_acc),\n",
    "      \"status\": old_status,\n",
    "      \"aliases\": [old_alias]\n",
    "    }\n",
    "    new_alias = \"static_header:replacing_item_{}_old_{}\".format(new_acc, old_acc)\n",
    "    new_header = {\n",
    "      \"body\": \"This experiment set supercedes [{0}](https://data.4dnucleome.org/experiment-set-replicates/{1}/) because {2}.\".format(old_acc, old_set_info['uuid'], reason),\n",
    "      \"award\": new_set_info['award']['uuid'],\n",
    "      \"lab\": new_set_info['lab']['uuid'],\n",
    "      \"name\": \"static-header.replacing_item_{}\".format(new_acc),\n",
    "      \"section_type\": \"Item Page Header\",\n",
    "      \"options\": {\"title_icon\": \"info\", \"default_open\": True, \"filetype\": \"md\", \"collapsible\": False},\n",
    "      \"title\": \"Note: Replacing Item - {}\".format(new_acc),\n",
    "      \"status\": new_status,\n",
    "      \"aliases\": [new_alias]\n",
    "    }\n",
    "\n",
    "    if action:\n",
    "        # post the static sections\n",
    "        try:\n",
    "            old_h_resp = ff_utils.post_metadata(old_header, 'StaticSection', my_key)['@graph'][0]\n",
    "            print(old_h_resp)\n",
    "        except:\n",
    "            print('old header already in system')\n",
    "            old_h_resp = ff_utils.get_metadata(old_alias, my_key)\n",
    "\n",
    "        try:\n",
    "            new_h_resp = ff_utils.post_metadata(new_header, 'StaticSection', my_key)['@graph'][0]\n",
    "        except:\n",
    "            print('new header already in system')\n",
    "            new_h_resp = ff_utils.get_metadata(new_alias, my_key) \n",
    "\n",
    "        #see if existing headers\n",
    "        old_header_list = []\n",
    "        new_header_list = []\n",
    "        if old_set_info.get('static_headers'):\n",
    "            old_header_list = [i['uuid'] for i in old_set_info['static_headers']]\n",
    "        if new_set_info.get('static_headers'):\n",
    "            new_header_list = [i['uuid'] for i in new_set_info['static_headers']]\n",
    "        # add new ones to the list\n",
    "        if old_h_resp['uuid'] in old_header_list:\n",
    "            pass\n",
    "        else:\n",
    "            old_header_list.append(old_h_resp['uuid'])\n",
    "        if new_h_resp['uuid'] in new_header_list:\n",
    "            pass\n",
    "        else:\n",
    "            new_header_list.append(new_h_resp['uuid'])\n",
    "\n",
    "        # set the status of old set to replaced\n",
    "        ff_utils.patch_metadata({'status':'replaced', 'static_headers': old_header_list},\n",
    "                                obj_id=old_set_info['uuid'], key=my_key)\n",
    "        # wait for indexing to take place\n",
    "        # you might need to repeat this last piece separately if indexing does not catch up\n",
    "        # new status needs to be indexed for alternate accession to be patched\n",
    "        time.sleep(60)\n",
    "        # set the alternate accession on the new set to the old one\n",
    "        alt_ac = []\n",
    "        if new_set_info.get('alternate_accessions'):\n",
    "            alt_ac = new_set_info['alternate_accessions']\n",
    "        alt_ac.append(old_acc)\n",
    "        alt_ac = list(set(alt_ac))\n",
    "        ff_utils.patch_metadata({'alternate_accessions':alt_ac, 'static_headers': new_header_list},\n",
    "                                obj_id=new_set_info['uuid'], key=my_key)\n",
    "\n",
    "    print('DONE')\n",
    "    print('CHECK THE OLD SET', 'https://data.4dnucleome.org/experiment-set-replicates/{}/'.format(old_set_info['uuid']))\n",
    "    print('CHECK THE NEW SET', 'https://data.4dnucleome.org/experiment-set-replicates/{}/'.format(new_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_relase the sets\n",
    "for acc in ['4DNESYTWHUH6', '4DNES21D8SP8']:\n",
    "    ff_utils.patch_metadata({'status': 'pre-release'}, acc, my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
