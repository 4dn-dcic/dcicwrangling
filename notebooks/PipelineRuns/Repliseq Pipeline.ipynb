{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for repliseq exps\n",
    "exp_types = ['Repli-seq']\n",
    "set_url = '/search/?'+'&'.join(['experiments_in_set.experiment_type='+i for i in exp_types])+ \\\n",
    "          '&type=ExperimentSetReplicate&limit=all'\n",
    "print(set_url)\n",
    "run_sets = ff_utils.search_metadata(set_url ,key=my_auth)\n",
    "\n",
    "add_pc = False\n",
    "add_rel = False\n",
    "add_wfr = False\n",
    "\n",
    "counter = 0\n",
    "all_sets = len(run_sets)\n",
    "print(str(all_sets)+' total number of sets')\n",
    "\n",
    "run_sets = [i for i in run_sets if \"RepliSeq_Pipeline_0.2.5\"  not in i.get('completed_processes', [])]\n",
    "\n",
    "print(str(all_sets-len(run_sets))+ ' sets completed')\n",
    "\n",
    "\n",
    "for a_set in run_sets: \n",
    "    counter += 1\n",
    "    print()\n",
    "    fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size, lab = find_pairs(a_set, my_env, lookfor = 'single')\n",
    "    if not bwa_ref or not chrsize_ref:\n",
    "        print(counter, a_set['accession'], organism, enzyme, 'skipping set with not chrsize/bwa index')\n",
    "        continue  \n",
    "    print(counter, a_set['accession'], enzyme, organism)      \n",
    "    # cycle through the experiments\n",
    "    for exp in fastqpairs.keys():\n",
    "        if not fastqpairs.get(exp):\n",
    "            print(exp, 'does not have any fastq files')\n",
    "            continue\n",
    "        # Check Part 1 and See if all are okay\n",
    "        exp_bams = []\n",
    "        part1 = 'done'\n",
    "        # part2 = 'done'\n",
    "        for pair in fastqpairs[exp]:\n",
    "            #############\n",
    "            pair_resp = ff_utils.get_metadata(pair, key=my_auth)\n",
    "            attributions = get_attribution(pair_resp)\n",
    "            report = get_wfr_out(pair, 'repliseq-parta 0.2.5', my_auth)\n",
    "            # if run is not successful\n",
    "            if report['status'].startswith('no'):\n",
    "                part1 = 'not done'\n",
    "                if add_wfr:\n",
    "                    inp_f = {'fastq':pair, 'chromsizes':chrsize_ref, 'bwaIndex':bwa_ref}\n",
    "                    name_tag = pair.split('/')[2]\n",
    "                    run_missing_wfr(step_settings('repliseq-parta', organism, attributions), inp_f, name_tag,my_auth, my_env)\n",
    "            elif report['status'] == 'running':\n",
    "                part1 = 'still running'\n",
    "                print('part1 still running')\n",
    "            # if successful\n",
    "            else:\n",
    "                assert report['status'] == 'complete'\n",
    "                if add_pc:\n",
    "                    # TODO check if the files already in processed files field.\n",
    "                    # don't do it if they are already carried there.\n",
    "                    print('Adding files to preliminary tab')\n",
    "                    add_preliminary_processed_files(exp,\n",
    "                                                    [report['bam'], report['bg']],\n",
    "                                                    my_auth,\n",
    "                                                    run_type='repliseq')\n",
    "        # stop progress to part2 \n",
    "        if part1 is not 'done':\n",
    "            print(exp, 'has missing Part1 runs')\n",
    "            part2 = 'not ready'\n",
    "            part3 = 'not ready'\n",
    "            continue\n",
    "        print(exp, 'part1 complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "# get key from keypairs.json\n",
    "my_key = get_key('koray_data')\n",
    "\n",
    "# which sets to move data\n",
    "set_url = '/search/?experiments_in_set.experiment_type=Repli-seq&type=ExperimentSetReplicate&limit=all'\n",
    "\n",
    "\n",
    "# move other processed files to processed files field\n",
    "def move_opc_to_pc(resp, move_title, con_key):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        print('There are files in processed_files field, expected empty')\n",
    "        return False\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "            # update status of pc to status of set or exp\n",
    "            release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "run_sets = ff_utils.search_metadata(set_url , key=my_key)\n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "#move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "move_title = \"Repli-Seq Pipeline - Preliminary Files\"\n",
    "\n",
    "print(len(run_sets), 'experiment sets in scope')\n",
    "for a_set in run_sets:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_key, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_key)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_key, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_key)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repliseq pipeline part 1 also produces a qc object on the files.\n",
    "# Change their status to the ones of the files (if the files are in processed files field)\n",
    "\n",
    "set_url = '/search/?experiments_in_set.experiment_type=Repli-seq&type=ExperimentSetReplicate&limit=all'\n",
    "run_sets = ff_utils.search_metadata(set_url , key=my_key)\n",
    "\n",
    "def release_qc(file_id, con_key):\n",
    "    file_resp = ff_utils.get_metadata(file_id, key=con_key, add_on = 'frame=raw')\n",
    "    file_status = file_resp['status']\n",
    "    # skip if file is not released/archived\n",
    "    if file_status not in ['released', 'released to project', 'archived']:\n",
    "        return\n",
    "    qc = file_resp.get('quality_metric')\n",
    "    if qc:\n",
    "        qc_resp = ff_utils.get_metadata(qc, key=con_key)\n",
    "        qc_status = qc_resp['status']\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    if qc_status != file_status:\n",
    "        ff_utils.patch_metadata({'status': file_status}, obj_id=qc, key=con_key)\n",
    "        print(qc, 'qc object updated with status', file_status)\n",
    "        return True\n",
    "    \n",
    "print(len(run_sets), 'experiment sets in scope')\n",
    "for a_set in run_sets:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_key, add_on='frame=raw')\n",
    "    if set_resp.get('processed_files'):\n",
    "        for a_pf in set_resp['processed_files']:\n",
    "            release_qc(a_pf, my_key)\n",
    "            \n",
    "    exps = set_resp['experiments_in_set']\n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_key, add_on='frame=raw')\n",
    "        if exp_resp.get('processed_files'):\n",
    "            for a_pf_e in exp_resp['processed_files']:\n",
    "                release_qc(a_pf_e, my_key)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
