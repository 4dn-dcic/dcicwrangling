{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 total number of sets\n",
      "0 sets completed\n"
     ]
    }
   ],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n",
    "\n",
    "# url for repliseq exps\n",
    "exp_types = ['TSA-seq']\n",
    "set_url = '/search/?'+'&'.join(['experiments_in_set.experiment_type='+i for i in exp_types])+ \\\n",
    "          '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "          '&status=pre-release&status=released&status=released%20to%20project'\n",
    "run_sets = ff_utils.search_metadata(set_url ,key=my_auth)\n",
    "\n",
    "\n",
    "all_sets = len(run_sets)\n",
    "print(str(all_sets)+' total number of sets')\n",
    "\n",
    "run_sets = [i for i in run_sets if \"RepliSeq_Pipeline_v13.1_step1\"  not in i.get('completed_processes', [])]\n",
    "run_sets = [i for i in run_sets if \"RepliSeq_Pipeline_v14_step1\"  not in i.get('completed_processes', [])]\n",
    "run_sets = [i for i in run_sets if \"RepliSeq_Pipeline_v16_step1\"  not in i.get('completed_processes', [])]\n",
    "\n",
    "print(str(all_sets-len(run_sets))+ ' sets completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 4DNES1QS8EMG None human\n",
      "Adding files to preliminary tab\n",
      "Error - Cannot add files to opc\n",
      "The same title already in other processed files\n",
      "4DNEXI4G5LU1 part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEXWZJBI9S part1 complete\n",
      "\n",
      "2 4DNESURW4RWH None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXAPYJ8TG part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEXI3XYU13 part1 complete\n",
      "\n",
      "3 4DNESVTOWO4Z None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX2857G1D part1 complete\n",
      "\n",
      "4 4DNES3HEGJ6H None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXEUCV9IP part1 complete\n",
      "\n",
      "5 4DNES55KQ5VP None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXO5SP8YZ part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX8IQTP7F part1 complete\n",
      "\n",
      "6 4DNESA1Z2ZVR None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX4GY52ML part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX3QJPIL5 part1 complete\n",
      "\n",
      "7 4DNES3IB82SC None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXAIEW35N part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX84YQ6EU part1 complete\n",
      "\n",
      "8 4DNESWB19384 None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXI6TUAIS part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX4F3NXK9 part1 complete\n",
      "\n",
      "9 4DNESX7F2Q3N None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX8E9G2K4 part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX42V9PY2 part1 complete\n",
      "\n",
      "10 4DNES19URNAH None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXDS5V96B part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX2AGB4N9 part1 complete\n",
      "\n",
      "11 4DNES1SV4Q4D None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXE4L25S7 part1 complete\n",
      "\n",
      "12 4DNESVZYLFAN None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXFASHD3M part1 complete\n",
      "\n",
      "13 4DNESD1KO9LC None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXRFX2R3V part1 complete\n",
      "\n",
      "14 4DNESQBBEUUS None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXSUI5N4W part1 complete\n",
      "\n",
      "15 4DNESYP9HV8H None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX82FPIKJ part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX9FLI32I part1 complete\n",
      "\n",
      "16 4DNESNER8V11 None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX3PNLGUI part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEXNNWOPOB part1 complete\n",
      "\n",
      "17 4DNES5C2KRSP None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX9NQEGUK part1 complete\n",
      "\n",
      "18 4DNESNZGPYE6 None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXXIZBV5L part1 complete\n",
      "\n",
      "19 4DNES27KAIML None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXYX9TKCD part1 complete\n",
      "\n",
      "20 4DNESYJEVPEW None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXWI62POK part1 complete\n",
      "\n",
      "21 4DNESEFCYCWK None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXJAP2KV5 part1 complete\n",
      "\n",
      "22 4DNESTIK6339 None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX1KTP34O part1 complete\n",
      "\n",
      "23 4DNESMV75X4B None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXYU33PNF part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEXXYU4DM1 part1 complete\n",
      "\n",
      "24 4DNESS84SF2T None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXF61GJ21 part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEXLEEOBI5 part1 complete\n",
      "\n",
      "25 4DNES2UMO8MC None human\n",
      "Adding files to preliminary tab\n",
      "4DNEX8K1UUUM part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEXRFX4BJN part1 complete\n",
      "\n",
      "26 4DNESSVB91CQ None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXWB8JHOV part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX4NOT4N1 part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEX1OEFU4V part1 complete\n",
      "Adding files to preliminary tab\n",
      "4DNEXF7ND5NO part1 complete\n",
      "\n",
      "27 4DNESOSG1ECS None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXBFUWUUC part1 complete\n",
      "\n",
      "28 4DNES67IYWBB None human\n",
      "Adding files to preliminary tab\n",
      "4DNEXYDYLDXN part1 complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "add_pc = True\n",
    "add_tag = True\n",
    "add_wfr = False\n",
    "counter = 0\n",
    "\n",
    "# change winsize parameter and output file description\n",
    "winsize = 25000\n",
    "\n",
    "\n",
    "for a_set in run_sets: \n",
    "    counter += 1\n",
    "    \n",
    "    print()\n",
    "    # run in single mode\n",
    "    paired = \"\"\n",
    "    try:\n",
    "        fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size, lab = find_pairs(a_set, my_env, lookfor = 'single')\n",
    "        paired = 'No'\n",
    "    # run in paired mode\n",
    "    except:\n",
    "        fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size, lab = find_pairs(a_set, my_env)\n",
    "        paired = 'Yes'\n",
    "    if not bwa_ref or not chrsize_ref:\n",
    "        print(counter, a_set['accession'], organism, enzyme, 'skipping set with not chrsize/bwa index')\n",
    "        continue  \n",
    "    print(counter, a_set['accession'], enzyme, organism)      \n",
    "    # cycle through the experiments\n",
    "    for exp in fastqpairs.keys():\n",
    "        if not fastqpairs.get(exp):\n",
    "            print(exp, 'does not have any fastq files')\n",
    "            continue\n",
    "        # Check Part 1 and See if all are okay\n",
    "        exp_bams = []\n",
    "        part1 = 'done'\n",
    "        # part2 = 'done'\n",
    "        for pair in fastqpairs[exp]:\n",
    "            #############\n",
    "            if paired == 'No':\n",
    "                pair_resp = ff_utils.get_metadata(pair, key=my_auth)\n",
    "                report = get_wfr_out(pair, 'repliseq-parta', my_auth, ['v13.1', 'v14', 'v16'])\n",
    "            if paired == 'Yes':\n",
    "                pair_resp = ff_utils.get_metadata(pair[0], key=my_auth)\n",
    "                report = get_wfr_out(pair[0], 'repliseq-parta', my_auth, ['v13.1', 'v14', 'v16'])\n",
    "                \n",
    "            attributions = get_attribution(pair_resp)\n",
    "            # if run is not successful\n",
    "            if report['status'].startswith('no'):\n",
    "                part1 = 'not done'\n",
    "                if add_wfr:\n",
    "                    if paired == 'Yes':\n",
    "                        inp_f = {'fastq':pair[0], 'fastq2': pair[1], 'chromsizes':chrsize_ref, 'bwaIndex':bwa_ref}\n",
    "                        name_tag = pair[0].split('/')[2] + '_' + pair[1].split('/')[2]\n",
    "                    if paired == 'No':\n",
    "                        inp_f = {'fastq':pair, 'chromsizes':chrsize_ref, 'bwaIndex':bwa_ref}\n",
    "                        name_tag = pair.split('/')[2]\n",
    "\n",
    "                    # run trsa on winsize\n",
    "                    params = {'winsize':winsize}\n",
    "                    step_set = step_settings('repliseq-parta', organism, attributions, params=params)\n",
    "                    step_set['custom_pf_fields']['count_bg']['description'] = 'read counts per {} kb bin, unfiltered, unnormalized'.format(int(winsize/1000)) \n",
    "                    run_missing_wfr(step_set, inp_f, name_tag,my_auth, my_env)\n",
    "                else:\n",
    "                    print(exp, 'has missing Part1 runs')\n",
    "                    \n",
    "            elif report['status'] == 'running':\n",
    "                part1 = 'still running'\n",
    "                print(exp, 'part1 still running')\n",
    "            # if successful\n",
    "            else:\n",
    "                assert report['status'] == 'complete'\n",
    "                if add_pc:\n",
    "                    # TODO check if the files already in processed files field.\n",
    "                    # don't do it if they are already carried there.\n",
    "                    print('Adding files to preliminary tab')\n",
    "                    add_preliminary_processed_files(exp,\n",
    "                                                    [report['bam'], report['bg']],\n",
    "                                                    my_auth,\n",
    "                                                    run_type='repliseq')\n",
    "                if add_tag:\n",
    "                    # TODO make this a function                    \n",
    "                    res_file_resp = ff_utils.get_metadata(report['bam'], my_auth)\n",
    "                    producing_wfr = res_file_resp['workflow_run_outputs'][0]['display_title']\n",
    "                    wfr_type, time_info = producing_wfr.split(' run ')\n",
    "                    wfr_type_base, wfr_version = wfr_type.strip().split(' ')\n",
    "                    assert wfr_version in ['v13.1', 'v14', 'v16']\n",
    "                    tag = 'RepliSeq_Pipeline_{}_step1'.format(wfr_version)\n",
    "                    updated_tag = a_set.get('completed_processes', [])\n",
    "                    updated_tag.append(tag)\n",
    "                    updated_tag = list(set(updated_tag))\n",
    "                    ff_utils.patch_metadata({\"completed_processes\":updated_tag}, obj_id=a_set['accession'] , key=my_auth)\n",
    "                     \n",
    "        # stop progress to part2 \n",
    "        if part1 is not 'done':\n",
    "            part2 = 'not ready'\n",
    "            part3 = 'not ready'\n",
    "            continue\n",
    "        print(exp, 'part1 complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "action = False\n",
    "\n",
    "# get key from keypairs.json\n",
    "my_key = get_key('koray_data')\n",
    "# which sets to move data\n",
    "set_url = '/search/?completed_processes=RepliSeq_Pipeline_V16_step1%27&completed_processes=RepliSeq_Pipeline_v16_step1&experiments_in_set.experiment_type=Repli-seq&limit=all&type=ExperimentSetReplicate'\n",
    "run_sets = ff_utils.search_metadata(set_url, my_key)\n",
    "print(len(run_sets))\n",
    "# move other processed files to processed files field\n",
    "def move_opc_to_pc(resp, move_title, con_key):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        print('There are files in processed_files field, expected empty')\n",
    "        return False\n",
    "    # are there files in opc\n",
    "    if not opc:\n",
    "        print('there are no other processed files, skipping')\n",
    "        return False\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action: \n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "#move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "move_title = \"Repli-Seq Pipeline - Preliminary Files\"\n",
    "\n",
    "print(len(run_sets), 'experiment sets in scope')\n",
    "for a_set in run_sets:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_key, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_key)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_key, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_key)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repliseq pipeline part 1 also produces a qc object on the files.\n",
    "# Change their status to the ones of the files (if the files are in processed files field)\n",
    "\n",
    "# set_url = '/search/?experiments_in_set.experiment_type=Repli-seq&type=ExperimentSetReplicate&limit=all'\n",
    "\n",
    "# set_url = '/search/?type=ExperimentSetReplicate&experiments_in_set.experiment_type=Repli-seq&experiments_in_set.biosample.biosource.individual.organism.name=mouse'\n",
    "\n",
    "\n",
    "# run_sets = ff_utils.search_metadata(set_url , key=my_key)\n",
    "\n",
    "def release_qc(file_id, con_key):\n",
    "    file_resp = ff_utils.get_metadata(file_id, key=con_key, add_on = 'frame=raw')\n",
    "    file_status = file_resp['status']\n",
    "    # skip if file is not released/archived\n",
    "    if file_status not in ['released', 'released to project', 'archived']:\n",
    "        return\n",
    "    qc = file_resp.get('quality_metric')\n",
    "    if qc:\n",
    "        qc_resp = ff_utils.get_metadata(qc, key=con_key)\n",
    "        qc_status = qc_resp['status']\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    if qc_status != file_status:\n",
    "        ff_utils.patch_metadata({'status': file_status}, obj_id=qc, key=con_key)\n",
    "        print(qc, 'qc object updated with status', file_status)\n",
    "        return True\n",
    "    \n",
    "print(len(run_sets), 'experiment sets in scope')\n",
    "for a_set in run_sets:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_key, add_on='frame=raw')\n",
    "    if set_resp.get('processed_files'):\n",
    "        for a_pf in set_resp['processed_files']:\n",
    "            release_qc(a_pf, my_key)\n",
    "            \n",
    "    exps = set_resp['experiments_in_set']\n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_key, add_on='frame=raw')\n",
    "        if exp_resp.get('processed_files'):\n",
    "            for a_pf_e in exp_resp['processed_files']:\n",
    "                release_qc(a_pf_e, my_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
