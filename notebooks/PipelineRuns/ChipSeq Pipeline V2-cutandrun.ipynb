{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 total number of sets\n",
      "0 sets completed\n",
      "51 ready for processing\n"
     ]
    }
   ],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n",
    "\n",
    "# all usable env names\n",
    "all_envs = ['data', 'staging', 'fourfront-webdev', 'fourfront-mastertest', 'fourfront-hotseat']\n",
    "# #get admin key from s3\n",
    "# my_env = 'fourfront-webdev'\n",
    "# my_auth = ff_utils.get_authentication_with_server({}, ff_env = my_env)\n",
    "\n",
    "\n",
    "\n",
    "set_url = '/search/?experiments_in_set.experiment_type=CUT%26RUN&type=ExperimentSetReplicate&limit=all&status=pre-release&status=released&status=released%20to%20project'\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "\n",
    "run_sets = [i for i in all_sets if \"CHIP_Pipeline_0.2.5\"  not in i.get('completed_processes', [])]\n",
    "print(len(all_sets), 'total number of sets')\n",
    "print(len(all_sets)-len(run_sets), 'sets completed')\n",
    "print(len(run_sets), 'ready for processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "all files ['a7980683-3c11-4b53-8848-1efdf4cd89ef', '7fb0a920-e6f8-4a6b-9b35-123bd94618dd', 'a6a6ab8a-3a42-4680-8e0a-77420ff1a6f9', 'b3715bbf-478e-4ef9-b52d-2d8334c26fca', '7e9f797c-7d90-4a6e-8edd-ca06e7078da4', 'efa65615-20e8-4232-a531-14a3f607e139']\n",
      "a7980683-3c11-4b53-8848-1efdf4cd89ef 4DNFISUSBDGO\n",
      "ind file ['a7980683-3c11-4b53-8848-1efdf4cd89ef', '7fb0a920-e6f8-4a6b-9b35-123bd94618dd']\n",
      "7fb0a920-e6f8-4a6b-9b35-123bd94618dd 4DNFI5S8S7EP\n",
      "skipped 2\n",
      "a6a6ab8a-3a42-4680-8e0a-77420ff1a6f9 4DNFILAYBB16\n",
      "ind file ['a6a6ab8a-3a42-4680-8e0a-77420ff1a6f9', 'b3715bbf-478e-4ef9-b52d-2d8334c26fca']\n",
      "b3715bbf-478e-4ef9-b52d-2d8334c26fca 4DNFIJCVPSP4\n",
      "skipped 2\n",
      "7e9f797c-7d90-4a6e-8edd-ca06e7078da4 4DNFIBIPUMNT\n",
      "ind file ['7e9f797c-7d90-4a6e-8edd-ca06e7078da4', 'efa65615-20e8-4232-a531-14a3f607e139']\n",
      "efa65615-20e8-4232-a531-14a3f607e139 4DNFIKXLPRKD\n",
      "skipped 2\n",
      "files are [[['a7980683-3c11-4b53-8848-1efdf4cd89ef', '7fb0a920-e6f8-4a6b-9b35-123bd94618dd'], ['a6a6ab8a-3a42-4680-8e0a-77420ff1a6f9', 'b3715bbf-478e-4ef9-b52d-2d8334c26fca'], ['7e9f797c-7d90-4a6e-8edd-ca06e7078da4', 'efa65615-20e8-4232-a531-14a3f607e139']]]\n",
      "\n",
      "4DNESC5FAH5U human paired False 4DNESHSQFMSM\n",
      "[[['a7980683-3c11-4b53-8848-1efdf4cd89ef', '7fb0a920-e6f8-4a6b-9b35-123bd94618dd'], ['a6a6ab8a-3a42-4680-8e0a-77420ff1a6f9', 'b3715bbf-478e-4ef9-b52d-2d8334c26fca'], ['7e9f797c-7d90-4a6e-8edd-ca06e7078da4', 'efa65615-20e8-4232-a531-14a3f607e139']]]\n"
     ]
    }
   ],
   "source": [
    "run_wfr = True\n",
    "add_pc = False\n",
    "add_tag = False\n",
    "\n",
    "\n",
    "def get_chip_files(replicate_exps, my_auth):\n",
    "    files = []\n",
    "    obj_key = []\n",
    "    paired = \"\"\n",
    "    for i in replicate_exps:\n",
    "        exp = i['replicate_exp']['uuid']\n",
    "        exp_resp = ff_utils.get_metadata(exp, my_auth)\n",
    "        # get exp files\n",
    "        exp_files = exp_resp['files']\n",
    "        print('all files', [i['uuid'] for i in exp_files])\n",
    "        b_index = int(i['bio_rep_no']) - 1\n",
    "        t_index = int(i['tec_rep_no']) - 1\n",
    "        for a_file in exp_files:\n",
    "            print(a_file['uuid'], a_file['accession'])\n",
    "            f_t = []\n",
    "            o_t = []\n",
    "            file_resp = ff_utils.get_metadata(a_file['uuid'], my_auth)\n",
    "            # get pair end no\n",
    "            pair_end = file_resp.get('paired_end')\n",
    "            if pair_end == '2':\n",
    "                paired = 'paired'\n",
    "                print('skipped 2')\n",
    "                continue\n",
    "            # get paired file\n",
    "            paired_with = \"\"\n",
    "            relations = file_resp.get('related_files')\n",
    "            if not relations:\n",
    "                pass\n",
    "            else:\n",
    "                for relation in relations:\n",
    "                    if relation['relationship_type'] == 'paired with':\n",
    "                        paired = 'paired'\n",
    "                        paired_with = relation['file']['uuid']\n",
    "            # decide if data is not paired end reads\n",
    "            if not paired_with:\n",
    "                if not paired:\n",
    "                    paired = 'single'\n",
    "                else:\n",
    "                    if paired != 'single':\n",
    "                        print('inconsistent fastq pair info')\n",
    "                        continue\n",
    "                f_t.append(file_resp['uuid'])\n",
    "                o_t.append(file_resp['display_title'])\n",
    "            else:\n",
    "                f2 = ff_utils.get_metadata(paired_with, my_auth)\n",
    "                f_t.append(file_resp['uuid'])\n",
    "                o_t.append(file_resp['display_title'])\n",
    "                f_t.append(f2['uuid'])\n",
    "                o_t.append(f2['display_title'])\n",
    "            try:\n",
    "                files[b_index]\n",
    "            except:\n",
    "                files.append([])\n",
    "            files[b_index].append(f_t)\n",
    "            #files[b_index][t_index] = f_t\n",
    "            try:\n",
    "                obj_key[b_index]\n",
    "            except:\n",
    "                obj_key.append([])\n",
    "            obj_key[b_index].append(o_t)\n",
    "            #obj_key[b_index][t_index] = o_t\n",
    "            print('ind file', f_t)\n",
    "        print('files are', files)\n",
    "        print()\n",
    "    return files, obj_key, paired\n",
    "\n",
    "\n",
    "for a_set in run_sets: \n",
    "    print()\n",
    "    counter += 1\n",
    "    # some feature to extract from each set\n",
    "    control = \"\"  # True or False (True if set in scope is control)\n",
    "    control_set = \"\"  # None (if no control exp is set), or the control experiment for the one in scope\n",
    "    target_type = \"\" # Histone or TF (or None for control)\n",
    "    paired = \"\" # single or paired\n",
    "    organism = \"\"\n",
    "    \n",
    "    # pass attributions to new objects\n",
    "    attributions = None\n",
    "    \n",
    "    replicate_exps = a_set['replicate_exps']\n",
    "    replicate_exps = sorted(replicate_exps, key = lambda x: [x['bio_rep_no'], x['tec_rep_no']])\n",
    "\n",
    "    # get organism, target and control from the first replicate\n",
    "    f_exp = replicate_exps[0]['replicate_exp']['uuid']\n",
    "    f_exp_resp = ff_utils.get_metadata(f_exp, key = my_auth)\n",
    "    control, control_set, target_type, organism = get_chip_info(f_exp_resp, my_auth)\n",
    "\n",
    "    # Get list of files and object keys in triple nesting\n",
    "    # sample = [[[b1t1,b1t1],[b1t2,b1t2]],[[b2t1,b2t1],[b2t2,b2t2]]]\n",
    "    files, obj_key, paired = get_chip_files(replicate_exps, my_auth)\n",
    "    \n",
    "    print(a_set['accession'], organism, paired, control, control_set)  \n",
    "    print(files)\n",
    "    break\n",
    "    \n",
    "    if organism not in ['mouse', 'human']:\n",
    "        print('orgamism not ready', organism)\n",
    "        continue\n",
    "\n",
    "    attributions = get_attribution(ff_utils.get_metadata(files[0][0][0], key = my_auth))\n",
    "    \n",
    "    if not control:\n",
    "        if not target_type:\n",
    "            print('set is not control, but missing target type, skipping')\n",
    "            continue\n",
    "\n",
    "    ta = []\n",
    "    taxcor = []\n",
    "    # check for step1, and start if missing\n",
    "    step1_status = 'Done'\n",
    "    for it in range(len(files)):\n",
    "        run_name = a_set['accession'] + '_B' + str(it+1)\n",
    "        print(run_name, end = ' status: ')\n",
    "        bio_rep_files = [files[it]]\n",
    "        bio_rep_obj = [obj_key[it]]\n",
    "        if control:\n",
    "            step1_result = get_wfr_out_file(bio_rep_files[0][0][0], 'encode-chipseq-aln-ctl 0.2.5', my_auth)\n",
    "            print(step1_result['status'])\n",
    "            if step1_result['status'] == 'complete':\n",
    "                if add_pc:\n",
    "                    add_chip_processed_files_exp([step1_result['chip.first_ta_ctl']], my_auth)\n",
    "            elif step1_result['status'] == 'running':\n",
    "                step1_status = 'Incomplete'\n",
    "            else:\n",
    "                step1_status = 'Incomplete'                \n",
    "                if run_wfr:\n",
    "                    print('starting run')\n",
    "                    run_missing_chip1(control, step_settings('encode-chipseq-aln-ctl', organism, attributions), \n",
    "                                      organism, 'tf', paired, bio_rep_files, bio_rep_obj, my_env, my_auth, run_name)\n",
    "        else:\n",
    "            step1_result = get_wfr_out_file(bio_rep_files[0][0][0], 'encode-chipseq-aln-chip 0.2.5', my_auth) \n",
    "            print(step1_result['status'])\n",
    "            if step1_result['status'] == 'complete':\n",
    "                ta.append(step1_result['chip.first_ta'])\n",
    "                taxcor.append(step1_result['chip.first_ta_xcor'])\n",
    "                if add_pc:\n",
    "                    add_chip_processed_files_exp([step1_result['chip.first_ta']], my_auth)\n",
    "            elif step1_result['status'] == 'running':\n",
    "                step1_status = 'Incomplete'\n",
    "            else:\n",
    "                step1_status = 'Incomplete'\n",
    "                if run_wfr:\n",
    "                    print('starting run')\n",
    "                    run_missing_chip1(control, step_settings('encode-chipseq-aln-chip', organism, attributions), \n",
    "                                      organism, target_type, paired, bio_rep_files, bio_rep_obj, my_env, my_auth, run_name)\n",
    "\n",
    "                    \n",
    "    if step1_status != 'Done':\n",
    "        continue\n",
    "        \n",
    "    # add pc files and tag for control\n",
    "    if control:\n",
    "        #add competed flag to set\n",
    "        if add_tag:\n",
    "            ff_utils.patch_metadata({\"completed_processes\":[\"CHIP_Pipeline_0.2.5\"]}, obj_id=a_set['accession'] , key=my_auth)\n",
    "        # add processed files to set\n",
    "        continue\n",
    "    \n",
    "    print('set ready for part2')\n",
    "    # check if control is ready\n",
    "    #cont_set_info = ff_utils.get_metadata(control_set, key=my_key)\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3 4DNESH4UTRNL DpnII mouse 800  -  4DNEX4KRGMAQ is missing Part2 - Fails at runtaskawsem\n",
    "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:hi-c-processing-bam_4DNEX4KRGMAQ9b484528-5651-4d72-a271-a9b37a42ab05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "recipe_no = 1\n",
    "exp_type, step3 = recipe[recipe_no]\n",
    "action = False\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "set_url = '/search/?'+ \\\n",
    "          '&'.join(['experiments_in_set.experiment_type='+i for i in exp_type])+ \\\n",
    "          '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "          '&status=released&status=released%20to%20project'\n",
    "\n",
    "print(set_url)\n",
    "set_url = '/search/?award.project=4DN&experimentset_type=replicate&lab.display_title=Chuck+Murry%2C+UW&status=pre-release&type=ExperimentSetReplicate'\n",
    "\n",
    "# exp\n",
    "# set_url = '/search/?'+ \\\n",
    "#           '&'.join(['experiment_type='+i for i in exp_type])+ \\\n",
    "#           '&type=Experiment&limit=all' + \\\n",
    "#           '&status=released&status=released%20to%20project'\n",
    "\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "\n",
    "ready_sets_1 = [i for i in all_sets if \"HiC_Pipeline_0.2.5\" in i.get('completed_processes', [])]\n",
    "print(len(ready_sets_1))\n",
    "ready_sets_2 = []\n",
    "for a_set in ready_sets_1:\n",
    "    if a_set.get('other_processed_files'):\n",
    "        print('a')\n",
    "        print(a_set['accession'])\n",
    "        if move_title in [i['title'] for i in a_set['other_processed_files']]:\n",
    "            print('b')\n",
    "            if a_set.get('processed_files'):\n",
    "                print('c')\n",
    "                print('WARN' ,a_set['accession'], 'has items in processed files, skipping ')\n",
    "                continue\n",
    "            else:\n",
    "                ready_sets_2.append(a_set)\n",
    "print(len(ready_sets_2), 'items are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move other processed files to processed files field\n",
    "action = True\n",
    "def move_opc_to_pc(resp, move_title, con_key):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        if opc:\n",
    "            print('There are files in processed_files field, expected empty', resp['accession'])\n",
    "            return False\n",
    "        else:\n",
    "            print('it is possible that move already happened, no opc but pc', resp['accession'])\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action:\n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "print(len(ready_sets_2), 'experiment sets in scope')\n",
    "for a_set in ready_sets_2:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_auth, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_auth)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_auth, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_auth)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
