{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n",
    "\n",
    "# different types of exps use different steps at the last step(3).\n",
    "recipe = [\n",
    "    [[\n",
    "      'in%20situ%20Hi-C', \n",
    "      'dilution%20Hi-C'\n",
    "     ], 'hi-c-processing-pairs'],\n",
    "    \n",
    "    [[#'micro-C',          \n",
    "      'DNase%20Hi-C'\n",
    "     ], 'hi-c-processing-pairs-nore'], # 2 low q dnase hi sets\n",
    "    \n",
    "    [['capture%20Hi-C',   'PLAC-seq'],        'hi-c-processing-pairs-nonorm'],\n",
    "    [['ChIA-PET',         'TrAC-loop'],       'hi-c-processing-pairs-nore-nonorm'],\n",
    "    [['TCC'], 'hi-c-processing-pairs'],\n",
    "    \n",
    "]\n",
    "\n",
    "# To Do assign core 8 and more memory (\"instance_type\": \"c4.4xlarge\",) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/search/?experiments_in_set.experiment_type=DNase%20Hi-C&type=ExperimentSetReplicate&limit=all&status=pre-release&status=released&status=released%20to%20project\n",
      "21 total number of sets\n",
      "19 sets completed\n",
      "0 sets skipped for small size\n",
      "2 ready for processing\n",
      "\n",
      "1 4DNESAGFJ56V DNaseI mouse 5\n",
      "4DNEX5MUCNOA part1 complete\n",
      "4DNEX5MUCNOA part2 complete\n",
      "4DNESAGFJ56V is missing Part3\n",
      "\n",
      "2 4DNESW72J62T DNaseI human 26\n",
      "4DNEX2BP2PR5 part1 complete\n",
      "4DNEX2BP2PR5 part2 complete\n",
      "4DNEXWPS7HX4 part1 complete\n",
      "4DNEXWPS7HX4 part2 complete\n",
      "4DNESW72J62T is missing Part3\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## TODO make sure set_url is compatible with the set_url sets\n",
    "\n",
    "#Choose the recipe element to run the pipeline on\n",
    "recipe_no = 1\n",
    "exp_type, step3 = recipe[recipe_no]\n",
    "\n",
    "#Choose the type of operations you want\n",
    "add_pc = False    #add processed files to 'other processed files\n",
    "add_tag = False   #add the completed process tag if done with all steps\n",
    "add_wfr = False   #start missing wfrs\n",
    "add_tag_small = False   # add skipped small tag\n",
    "\n",
    "set_url = '/search/?'+ \\\n",
    "          '&'.join(['experiments_in_set.experiment_type='+i for i in exp_type])+ \\\n",
    "          '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "          '&status=pre-release&status=released&status=released%20to%20project'\n",
    "\n",
    "#print(set_url)\n",
    "\n",
    "# set_url = '/search/?experiments_in_set.experiment_type=DNase+Hi-C&experimentset_type=replicate&lab.display_title=Chuck+Murry%2C+UW&type=ExperimentSetReplicate'\n",
    "# set_url = '/search/?award.project=4DN&experiments_in_set.experiment_type=PLAC-seq&experimentset_type=replicate&status=pre-release&type=ExperimentSetReplicate'\n",
    "#set_url = '/search/?award.project=External&experiments_in_set.experiment_type=in+situ+Hi-C&experimentset_type=replicate&lab.display_title=Rafael+Casellas%2C+NIH&type=ExperimentSetReplicate'\n",
    "print(set_url)\n",
    "#set_url = '/search/?award.project=ENCODE&experimentset_type=replicate&type=ExperimentSetReplicate'\n",
    "\n",
    "#set_url = '/search/?award.project=4DN&experiments_in_set.biosample.biosource.individual.organism.name=fruit-fly&experimentset_type=replicate&type=ExperimentSetReplicate'\n",
    "#print set_url\n",
    "\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "\n",
    "uncompleted_sets = [i for i in all_sets if \"HiC_Pipeline_0.2.6\"  not in i.get('completed_processes', [])]\n",
    "run_sets = [i for i in uncompleted_sets if \"HiC_Pipeline_0.2.6_skipped-small-set\"  not in i.get('completed_processes', [])]\n",
    "run_sets = [i for i in run_sets if \"HiC_Pipeline_0.2.7\"  not in i.get('completed_processes', [])]\n",
    "\n",
    "\n",
    "\n",
    "for i in run_sets:\n",
    "    if i.get('completed_processes'):\n",
    "        print(i['accession'], i['completed_processes'])\n",
    "print(len(all_sets), 'total number of sets')\n",
    "print(len(all_sets)-len(uncompleted_sets), 'sets completed')\n",
    "print(len(uncompleted_sets)-len(run_sets), 'sets skipped for small size')\n",
    "print(len(run_sets), 'ready for processing')\n",
    "\n",
    "\n",
    "for a_set in run_sets: \n",
    "    \n",
    "\n",
    "    attributions = None\n",
    "    print()\n",
    "    counter += 1\n",
    "        \n",
    "    fastqpairs, organism, enzyme, bwa_ref, chrsize_ref, enz_ref, f_size, lab = find_pairs(a_set, my_env)\n",
    "    # skip based on these conditions\n",
    "    if not bwa_ref or not chrsize_ref:\n",
    "        print(counter, a_set['accession'], organism, enzyme, 'skipping set with no chrsize/bwa index')\n",
    "        continue\n",
    "    if 'nonorm' not in step3:\n",
    "        if f_size < 4:\n",
    "            print(counter, a_set['accession'], 'skipping small file size', str(f_size))\n",
    "            if add_tag_small:\n",
    "                ff_utils.patch_metadata({\"completed_processes\":[\"HiC_Pipeline_0.2.6_skipped-small-set\"]}, obj_id=a_set['accession'] , key=my_auth)\n",
    "                \n",
    "            continue\n",
    "    if 'nore' not in step3:\n",
    "        if not enz_ref:\n",
    "            print(counter, a_set['accession'], 'skipping not ready NZ', organism, enzyme)\n",
    "            continue\n",
    "    \n",
    "    print(counter, a_set['accession'],enzyme, organism,f_size)\n",
    "    part3 = 'done'\n",
    "    set_pairs = []        \n",
    "    # cycle through the experiments, skip the ones without usable files\n",
    "    for exp in fastqpairs.keys():\n",
    "        if not fastqpairs.get(exp):\n",
    "            print(exp, 'does not have any fastq pairs')\n",
    "            continue\n",
    "        # Check Part 1 and See if all are okay\n",
    "        exp_bams = []\n",
    "        part1 = 'done'\n",
    "        part2 = 'done'\n",
    "        \n",
    "        for pair in fastqpairs[exp]:\n",
    "            #############\n",
    "            if not attributions:\n",
    "                attributions = get_attribution(ff_utils.get_metadata(pair[0], key = my_auth))\n",
    "            step1_result = get_wfr_out(pair[0], 'bwa-mem', my_auth, ['0.2.6'])\n",
    " \n",
    "            # if successful\n",
    "            if step1_result['status'] == 'complete':\n",
    "                exp_bams.append(step1_result['bam'])\n",
    "                continue\n",
    "            # if still running\n",
    "            elif step1_result['status'] == 'running':\n",
    "                part1 = 'not done'\n",
    "                print('part1 still running')\n",
    "                continue\n",
    "            # if run is not successful\n",
    "            else:\n",
    "                part1 = 'not done'\n",
    "                if add_wfr:\n",
    "                    # RUN PART 1\n",
    "                    inp_f = {'fastq1':pair[0], 'fastq2':pair[1], 'bwa_index':bwa_ref}\n",
    "                    name_tag = pair[0].split('/')[2]+'_'+pair[1].split('/')[2]\n",
    "                    run_missing_wfr(step_settings('bwa-mem', organism, attributions), inp_f, name_tag, my_auth, my_env)\n",
    "        # stop progress to part2 \n",
    "        if part1 is not 'done':\n",
    "            print(exp, 'has missing Part1 runs')\n",
    "            part2 = 'not ready'\n",
    "            part3 = 'not ready'\n",
    "            continue\n",
    "        print(exp, 'part1 complete')\n",
    "           \n",
    "        #make sure all input bams went through same last step2\n",
    "        all_step2s = []\n",
    "        for bam in exp_bams:\n",
    "            step2_result = get_wfr_out(bam, 'hi-c-processing-bam', my_auth, ['0.2.6'])\n",
    "            all_step2s.append((step2_result['status'],step2_result.get('bam')))\n",
    "        if len(list(set(all_step2s))) != 1:\n",
    "            print('inconsistent step2 run for input bams')\n",
    "            # this run will be repeated if add_wfr\n",
    "            step2_result['status'] = 'inconsistent run'\n",
    "            \n",
    "        #check if part 2 is run already, it not start the run\n",
    "        # if successful\n",
    "        if step2_result['status'] == 'complete':\n",
    "            set_pairs.append(step2_result['pairs'])\n",
    "            if add_pc:\n",
    "                add_preliminary_processed_files(exp, [step2_result['bam'],step2_result['pairs']], my_auth)\n",
    "            print(exp, 'part2 complete')\n",
    "            continue\n",
    "        # if still running\n",
    "        elif step2_result['status'] == 'running':\n",
    "            part2 = 'not done'\n",
    "            part3 = 'not ready'\n",
    "            print(exp, 'part2 still running')\n",
    "            continue\n",
    "        # if run is not successful\n",
    "        else:\n",
    "            part2 = 'not done'\n",
    "            part3 = 'not ready'\n",
    "            print(exp, 'is missing Part2')\n",
    "            if add_wfr:\n",
    "                # RUN PART 2\n",
    "                inp_f = {'input_bams':exp_bams, 'chromsize':chrsize_ref}           \n",
    "                run_missing_wfr(step_settings('hi-c-processing-bam', organism, attributions), inp_f, exp, my_auth, my_env) \n",
    "\n",
    "                \n",
    "    if part3 is not 'done':\n",
    "        print('Part3 not ready')\n",
    "        continue\n",
    "    if not set_pairs:\n",
    "        print('no pairs can be produced from this set')\n",
    "        continue\n",
    "\n",
    "    #make sure all input bams went through same last step3\n",
    "    all_step3s = []\n",
    "    for a_pair in set_pairs:\n",
    "        step3_result = get_wfr_out(a_pair, step3, my_auth, ['0.2.6', '0.2.7'])\n",
    "        all_step3s.append((step3_result['status'], step3_result.get('mcool')))\n",
    "    if len(list(set(all_step3s))) != 1:\n",
    "        print('inconsistent step3 run for input pairs')\n",
    "        # this run will be repeated if add_wfr\n",
    "        step3_result['status'] = 'inconsistent run'\n",
    "    #check if part 3 is run already, it not start the run\n",
    "    # if successful\n",
    "    if step3_result['status'] == 'complete':\n",
    "        completed += 1\n",
    "        completed_acc.append(a_set['accession'])\n",
    "        #add competed flag to experiment\n",
    "        if add_tag:\n",
    "            ff_utils.patch_metadata({\"completed_processes\":[\"HiC_Pipeline_0.2.7\"]}, obj_id=a_set['accession'] , key=my_auth)\n",
    "        # add processed files to set\n",
    "        if add_pc:\n",
    "            add_preliminary_processed_files(a_set['accession'], \n",
    "                                            [step3_result['pairs'],\n",
    "                                             step3_result['hic'],\n",
    "                                             step3_result['mcool']], \n",
    "                                            my_auth)\n",
    "        print(a_set['accession'], 'part3 complete')\n",
    "    # if still running\n",
    "    elif step3_result['status'] == 'running':\n",
    "        print('part3 still running')\n",
    "        continue\n",
    "    # if run is not successful\n",
    "    else:\n",
    "        print(a_set['accession'], 'is missing Part3')\n",
    "        if add_wfr:\n",
    "            # RUN PART 3\n",
    "            inp_f = {'input_pairs':set_pairs, 'chromsizes':chrsize_ref}\n",
    "            if recipe_no in [0,2,4]:\n",
    "                inp_f['restriction_file'] = enz_ref\n",
    "            run_missing_wfr(step_settings(step3, organism, attributions), inp_f, a_set['accession'], my_auth, my_env)\n",
    "\n",
    "print(completed)\n",
    "print(completed_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3 4DNESH4UTRNL DpnII mouse 800  -  4DNEX4KRGMAQ is missing Part2 - Fails at runtaskawsem\n",
    "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:hi-c-processing-bam_4DNEX4KRGMAQ9b484528-5651-4d72-a271-a9b37a42ab05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6_skipped-small-set']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.7']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.7']\n",
      "['HiC_Pipeline_0.2.6']\n",
      "['HiC_Pipeline_0.2.7']\n",
      "9\n",
      "4DNESCQZYHLJ\n",
      "4DNESUZCBCG9\n",
      "4DNESFNBTRO3\n",
      "4DNESHF65RC4\n",
      "4DNESKLFZ31S\n",
      "4DNES4GDH4BG\n",
      "4DNES3O1B45O\n",
      "4DNESE1VMAMD\n",
      "4DNES7ODZ4MZ\n",
      "9 items are ready\n"
     ]
    }
   ],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "recipe_no = 0\n",
    "exp_type, step3 = recipe[recipe_no]\n",
    "action = False\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "set_url = '/search/?experiments_in_set.experiment_type.title=in+situ+Hi-C&experimentset_type=replicate&publications_of_set.uuid=89ec268b-d05c-4841-8806-a49fb0d99e78&type=ExperimentSetReplicate'\n",
    "all_sets = ff_utils.search_metadata(set_url, my_auth)\n",
    "print(len(all_sets))\n",
    "for i in all_sets:\n",
    "    print(i['completed_processes'])\n",
    "\n",
    "ready_sets_1 = [i for i in all_sets if \"HiC_Pipeline_0.2.6\" in i.get('completed_processes', [])]\n",
    "print(len(ready_sets_1))\n",
    "ready_sets_2 = []\n",
    "for a_set in ready_sets_1:\n",
    "    if a_set.get('other_processed_files'):\n",
    "        print(a_set['accession'])\n",
    "        if move_title in [i['title'] for i in a_set['other_processed_files']]:\n",
    "            if a_set.get('processed_files'):\n",
    "                print('WARN' ,a_set['accession'], 'has items in processed files, skipping ')\n",
    "                continue\n",
    "            else:\n",
    "                ready_sets_2.append(a_set)\n",
    "print(len(ready_sets_2), 'items are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 experiment sets in scope\n",
      "1 4DNESCQZYHLJ\n",
      "4DNESCQZYHLJ files will move\n",
      "4DNESCQZYHLJ moved to pc\n",
      "4DNEXTJW44KF files will move\n",
      "4DNEXTJW44KF moved to pc\n",
      "4DNEXTBUGNYO files will move\n",
      "4DNEXTBUGNYO moved to pc\n",
      "4DNEXYORQGXF files will move\n",
      "4DNEXYORQGXF moved to pc\n",
      "4DNEXHVCM2EO files will move\n",
      "4DNEXHVCM2EO moved to pc\n",
      "4DNEXA3MPMR5 files will move\n",
      "4DNEXA3MPMR5 moved to pc\n",
      "4DNEXGUER83P files will move\n",
      "4DNEXGUER83P moved to pc\n",
      "4DNEXRKOBMG5 files will move\n",
      "4DNEXRKOBMG5 moved to pc\n",
      "4DNEXRADON17 files will move\n",
      "4DNEXRADON17 moved to pc\n",
      "4DNEXLR5I2I8 files will move\n",
      "4DNEXLR5I2I8 moved to pc\n",
      "4DNEXHD3YPFT files will move\n",
      "4DNEXHD3YPFT moved to pc\n",
      "\n",
      "2 4DNESUZCBCG9\n",
      "4DNESUZCBCG9 files will move\n",
      "4DNESUZCBCG9 moved to pc\n",
      "4DNEXIMZGCE3 files will move\n",
      "4DNEXIMZGCE3 moved to pc\n",
      "4DNEXT4EKUL8 files will move\n",
      "4DNEXT4EKUL8 moved to pc\n",
      "4DNEXQELXX5F files will move\n",
      "4DNEXQELXX5F moved to pc\n",
      "4DNEXLASP92J files will move\n",
      "4DNEXLASP92J moved to pc\n",
      "4DNEXUS88PRH files will move\n",
      "4DNEXUS88PRH moved to pc\n",
      "4DNEXT8435BB files will move\n",
      "4DNEXT8435BB moved to pc\n",
      "4DNEX2LJ7O4H files will move\n",
      "4DNEX2LJ7O4H moved to pc\n",
      "4DNEXOX7HGSF files will move\n",
      "4DNEXOX7HGSF moved to pc\n",
      "4DNEXY9AL6NK files will move\n",
      "4DNEXY9AL6NK moved to pc\n",
      "4DNEXISZ4GBS files will move\n",
      "4DNEXISZ4GBS moved to pc\n",
      "\n",
      "3 4DNESFNBTRO3\n",
      "4DNESFNBTRO3 files will move\n",
      "4DNESFNBTRO3 moved to pc\n",
      "4DNEXFB1XFAM files will move\n",
      "4DNEXFB1XFAM moved to pc\n",
      "4DNEXIH2VL8A files will move\n",
      "4DNEXIH2VL8A moved to pc\n",
      "4DNEXOTB4BES files will move\n",
      "4DNEXOTB4BES moved to pc\n",
      "4DNEXVJM5487 files will move\n",
      "4DNEXVJM5487 moved to pc\n",
      "4DNEXLKPC6LE files will move\n",
      "4DNEXLKPC6LE moved to pc\n",
      "4DNEXPYWIJBQ files will move\n",
      "4DNEXPYWIJBQ moved to pc\n",
      "4DNEX6FH8G1J files will move\n",
      "4DNEX6FH8G1J moved to pc\n",
      "4DNEXLUET4T8 files will move\n",
      "4DNEXLUET4T8 moved to pc\n",
      "4DNEXUEZ73BS files will move\n",
      "4DNEXUEZ73BS moved to pc\n",
      "4DNEXPF7FEPS files will move\n",
      "4DNEXPF7FEPS moved to pc\n",
      "\n",
      "4 4DNESHF65RC4\n",
      "4DNESHF65RC4 files will move\n",
      "4DNESHF65RC4 moved to pc\n",
      "4DNEXQIQYBE3 files will move\n",
      "4DNEXQIQYBE3 moved to pc\n",
      "4DNEXWNRWN3Z files will move\n",
      "4DNEXWNRWN3Z moved to pc\n",
      "4DNEXRWB6CC2 files will move\n",
      "4DNEXRWB6CC2 moved to pc\n",
      "4DNEXQ7UY6QF files will move\n",
      "4DNEXQ7UY6QF moved to pc\n",
      "4DNEXUMDLALN files will move\n",
      "4DNEXUMDLALN moved to pc\n",
      "4DNEXCDOHAOY files will move\n",
      "4DNEXCDOHAOY moved to pc\n",
      "4DNEX53SKRQG files will move\n",
      "4DNEX53SKRQG moved to pc\n",
      "4DNEXNNS1CXS files will move\n",
      "4DNEXNNS1CXS moved to pc\n",
      "4DNEXM7P4FMX files will move\n",
      "4DNEXM7P4FMX moved to pc\n",
      "4DNEXKKELUM1 files will move\n",
      "4DNEXKKELUM1 moved to pc\n",
      "\n",
      "5 4DNESKLFZ31S\n",
      "4DNESKLFZ31S files will move\n",
      "4DNESKLFZ31S moved to pc\n",
      "4DNEXWLU575K files will move\n",
      "4DNEXWLU575K moved to pc\n",
      "4DNEXBPP27G9 files will move\n",
      "4DNEXBPP27G9 moved to pc\n",
      "4DNEXDKVJTLE files will move\n",
      "4DNEXDKVJTLE moved to pc\n",
      "4DNEXJHBIR4D files will move\n",
      "4DNEXJHBIR4D moved to pc\n",
      "4DNEX8NLXSVO files will move\n",
      "4DNEX8NLXSVO moved to pc\n",
      "4DNEXWX2C3EN files will move\n",
      "4DNEXWX2C3EN moved to pc\n",
      "4DNEXPZBTZJB files will move\n",
      "4DNEXPZBTZJB moved to pc\n",
      "4DNEXLYC2SAM files will move\n",
      "4DNEXLYC2SAM moved to pc\n",
      "4DNEXUG5OQFM files will move\n",
      "4DNEXUG5OQFM moved to pc\n",
      "4DNEXW3495QV files will move\n",
      "4DNEXW3495QV moved to pc\n",
      "4DNEXFQCBHAU files will move\n",
      "4DNEXFQCBHAU moved to pc\n",
      "4DNEXNG28KDK files will move\n",
      "4DNEXNG28KDK moved to pc\n",
      "4DNEX9HIVFU5 files will move\n",
      "4DNEX9HIVFU5 moved to pc\n",
      "4DNEX32C8NZ7 files will move\n",
      "4DNEX32C8NZ7 moved to pc\n",
      "\n",
      "6 4DNES4GDH4BG\n",
      "4DNES4GDH4BG files will move\n",
      "4DNES4GDH4BG moved to pc\n",
      "4DNEXRXA8LXQ files will move\n",
      "4DNEXRXA8LXQ moved to pc\n",
      "4DNEXD9YHU2D files will move\n",
      "4DNEXD9YHU2D moved to pc\n",
      "4DNEX4BTSE1F files will move\n",
      "4DNEX4BTSE1F moved to pc\n",
      "4DNEXDDVAFUH files will move\n",
      "4DNEXDDVAFUH moved to pc\n",
      "4DNEXFUEI5EM files will move\n",
      "4DNEXFUEI5EM moved to pc\n",
      "4DNEXAK9CG4Z files will move\n",
      "4DNEXAK9CG4Z moved to pc\n",
      "4DNEXFKQ9P2C files will move\n",
      "4DNEXFKQ9P2C moved to pc\n",
      "4DNEXSRZW1WD files will move\n",
      "4DNEXSRZW1WD moved to pc\n",
      "4DNEXHT9YAES files will move\n",
      "4DNEXHT9YAES moved to pc\n",
      "4DNEXL7KJTMX files will move\n",
      "4DNEXL7KJTMX moved to pc\n",
      "4DNEXPFX8RMK files will move\n",
      "4DNEXPFX8RMK moved to pc\n",
      "4DNEX971B567 files will move\n",
      "4DNEX971B567 moved to pc\n",
      "4DNEX5KF17AV files will move\n",
      "4DNEX5KF17AV moved to pc\n",
      "4DNEXA2PNFWE files will move\n",
      "4DNEXA2PNFWE moved to pc\n",
      "4DNEX398UQJ4 files will move\n",
      "4DNEX398UQJ4 moved to pc\n",
      "4DNEXA51AJL6 files will move\n",
      "4DNEXA51AJL6 moved to pc\n",
      "4DNEXX3EZWP6 files will move\n",
      "4DNEXX3EZWP6 moved to pc\n",
      "4DNEXIKWMWTK files will move\n",
      "4DNEXIKWMWTK moved to pc\n",
      "4DNEXLJFQYJ8 files will move\n",
      "4DNEXLJFQYJ8 moved to pc\n",
      "4DNEXXPRWKX6 files will move\n",
      "4DNEXXPRWKX6 moved to pc\n",
      "4DNEXOJ66NGQ files will move\n",
      "4DNEXOJ66NGQ moved to pc\n",
      "4DNEXA632CME files will move\n",
      "4DNEXA632CME moved to pc\n",
      "4DNEXQX1T1GK files will move\n",
      "4DNEXQX1T1GK moved to pc\n",
      "4DNEXX5WHC1A files will move\n",
      "4DNEXX5WHC1A moved to pc\n",
      "\n",
      "7 4DNES3O1B45O\n",
      "4DNES3O1B45O files will move\n",
      "4DNES3O1B45O moved to pc\n",
      "4DNEX4JMQO9U files will move\n",
      "4DNEX4JMQO9U moved to pc\n",
      "4DNEXVKRYYDB files will move\n",
      "4DNEXVKRYYDB moved to pc\n",
      "4DNEXKS17DNT files will move\n",
      "4DNEXKS17DNT moved to pc\n",
      "4DNEXBAR7DBY files will move\n",
      "4DNEXBAR7DBY moved to pc\n",
      "4DNEX4Y655YT files will move\n",
      "4DNEX4Y655YT moved to pc\n",
      "4DNEXFPMBMV8 files will move\n",
      "4DNEXFPMBMV8 moved to pc\n",
      "\n",
      "8 4DNESE1VMAMD\n",
      "4DNESE1VMAMD files will move\n",
      "4DNESE1VMAMD moved to pc\n",
      "4DNEXW8BROD1 files will move\n",
      "4DNEXW8BROD1 moved to pc\n",
      "4DNEX4117Y6M files will move\n",
      "4DNEX4117Y6M moved to pc\n",
      "4DNEXB4BMJKZ files will move\n",
      "4DNEXB4BMJKZ moved to pc\n",
      "4DNEX8B6PEVL files will move\n",
      "4DNEX8B6PEVL moved to pc\n",
      "4DNEX4QJ4ZLB files will move\n",
      "4DNEX4QJ4ZLB moved to pc\n",
      "4DNEXV485ZVS files will move\n",
      "4DNEXV485ZVS moved to pc\n",
      "\n",
      "9 4DNES7ODZ4MZ\n",
      "4DNES7ODZ4MZ files will move\n",
      "4DNES7ODZ4MZ moved to pc\n",
      "4DNEXRQPAEQK files will move\n",
      "4DNEXRQPAEQK moved to pc\n",
      "4DNEXOO6SD28 files will move\n",
      "4DNEXOO6SD28 moved to pc\n",
      "4DNEXOYOGGZA files will move\n",
      "4DNEXOYOGGZA moved to pc\n",
      "4DNEXDS6Z21V files will move\n",
      "4DNEXDS6Z21V moved to pc\n",
      "4DNEXG9PCRZT files will move\n",
      "4DNEXG9PCRZT moved to pc\n",
      "4DNEX279YP3H files will move\n",
      "4DNEX279YP3H moved to pc\n",
      "4DNEXGH69W5N files will move\n",
      "4DNEXGH69W5N moved to pc\n",
      "4DNEXPPPEJOO files will move\n",
      "4DNEXPPPEJOO moved to pc\n",
      "4DNEXC8YE2AV files will move\n",
      "4DNEXC8YE2AV moved to pc\n",
      "4DNEX5W57SL2 files will move\n",
      "4DNEX5W57SL2 moved to pc\n",
      "4DNEXPVALTIB files will move\n",
      "4DNEXPVALTIB moved to pc\n",
      "4DNEXDJMCI9O files will move\n",
      "4DNEXDJMCI9O moved to pc\n",
      "4DNEXPCNHZ6H files will move\n",
      "4DNEXPCNHZ6H moved to pc\n",
      "4DNEXWOVWCAW files will move\n",
      "4DNEXWOVWCAW moved to pc\n",
      "4DNEXMTHRAEH files will move\n",
      "4DNEXMTHRAEH moved to pc\n",
      "\n",
      "9\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "# move other processed files to processed files field\n",
    "action = False\n",
    "def move_opc_to_pc(resp, move_title, con_key, action):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        if opc:\n",
    "            print('There are files in processed_files field, expected empty', resp['accession'])\n",
    "            return False\n",
    "        else:\n",
    "            print('it is possible that move already happened, no opc but pc', resp['accession'])\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action:\n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                # release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "print(len(ready_sets_1), 'experiment sets in scope')\n",
    "for a_set in ready_sets_1:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_auth, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_auth, action)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_auth, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_auth, action)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release processed files if not\n",
    "action = False\n",
    "print(len(ready_sets_2), 'experiment sets in scope')\n",
    "\n",
    "for a_set in ready_sets_1:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_auth, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    \n",
    "    set_pc = set_resp.get('processed_files')\n",
    "    if set_pc & action:\n",
    "        release_files(resp['uuid'], set_pc, con_key)\n",
    "    \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_auth, add_on='frame=raw')\n",
    "        exp_pc = exp_resp.get('processed_files')\n",
    "        if exp_pc & action:\n",
    "            release_files(exp['uuid'], exp_pc, con_key)\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
