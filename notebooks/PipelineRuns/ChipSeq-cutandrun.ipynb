{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/search/?experiments_in_set.experiment_type=CUT%26RUN&type=ExperimentSetReplicate&limit=all&status=pre-release&status=released&status=released%20to%20project\n",
      "51 total number of sets\n",
      "0 sets completed\n",
      "51 ready for processing\n"
     ]
    }
   ],
   "source": [
    "from dcicutils import ff_utils\n",
    "from functions.wfr import *\n",
    "from functions.wfr_settings import *\n",
    "from functions.notebook_functions import *\n",
    "\n",
    "# tibanna = Tibanna(env=env)\n",
    "my_env = 'data'\n",
    "my_auth = get_key('koray_data')\n",
    "\n",
    "# different types of exps use different steps at the last step(3).\n",
    "recipe = [\n",
    "    [['ChIP-seq'], ''],\n",
    "    [['CUT%26RUN'], ''],\n",
    "]\n",
    "\n",
    "#Choose the recipe element to run the pipeline on\n",
    "recipe_no = 1\n",
    "exp_type, step3 = recipe[recipe_no]\n",
    "\n",
    "set_url = '/search/?'+ \\\n",
    "          '&'.join(['experiments_in_set.experiment_type='+i for i in exp_type])+ \\\n",
    "          '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "          '&status=pre-release&status=released&status=released%20to%20project'\n",
    "print(set_url)\n",
    "#set_url = '/search/?award.project=ENCODE&experimentset_type=replicate&type=ExperimentSetReplicate'\n",
    "\n",
    "#set_url = '/search/?award.project=4DN&experiments_in_set.biosample.biosource.individual.organism.name=fruit-fly&experimentset_type=replicate&type=ExperimentSetReplicate'\n",
    "#print set_url\n",
    "\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "counter = 0\n",
    "completed = 0\n",
    "completed_acc = []\n",
    "\n",
    "run_sets = [i for i in all_sets if \"CHIP_Pipeline_0.2.5\"  not in i.get('completed_processes', [])]\n",
    "\n",
    "print(len(all_sets), 'total number of sets')\n",
    "print(len(all_sets)-len(run_sets), 'sets completed')\n",
    "print(len(run_sets), 'ready for processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4DNESC5FAH5U human yes\n",
      "Replicates of CUT&RUN on K562 cells using H3K4me3 (39159 AM) with a digestion time of 3min\n",
      "wfr is still running\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:encode-chipseq_4DNESC5FAH5Uf484b047-bf30-4d6a-8be2-03349f2ea0be' target='_blank'>RUNNING</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample = [[[b1t1,b1t1],[b1t2,b1t2]],[[b2t1,b2t1],[b2t2,b2t2]]]\n",
    "\n",
    "my_s3_util = s3Utils(env=my_env)\n",
    "raw_bucket = my_s3_util.raw_file_bucket\n",
    "out_bucket = my_s3_util.outfile_bucket\n",
    "\n",
    "def clean_extra(fastq_resp, key):\n",
    "    if 'extra_files' in fastq_resp:\n",
    "        ff_utils.patch_metadata({}, obj_id = fastq_resp['uuid'], key = key, add_on = 'delete_fields=extra_files')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "for a_set in run_sets: \n",
    "    paired = \"\"\n",
    "    attributions = None\n",
    "    counter += 1\n",
    "    replicate_exps = a_set['replicate_exps']\n",
    "    files = []\n",
    "    cont_files = []\n",
    "    obj_key = []\n",
    "    cont_obj_key = []\n",
    "    replicate_exps = sorted(replicate_exps, key = lambda x: [x['bio_rep_no'], x['tec_rep_no']])\n",
    "    \n",
    "    for i in replicate_exps:\n",
    "        exp = i['replicate_exp']['uuid']\n",
    "        exp_resp = ff_utils.get_metadata(exp, key = my_auth)\n",
    "        \n",
    "        \n",
    "        #get organism and control\n",
    "        biosample = exp_resp['biosample']\n",
    "        organism = list(set([bs['individual']['organism']['name'] for bs in biosample['biosource']]))[0]\n",
    "        control = ''\n",
    "        exp_relation = exp_resp.get('experiment_relation')\n",
    "        if exp_relation:\n",
    "            rel_type = [i['relationship_type'] for i in exp_relation]\n",
    "            if 'control for' in rel_type:\n",
    "                control = \"control experiment\"\n",
    "                break\n",
    "            elif 'controlled by' in rel_type:\n",
    "                controls = [i['experiment'] for i in exp_relation if i['relationship_type'] == 'controlled by']\n",
    "                control = controls[0]['uuid']\n",
    "        else:\n",
    "            control = False\n",
    "            break\n",
    "\n",
    "                    \n",
    "        # get exp files\n",
    "        exp_files = exp_resp['files']\n",
    "        b_index = int(i['bio_rep_no']) - 1\n",
    "        t_index = int(i['tec_rep_no']) - 1\n",
    "        for a_file in exp_files:\n",
    "            f_t = []\n",
    "            o_t = []\n",
    "            file_resp = ff_utils.get_metadata(a_file['uuid'], key = my_auth) \n",
    "            ex = clean_extra(file_resp, key = my_auth)\n",
    "            if ex:\n",
    "                print(file_resp['accession'], 'cleaned extra file')\n",
    "            # get pair end no\n",
    "            pair_end = file_resp.get('paired_end')\n",
    "            if pair_end == '2':\n",
    "                paired = 'yes'\n",
    "                continue\n",
    "            # get paired file\n",
    "            paired_with = \"\"\n",
    "            relations = file_resp.get('related_files')\n",
    "            if not relations:\n",
    "                pass\n",
    "            else:\n",
    "                for relation in relations:\n",
    "                    if relation['relationship_type'] == 'paired with':\n",
    "                        paired = 'yes'\n",
    "                        paired_with = relation['file']['uuid']\n",
    "            # decide if data is not paired end reads\n",
    "            if not paired_with:\n",
    "                if not paired:\n",
    "                    paired = 'single'\n",
    "                else:\n",
    "                    if paired != 'single':\n",
    "                        print('inconsistent fastq pair info')\n",
    "                        continue\n",
    "                        \n",
    "                f_t.append(file_resp['uuid'])\n",
    "                o_t.append(file_resp['display_title'])\n",
    "            else:\n",
    "                f2 = ff_utils.get_metadata(paired_with, key = my_auth)\n",
    "                ex = clean_extra(f2, key = my_auth)\n",
    "                if ex:\n",
    "                    print(f2['accession'], 'cleaned extra file')\n",
    "                f_t.append(file_resp['uuid'])\n",
    "                o_t.append(file_resp['display_title'])\n",
    "                f_t.append(f2['uuid'])\n",
    "                o_t.append(f2['display_title'])\n",
    "                \n",
    "            try: \n",
    "                files[b_index]\n",
    "            except:\n",
    "                files.append([])\n",
    "            files[b_index].append(f_t)\n",
    "            files[b_index][t_index] = f_t\n",
    "            \n",
    "            try:\n",
    "                obj_key[b_index]\n",
    "            except:\n",
    "                obj_key.append([])\n",
    "            obj_key[b_index].append(o_t)\n",
    "            obj_key[b_index][t_index] = o_t\n",
    "            \n",
    "    if control:\n",
    "        control_set_acc = ff_utils.get_metadata(control, key = my_auth)['experiment_sets'][0]['accession']\n",
    "        cont_set = ff_utils.get_metadata(control_set_acc, key=my_auth)\n",
    "        cont_replicate_exps = cont_set['replicate_exps']\n",
    "        cont_replicate_exps = sorted(cont_replicate_exps, key = lambda x: [x['bio_rep_no'], x['tec_rep_no']])\n",
    "    \n",
    "        for i in cont_replicate_exps:\n",
    "            cont_exp = i['replicate_exp']['uuid']\n",
    "            cont_exp_resp = ff_utils.get_metadata(cont_exp, key = my_auth)\n",
    "            cont_exp_files = cont_exp_resp['files']\n",
    "            b_index = int(i['bio_rep_no']) - 1\n",
    "            t_index = int(i['tec_rep_no']) - 1\n",
    "            for a_file in cont_exp_files:\n",
    "                f_t = []\n",
    "                o_t = []\n",
    "                file_resp = ff_utils.get_metadata(a_file['uuid'], key = my_auth)           \n",
    "                # get pair end no\n",
    "                pair_end = file_resp.get('paired_end')\n",
    "                if pair_end == '2':\n",
    "                    paired = 'yes'\n",
    "                    continue\n",
    "                # get paired file\n",
    "                paired_with = \"\"\n",
    "                relations = file_resp.get('related_files')\n",
    "                if not relations:\n",
    "                    pass\n",
    "                else:\n",
    "                    for relation in relations:\n",
    "                        if relation['relationship_type'] == 'paired with':\n",
    "                            paired = 'yes'\n",
    "                            paired_with = relation['file']['uuid']\n",
    "                # decide if data is not paired end reads\n",
    "                if not paired_with:\n",
    "                    if not paired:\n",
    "                        paired = 'single'\n",
    "                    else:\n",
    "                        if paired != 'single':\n",
    "                            print('inconsistent fastq pair info')\n",
    "                            continue\n",
    "\n",
    "                    f_t.append(file_resp['uuid'])\n",
    "                    o_t.append(file_resp['display_title'])\n",
    "                else:\n",
    "                    f2 = ff_utils.get_metadata(paired_with, key = my_auth)\n",
    "                    f_t.append(file_resp['uuid'])\n",
    "                    o_t.append(file_resp['display_title'])\n",
    "                    f_t.append(f2['uuid'])\n",
    "                    o_t.append(f2['display_title'])\n",
    "\n",
    "                try: \n",
    "                    cont_files[b_index]\n",
    "                except:\n",
    "                    cont_files.append([])\n",
    "                cont_files[b_index].append(f_t)\n",
    "                cont_files[b_index][t_index] = f_t\n",
    "\n",
    "                try:\n",
    "                    cont_obj_key[b_index]\n",
    "                except:\n",
    "                    cont_obj_key.append([])\n",
    "                cont_obj_key[b_index].append(o_t)\n",
    "                cont_obj_key[b_index][t_index] = o_t\n",
    "              \n",
    "                \n",
    "    if not control:\n",
    "        print('no control exp')\n",
    "        continue\n",
    "    \n",
    "    if organism not in ['mouse', 'human']:\n",
    "        print('orgamism not ready', organism)\n",
    "        continue\n",
    "    \n",
    "    if paired == 'single':\n",
    "        print('skipping single end read set')\n",
    "        continue\n",
    "\n",
    "                        \n",
    "    print(a_set['accession'], organism, paired)  \n",
    "    print(a_set['description'])\n",
    "    attributions = get_attribution(ff_utils.get_metadata(files[0][0][0], key = my_auth))\n",
    "    \n",
    "    caller = ''\n",
    "    org = ''\n",
    "    typ = ''\n",
    "    if organism == \"human\":\n",
    "        org = 'hs'\n",
    "        typ = 'tf'\n",
    "        caller = 'mac2'\n",
    "        input_files = [{\n",
    "              \"object_key\": \"4DNFIZQB369V.bwaIndex.tar\",\n",
    "              \"rename\": \"GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta.tar\",\n",
    "              \"bucket_name\": raw_bucket,\n",
    "              \"workflow_argument_name\": \"chip.bwa_idx_tar\",\n",
    "              \"uuid\": \"38077b98-3862-45cd-b4be-8e28e9494549\"\n",
    "              },\n",
    "              {\n",
    "              \"object_key\": \"4DNFIZ1TGJZR.bed.gz\",\n",
    "              \"bucket_name\": raw_bucket,\n",
    "              \"workflow_argument_name\": \"chip.blacklist\",\n",
    "              \"uuid\": \"9562ffbd-9f7a-4bd7-9c10-c335137d8966\"\n",
    "              },\n",
    "              {\n",
    "              \"object_key\": \"4DNFIZJB62D1.chrom.sizes\",\n",
    "              \"bucket_name\": raw_bucket,\n",
    "              \"workflow_argument_name\": \"chip.chrsz\",\n",
    "              \"uuid\": \"9866d158-da3c-4d9b-96a9-1d59632eabeb\"\n",
    "              }]\n",
    "          \n",
    "    elif organism == \"mouse\":\n",
    "        org = 'mm'\n",
    "        typ = 'tf'\n",
    "        caller = 'mac2'\n",
    "        input_files = [{\n",
    "              \"object_key\": \"4DNFIZ2PWCC2.bwaIndex.tar\",\n",
    "              \"rename\": \"mm10_no_alt_analysis_set_ENCODE.fasta.tar\",\n",
    "              \"bucket_name\": raw_bucket,\n",
    "              \"workflow_argument_name\": \"chip.bwa_idx_tar\",\n",
    "              \"uuid\": \"f4b63d31-65d8-437f-a76a-6bedbb52ae6f\"\n",
    "              },\n",
    "              {\n",
    "              \"object_key\": \"4DNFIZ3FBPK8.bed.gz\",\n",
    "              \"bucket_name\": raw_bucket,\n",
    "              \"workflow_argument_name\": \"chip.blacklist\",\n",
    "              \"uuid\": \"a32747a3-8a9e-4a9e-a7a1-4db0e8b65925\"\n",
    "              },\n",
    "              {\n",
    "              \"object_key\": \"4DNFIBP173GC.chrom.sizes\",\n",
    "              \"bucket_name\": raw_bucket,\n",
    "              \"workflow_argument_name\": \"chip.chrsz\",\n",
    "              \"uuid\": \"be0a9819-d2ce-4422-be4b-234fb1677dd9\"\n",
    "              }]\n",
    "        \n",
    "    input_files.append({\"object_key\": obj_key,\n",
    "                       \"bucket_name\": raw_bucket,\n",
    "                       \"workflow_argument_name\": \"chip.fastqs\",\n",
    "                       \"uuid\": files})\n",
    "\n",
    "    input_files.append({\"object_key\": cont_obj_key,\n",
    "                       \"bucket_name\": raw_bucket,\n",
    "                       \"workflow_argument_name\": \"chip.ctl_fastqs\",\n",
    "                       \"uuid\": cont_files})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if paired == 'single':\n",
    "        chip_p = False\n",
    "    if paired == 'yes':\n",
    "        chip_p = True\n",
    "    \n",
    "    parameters = {\n",
    "        \"chip.qc_report.desc\": a_set.get('description'),\n",
    "        \"chip.paired_end\": chip_p,\n",
    "        \"chip.bam2ta.regex_grep_v_ta\": \"chr[MUE]|random|alt\",\n",
    "        \"chip.gensz\": org,\n",
    "        \"chip.qc_report.name\": \"CHIP-seq Report\",\n",
    "        \"chip.choose_ctl.always_use_pooled_ctl\": True,\n",
    "        \"chip.pipeline_type\": typ,\n",
    "        \"chip.peak_caller\": caller}\n",
    "    \n",
    "    wf_info = step_settings('encode-chipseq',organism, attributions)\n",
    "    tag = '0.2.5'\n",
    "    run_name = a_set['accession']\n",
    "\n",
    "    \"\"\"Creates the trigger json that is used by foufront endpoint.\n",
    "    \"\"\"\n",
    "    input_json = {'input_files': input_files,\n",
    "                  'output_bucket': out_bucket,\n",
    "                  'workflow_uuid': wf_info['wf_uuid'],\n",
    "                  \"app_name\": wf_info['wf_name'],\n",
    "                  \"wfr_meta\": wf_info['wfr_meta'],\n",
    "                  \"parameters\": parameters,\n",
    "                  \"config\": wf_info['config'],\n",
    "                  \"custom_pf_fields\": wf_info['custom_pf_fields'],\n",
    "                  \"_tibanna\": {\"env\": my_env,\n",
    "                               \"run_type\": wf_info['wf_name'],\n",
    "                               \"run_id\": run_name},\n",
    "                  \"tag\": tag\n",
    "                  }\n",
    "    run_result =  get_wfr_out(files[0][0][0], 'encode-chipseq 0.2.5', my_auth)\n",
    "    \n",
    "    if run_result['status'] == 'running':\n",
    "        print('wfr is still running')\n",
    "        continue\n",
    "    elif run_result['status'].startswith('no'):\n",
    "        print('starting run')\n",
    "\n",
    "    r = json.dumps(input_json)\n",
    "    #print(r)\n",
    "    e = ff_utils.post_metadata(input_json, 'WorkflowRun/run', key=my_auth)\n",
    "    url = json.loads(e['input'])['_tibanna']['url']\n",
    "    display(HTML(\"<a href='{}' target='_blank'>{}</a>\".format(url, e['status'])))\n",
    "    break\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3 4DNESH4UTRNL DpnII mouse 800  -  4DNEX4KRGMAQ is missing Part2 - Fails at runtaskawsem\n",
    "https://console.aws.amazon.com/states/home?region=us-east-1#/executions/details/arn:aws:states:us-east-1:643366669028:execution:tibanna_pony:hi-c-processing-bam_4DNEX4KRGMAQ9b484528-5651-4d72-a271-a9b37a42ab05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files from opc to pc\n",
    "from dcicutils import ff_utils\n",
    "from functions.notebook_functions import *\n",
    "from functions.wfr import *\n",
    "\n",
    "recipe_no = 1\n",
    "exp_type, step3 = recipe[recipe_no]\n",
    "action = False\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "set_url = '/search/?'+ \\\n",
    "          '&'.join(['experiments_in_set.experiment_type='+i for i in exp_type])+ \\\n",
    "          '&type=ExperimentSetReplicate&limit=all' + \\\n",
    "          '&status=released&status=released%20to%20project'\n",
    "\n",
    "print(set_url)\n",
    "set_url = '/search/?award.project=4DN&experimentset_type=replicate&lab.display_title=Chuck+Murry%2C+UW&status=pre-release&type=ExperimentSetReplicate'\n",
    "\n",
    "# exp\n",
    "# set_url = '/search/?'+ \\\n",
    "#           '&'.join(['experiment_type='+i for i in exp_type])+ \\\n",
    "#           '&type=Experiment&limit=all' + \\\n",
    "#           '&status=released&status=released%20to%20project'\n",
    "\n",
    "all_sets = ff_utils.search_metadata(set_url , key=my_auth)\n",
    "\n",
    "ready_sets_1 = [i for i in all_sets if \"HiC_Pipeline_0.2.5\" in i.get('completed_processes', [])]\n",
    "print(len(ready_sets_1))\n",
    "ready_sets_2 = []\n",
    "for a_set in ready_sets_1:\n",
    "    if a_set.get('other_processed_files'):\n",
    "        print('a')\n",
    "        print(a_set['accession'])\n",
    "        if move_title in [i['title'] for i in a_set['other_processed_files']]:\n",
    "            print('b')\n",
    "            if a_set.get('processed_files'):\n",
    "                print('c')\n",
    "                print('WARN' ,a_set['accession'], 'has items in processed files, skipping ')\n",
    "                continue\n",
    "            else:\n",
    "                ready_sets_2.append(a_set)\n",
    "print(len(ready_sets_2), 'items are ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move other processed files to processed files field\n",
    "action = True\n",
    "def move_opc_to_pc(resp, move_title, con_key):\n",
    "    opc = resp.get('other_processed_files')\n",
    "    pc = resp.get('processed_files')\n",
    "    # if processed_files field already has values, exit\n",
    "    if pc:\n",
    "        if opc:\n",
    "            print('There are files in processed_files field, expected empty', resp['accession'])\n",
    "            return False\n",
    "        else:\n",
    "            print('it is possible that move already happened, no opc but pc', resp['accession'])\n",
    "    # see if there are other_processed_files to move\n",
    "    if opc:\n",
    "        titles = [i['title'] for i in opc]\n",
    "        if move_title in titles:\n",
    "            print(resp['accession'], 'files will move')\n",
    "            move_item = [i for i in opc if i['title'] == move_title]\n",
    "            assert len(move_item) == 1\n",
    "            assert move_item[0]['type'] == 'preliminary'\n",
    "            new_pc = move_item[0]['files']\n",
    "            new_opc = [i for i in opc if i['title'] != move_title]\n",
    "            # Time to patch\n",
    "            patch_data = {}\n",
    "            add_on = \"\"\n",
    "            #if there is something left in opc, patch it, if not delete field\n",
    "            if new_opc:\n",
    "                patch_data['other_processed_files'] = opc\n",
    "            else:\n",
    "                add_on = 'delete_fields=other_processed_files'\n",
    "            # patch with processed files\n",
    "            patch_data['processed_files'] = new_pc\n",
    "            if action:\n",
    "                ff_utils.patch_metadata(patch_data, resp['uuid'], key = con_key, add_on = add_on)\n",
    "                # update status of pc to status of set or exp\n",
    "                release_files(resp['uuid'], new_pc, con_key)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        \n",
    "set_w_apf = 0\n",
    "exp_w_apf = 0\n",
    "counter = 0\n",
    "move_title = 'HiC Processing Pipeline - Preliminary Files'\n",
    "\n",
    "print(len(ready_sets_2), 'experiment sets in scope')\n",
    "for a_set in ready_sets_2:\n",
    "    set_resp = ff_utils.get_metadata(a_set['uuid'],key=my_auth, add_on='frame=raw')\n",
    "    counter += 1\n",
    "    print(counter, set_resp['accession'])\n",
    "    exps = set_resp['experiments_in_set']\n",
    "    res =  move_opc_to_pc(set_resp, move_title, my_auth)\n",
    "    if res:\n",
    "        set_w_apf += 1\n",
    "        print(set_resp['accession'], 'moved to pc')\n",
    "  \n",
    "    for exp in exps:\n",
    "        exp_resp = ff_utils.get_metadata(exp, key=my_auth, add_on='frame=raw')\n",
    "        res_e =  move_opc_to_pc(exp_resp,move_title,my_auth)\n",
    "        if res_e:\n",
    "            exp_w_apf += 1\n",
    "            print(exp_resp['accession'], 'moved to pc')\n",
    "    print()\n",
    "\n",
    "print(set_w_apf)\n",
    "print(exp_w_apf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
