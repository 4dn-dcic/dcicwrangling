# Copyright 2018 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#!/bin/bash

# Create the machine on GCP via:
#
# gcloud beta compute instances create \
#   "deepvariant-caller-benchmarks" --zone "us-west1-b" \
#   --machine-type "n1-standard-64" --subnet "default" \
#   --image-family "ubuntu-1604-lts" --image-project "ubuntu-os-cloud" \
#   --boot-disk-size "1500" --boot-disk-type "pd-ssd"

set -euo pipefail
set -x

BUILD_DIR=${HOME}/setup_software_build
VERSIONS=/usr/local/versions.txt


export DATA_DIR="/benchmarking/data"
export REF="${DATA_DIR}/hs37d5.fa"
export GZREF="${DATA_DIR}/hs37d5.fa.gz"
export BAM="${DATA_DIR}/HG002_NIST_150bp_50x.bam"
export TRUTH_VCF="${DATA_DIR}/HG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.vcf.gz"
export TRUTH_BED="${DATA_DIR}/HG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf.bed"
export DBSNP="${DATA_DIR}/dbsnp_All_20170710.GRCh37p13.vcf.gz"

# GATK3 and GATK4
export GATK3_JAR=/usr/local/bin/GenomeAnalysisTK3.jar
export GATK4_JAR=/usr/local/bin/GenomeAnalysisTK4.jar
export GATK_BUNDLE="${DATA_DIR}/gatk_b37_resource_bundle"
export DBSNP="${GATK_BUNDLE}/dbsnp_138.b37.vcf.gz"
export K1G="${GATK_BUNDLE}/1000G_phase3_v4_20130502.sites.vcf.gz"
export OMNI="${GATK_BUNDLE}/1000G_omni2.5.b37.vcf.gz"
export MILLS="${GATK_BUNDLE}/Mills_and_1000G_gold_standard.indels.b37.vcf.gz"
export HAPMAP="${GATK_BUNDLE}/hapmap_3.3.b37.vcf.gz"

# Leave this argument as the empty string to evaluate all confident variants.
# export HAPPY_EVAL_REGION_ARG="-l 20"
export HAPPY_EVAL_REGION_ARG=""

export OUTPUT_DIR="${HOME}/caller_evaluation"
export LOG_DIR="${OUTPUT_DIR}/logs"
TIMESTAMP=$(date +%s)

export RUN_FILTERING="/usr/local/CHM-eval.kit/run-flt"

declare -a CHROMS;
# REGIONS=(2)
REGIONS=(1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22)
# REGIONS=(20:10000000-10010000 20:11000000-11010000)

DV_DATA=gs://deepvariant/case-study-testdata
BAM_ROOT="${DV_DATA}/HG002_NIST_150bp_50x.bam"
GIAB_ROOT="${DV_DATA}/HG002_GIAB_highconf_IllFB-IllGATKHC-CG-Ion-Solid_CHROM1-22_v3.2.2_highconf"
FASTA_ROOT="${DV_DATA}/hs37d5.fa"

BENCHMARK_ROOT=/benchmarking
BENCHMARK_PATH=/benchmarking/data
TIMESTAMP=$(date +%s)

DBSNP_NBCI_FTP="ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz"
DBSNP_VCF_NAME="dbsnp_All_20170710.GRCh37p13.vcf.gz"
GATK_RESOURCE_BUNDLE_DIRNAME="gatk_b37_resource_bundle"
GATK_RESOURCE_BUNDLE_FTP_ROOT="ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/b37"

# DeepVariant historical datasets. Each entry here will produce a VCF file in
# the benchmarking datasets called deepvariant-KEY.vcf.gz from the source gcs
# pass value.
declare -A DEEPVARIANT_HISTORICAL_CALLSETS     # Explicitly declare
DEEPVARIANT_HISTORICAL_CALLSETS['pfda']="gs://deepvariant/historical_datasets/precision_fda_submission/HG002-NA24385-pFDA.vcf.gz"
DEEPVARIANT_HISTORICAL_CALLSETS['r0.4']="gs://deepvariant/historical_datasets/case_study/deepvariant-0.4.0-HG002.output.vcf.gz"



################################################################################
#
# setting up the software
#
################################################################################

function setup_common() {
  apt-get -y install htop
  apt-get -y install parallel
  apt-get -y install emacs-nox
  apt-get -y install docker.io

  apt-get update -qq
  apt-get -y install make gcc g++
  apt-get -y install python-dev python-setuptools

  apt-get -y install python-pip
  pip install -U crcmod

  apt-get -y install lftp curl wget bzip2 git unzip
  apt-get install -y libncurses-dev zlib1g-dev libbz2-dev liblzma-dev

  touch "${VERSIONS}"
  chmod 666 "${VERSIONS}"
  echo "software version" > "${VERSIONS}"
}

function setup_samtools() {
  wget https://github.com/samtools/samtools/releases/download/1.6/samtools-1.6.tar.bz2
  tar xvjf samtools-1.6.tar.bz2
  pushd samtools-1.6
  ./configure --prefix=/usr/local
  make
  make install
  popd
  /usr/local/bin/samtools --version | head -n 1 >> "${VERSIONS}"
}

function setup_htslib() {
  wget https://github.com/samtools/htslib/releases/download/1.6/htslib-1.6.tar.bz2
  tar xvjf htslib-1.6.tar.bz2
  pushd htslib-1.6
  ./configure --prefix=/usr/local
  make
  make install
  popd
  /usr/local/bin/tabix --version | head -n 1 | awk '{print $1, $3}' >> "${VERSIONS}"
}

function setup_bcftools() {
  wget https://github.com/samtools/bcftools/releases/download/1.6/bcftools-1.6.tar.bz2
  tar xvjf bcftools-1.6.tar.bz2
  pushd bcftools-1.6
  ./configure --prefix=/usr/local
  make
  make install
  popd
  /usr/local/bin/bcftools --version | head -n 1 >> "${VERSIONS}"
}

function setup_chmeval() {
  wget https://github.com/lh3/CHM-eval/releases/download/v0.5/CHM-evalkit-20180222.tar
  tar xvf CHM-evalkit-20180222.tar -C /usr/local/
  echo "CHM-evalkit v0.5 20180222" >> "${VERSIONS}"

  git clone https://github.com/lh3/CHM-eval.git
  mv CHM-eval/run-flt /usr/local/CHM-eval.kit/
}

function setup_bedtools() {
  apt-get install bedtools
  bedtools --version >> "${VERSIONS}"
}

function setup_freebayes() {
  git clone --recursive git://github.com/ekg/freebayes.git
  pushd freebayes
  make
  make install
  popd
  /usr/local/bin/freebayes --version | awk '{print "freebayes", $2}' >> "${VERSIONS}"
}

function setup_16gt() {
  # instructions from https://github.com/aquaskyline/16GT
  # fetched version 34e8f934e9f905427293693953e55e9bfe14288e
  git clone https://github.com/aquaskyline/16GT
  pushd 16GT
  # Fix bad filter field separator.
  sed -i 's/join ",", @filterTag/join ";", @filterTag/' filterVCF.pl
  git log | head -n 1 | awk '{print "16GT", "v1.0-" $2}' >> "${VERSIONS}" || true
  make

  # 1. Build reference index
  git clone https://github.com/aquaskyline/SOAP3-dp.git
  pushd SOAP3-dp
  make SOAP3-Builder
  make BGS-Build
  popd

  popd
  rm -rf /usr/local/bin/16GT
  mv 16GT /usr/local/bin
  # No version mode, this produces retval != 0.
  # /usr/local/bin/16GT/bam2snapshot
}

function setup_strelka() {
  # apt-get install -qq bzip2 gcc g++ make python zlib1g-dev
  git clone https://github.com/Illumina/strelka
  pushd strelka
  mkdir build && cd build
  ../configure --jobs=60 --prefix=/usr/local
  make -j4 install
  popd
  /usr/local/bin/configureStrelkaGermlineWorkflow.py --version | awk '{print "strelka", $1}' >> "${VERSIONS}"
}

function setup_picard() {
  # GATK requires Java 1.8
  apt-get install -y default-jre
  # Get the GATK.
  wget https://github.com/broadinstitute/picard/releases/download/2.17.1/picard.jar
  mv picard.jar /usr/local/bin
  echo 'picard 2.17.1' >> "${VERSIONS}"
}

function setup_gatk3() {
  # GATK requires Java 1.8
  apt-get install -y default-jre
  # Get the GATK.
  wget https://software.broadinstitute.org/gatk/download/auth?package=GATK \
    -O GenomeAnalysisTK-3.8-0.tar.bz2
  tar xvjf GenomeAnalysisTK-3.8-0.tar.bz2
  mv GenomeAnalysisTK-3.8-0-*/GenomeAnalysisTK.jar /usr/local/bin/GenomeAnalysisTK3.jar
  java -jar /usr/local/bin/GenomeAnalysisTK3.jar -version | awk '{print "GATK3", $1}' >> "${VERSIONS}"
}


function setup_happy() {
  apt-get install -y cmake autoconf
  pip install pysam pandas scipy
  git clone https://github.com/Illumina/hap.py.git
  pushd hap.py
  # We cannot run tests hap.py requires some magical settings for HG19 and HGREF
  # that don't seem to work: https://github.com/Illumina/hap.py/issues/25
  # gsutil cp gs://deepvariant/case-study-testdata/hs37d5.fa.gz /tmp
  # gunzip -f /tmp/hs37d5.fa.gz
  # export HG19=/tmp/hs37d5.fa
  # export HGREF=/tmp/hs37d5.fa
  python install.py --no-tests /usr/local/bin/happy
  # rm -f /tmp/hs37d5.fa
  popd
  /usr/local/bin/happy/bin/hap.py --version >> "${VERSIONS}"
}


################################################################################
#
# setting up the data
#
################################################################################

function prepare_wgs_sample() {
  # Copy the full BAM.
  gsutil cp ${BAM_ROOT}* "${BENCHMARK_PATH}"
}

function prepare_giab() {
  gsutil cp ${GIAB_ROOT}* "${BENCHMARK_PATH}"
}

function prepare_ref() {
  gsutil cp ${FASTA_ROOT}* "${BENCHMARK_PATH}"
  gunzip -c "${BENCHMARK_PATH}/hs37d5.fa.gz" > "${BENCHMARK_PATH}/hs37d5.fa"
  samtools faidx "${BENCHMARK_PATH}/hs37d5.fa"

  rm -f "${BENCHMARK_PATH}/hs37d5.dict"
  java -jar /usr/local/bin/picard.jar CreateSequenceDictionary \
    R="${BENCHMARK_PATH}/hs37d5.fa" \
    O="${BENCHMARK_PATH}/hs37d5.dict"
}

function fetch_deepvariant_historical_callsets() {
  for name in "${!DEEPVARIANT_HISTORICAL_CALLSETS[@]}"; do
    gcs_path=${DEEPVARIANT_HISTORICAL_CALLSETS[$name]}
    destination="${BENCHMARK_PATH}/deepvariant-${name}.vcf.gz"
    gsutil cp "${gcs_path}" "${destination}"
    tabix -f -p vcf "${destination}"
  done
}

function fetch_dbsnp() {
  output_file="${BENCHMARK_PATH}/${DBSNP_VCF_NAME}"
  rm -f "${output_file}"
  lftp -c "pget -n 10 ${DBSNP_NBCI_FTP} -o ${output_file}"
  tabix -f -p vcf "${output_file}"
}


function fetch_chm1_chm13() {
  chm1_chm13_ftp="ftp://ftp.sra.ebi.ac.uk/vol1/ERA596/ERA596361/bam/CHM1_CHM13_2.bam"
  output_file="${BENCHMARK_PATH}/CHM1_CHM13_2.bam"
  rm -f "${output_file}"
  lftp -c "pget -n 10 ${chm1_chm13_ftp} -o ${output_file}"
  lftp -c "pget -n 10 ${chm1_chm13_ftp}.bai -o ${output_file}.bai"
}

function soap_index_fasta() {
  /usr/local/bin/16GT/SOAP3-dp/soap3-dp-builder "${BENCHMARK_PATH}/hs37d5.fa"
  /usr/local/bin/16GT/SOAP3-dp/BGS-Build "${BENCHMARK_PATH}/hs37d5.fa.index"
}

function fetch_gatk_resource_bundle() {
  mkdir -p "${BENCHMARK_PATH}/${GATK_RESOURCE_BUNDLE_DIRNAME}"
  pushd "${BENCHMARK_PATH}/${GATK_RESOURCE_BUNDLE_DIRNAME}"

  vcfs=(dbsnp_138.b37.vcf.gz
        1000G_omni2.5.b37.vcf.gz
        Mills_and_1000G_gold_standard.indels.b37.vcf.gz
        hapmap_3.3.b37.vcf.gz
        1000G_phase3_v4_20130502.sites.vcf.gz)
  for file in ${vcfs[@]}; do
    # gunzip the vcf, recompress with bgzip, and tabix index it.
    rm -f "${file}"
    lftp -c "pget -n 10 ${GATK_RESOURCE_BUNDLE_FTP_ROOT}/${file}"
    gunzip "${file}"
    bgzip "${file%.gz}"
    tabix -f -p vcf "${file}"
  done

  popd
}

################################################################################
#
# run and evaluate the callers
#
################################################################################


# ------------------------------------------------------------------------------
#
# 16GT
#
# ------------------------------------------------------------------------------

function call_16gt() {
  pushd "${OUTPUT_DIR}"
  mkdir -p 16gt_output
  PATH_16GT=/usr/local/bin/16GT

  ${PATH_16GT}/bam2snapshot -i ${REF}.index -b ${BAM} -o 16gt_output/prefix
  ${PATH_16GT}/snapshotSnpcaller -i ${REF}.index -o 16gt_output/prefix
  perl ${PATH_16GT}/txt2vcf.pl 16gt_output/prefix.txt HG002 ${REF} > 16gt.raw.vcf
  perl ${PATH_16GT}/filterVCF.pl 16gt.raw.vcf ${DBSNP} > 16gt.filtered.vcf

  bgzip -f 16gt.filtered.vcf
  tabix -f -p vcf 16gt.filtered.vcf.gz

  rm -rf 16gt_output 16gt.raw.vcf
}

# ------------------------------------------------------------------------------
#
# Freebayes
#
# ------------------------------------------------------------------------------

function call_freebayes1() {
  region=$1
  samtools view -hb "${BAM}" "${region}" | freebayes -f "${REF}" -P 0.01 - | bgzip > freebayes."${region}".tmp.vcf.gz
  tabix -f -p vcf freebayes."${region}".tmp.vcf.gz
}
export -f call_freebayes1

function call_freebayes() {
  pushd "${OUTPUT_DIR}"
  time parallel --eta --halt 2 --joblog "freebayes.log" --progress \
    call_freebayes1 {} ::: "${REGIONS[@]}"
  bcftools concat -a freebayes.*.tmp.vcf.gz -O z -o freebayes.raw.vcf.gz
  rm -f freebayes.*.tmp.vcf.gz*
  tabix -f -p vcf freebayes.raw.vcf.gz
  "${RUN_FILTERING}" -o freebayes freebayes.raw.vcf.gz
  rm -f freebayes.anno.gz
  popd
}


# ------------------------------------------------------------------------------
#
# samtools
#
# ------------------------------------------------------------------------------

function call_samtools1() {
  region=$1
  samtools view -u "${BAM}" "${region}" | samtools mpileup -ugf "${REF}" - \
   | bcftools call -vmO z -o samtools."${region}".tmp.vcf.gz
  tabix -f -p vcf samtools."${region}".tmp.vcf.gz
}
export -f call_samtools1

function call_samtools() {
  # Make the calls
  pushd "${OUTPUT_DIR}"
  time parallel --eta --halt 2 --joblog "samtools.log" --progress \
    call_samtools1 {} ::: "${REGIONS[@]}"
  bcftools concat -a samtools.*.tmp.vcf.gz -O z -o samtools.raw.vcf.gz
  rm samtools.*.tmp.vcf.gz*
  tabix -f -p vcf samtools.raw.vcf.gz
  "${RUN_FILTERING}" -o samtools samtools.raw.vcf.gz
  rm -f samtools.anno.gz
  popd
}


# ------------------------------------------------------------------------------
#
# strelka2
#
# ------------------------------------------------------------------------------

function call_strelka() {
  rm -f "${OUTPUT_DIR}/strelka/runWorkflow.py"
  /usr/local/bin/configureStrelkaGermlineWorkflow.py \
  --bam "${BAM}" \
  --referenceFasta "${REF}" \
  --runDir "${OUTPUT_DIR}/strelka/"
  # --callRegions "${BED}"
  time "${OUTPUT_DIR}/strelka/runWorkflow.py" -m local -j 60

  mv "${OUTPUT_DIR}/strelka/results/variants/variants.vcf.gz" "${OUTPUT_DIR}/strelka.vcf.gz"
  mv "${OUTPUT_DIR}/strelka/results/variants/variants.vcf.gz.tbi" "${OUTPUT_DIR}/strelka.vcf.gz.tbi"
  # Remove the giant gVCF files and the scartch workspace.
  rm -f "${OUTPUT_DIR}/strelka/results/variants/genome*.vcf.gz*"
  rm -rf "${OUTPUT_DIR}/strelka/workspace"
}


function run_happy() {
  name=${1}
  vcf=${2}

  /usr/local/bin/happy/bin/hap.py \
    "${TRUTH_VCF}" \
    "${vcf}" \
    --preprocess-truth \
    -f "${TRUTH_BED}" \
    -r "${REF}" \
    -o "${OUTPUT_DIR}/${name}.happy.output" \
    ${HAPPY_EVAL_REGION_ARG}
}


# ------------------------------------------------------------------------------
#
# GATK3
#
# ------------------------------------------------------------------------------

function call_gatk3_hc1() {
  region=$1
  java -jar "${GATK3_JAR}" \
      -T HaplotypeCaller \
      -R "${REF}" \
      -I "${BAM}" \
      -L "${region}" \
      -o "${OUTPUT_DIR}/gatk3.raw.${region}.tmp.vcf"
  bgzip gatk3.raw.${region}.tmp.vcf
  tabix -f -p vcf gatk3.raw.${region}.tmp.vcf.gz
}
export -f call_gatk3_hc1

function call_gatk3() {
  pushd "${OUTPUT_DIR}"

  # Make calls with the HaplotypeCaller
  time parallel --eta --halt 2 --joblog "call_gatk3_hc.log" --res "${OUTPUT_DIR}/gatk3_hc.logs" --progress \
    call_gatk3_hc1 {} ::: "${REGIONS[@]}"

  bcftools concat -a gatk3.raw.*.tmp.vcf.gz -O z -o gatk3-raw.vcf.gz
  tabix -f -p vcf gatk3-raw.vcf.gz
  rm -f gatk3.*.tmp.vcf*

  "${RUN_FILTERING}" -o gatk3 gatk3-raw.vcf.gz
  rm -f gatk3.anno.gz

  regions_arg=$(printf -- "-L %s " "${REGIONS[@]}")

  echo "====== VQSR SNPs"
  java -jar "${GATK3_JAR}" \
      -T VariantRecalibrator \
      -R "${REF}" \
      -input gatk3-raw.vcf.gz \
      ${regions_arg[@]} \
      -resource:hapmap,known=false,training=true,truth=true,prior=15.0 "${HAPMAP}" \
      -resource:omni,known=false,training=true,truth=true,prior=12.0 "${OMNI}" \
      -resource:1000G,known=false,training=true,truth=false,prior=10.0 "${K1G}" \
      -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 "${DBSNP}" \
      -an DP \
      -an QD \
      -an FS \
      -an SOR \
      -an MQ \
      -an MQRankSum \
      -an ReadPosRankSum \
      -mode SNP \
      -nt 4 \
      -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 \
      -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 \
      -tranche 97.0 -tranche 90.0 \
      -allPoly \
      -max_attempts 3 \
      -recalFile ${OUTPUT_DIR}/gatk3.recalibrate_SNP.recal \
      -tranchesFile ${OUTPUT_DIR}/gatk3.recalibrate_SNP.tranches

  java -jar "${GATK3_JAR}" \
      -T ApplyRecalibration \
      -R "${REF}" \
      -input gatk3-raw.vcf.gz \
      ${regions_arg[@]} \
      -mode SNP \
      --ts_filter_level 99.9 \
      -recalFile ${OUTPUT_DIR}/gatk3.recalibrate_SNP.recal \
      -tranchesFile ${OUTPUT_DIR}/gatk3.recalibrate_SNP.tranches \
      -o ${OUTPUT_DIR}/gatk3.recalibrated_snps_raw_indels.vcf

  echo "====== VQSR Indels"
  java -jar "${GATK3_JAR}" \
      -T VariantRecalibrator \
      -R "${REF}" \
      -input "${OUTPUT_DIR}/gatk3.recalibrated_snps_raw_indels.vcf" \
      ${regions_arg[@]} \
      -resource:mills,known=false,training=true,truth=true,prior=12.0 "${MILLS}" \
      -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 "${DBSNP}" \
      -an QD \
      -an DP \
      -an FS \
      -an SOR \
      -an MQRankSum \
      -an ReadPosRankSum \
      -mode INDEL \
      -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 \
      -nt 4 \
      -allPoly \
      -max_attempts 3 \
      --maxGaussians 4 \
      -recalFile "${OUTPUT_DIR}/gatk3.recalibrate_INDEL.recal" \
      -tranchesFile "${OUTPUT_DIR}/gatk3.recalibrate_INDEL.tranches"

  java -jar "${GATK3_JAR}" \
      -T ApplyRecalibration \
      -R "${REF}" \
      -input "${OUTPUT_DIR}/gatk3.recalibrated_snps_raw_indels.vcf" \
      ${regions_arg[@]} \
      -mode INDEL \
      --ts_filter_level 99.0 \
      -recalFile "${OUTPUT_DIR}/gatk3.recalibrate_INDEL.recal" \
      -tranchesFile "${OUTPUT_DIR}/gatk3.recalibrate_INDEL.tranches" \
      -o "${OUTPUT_DIR}/gatk3-vqsr.vcf"

  # Cleanup after GATK:
  rm -f gatk3.recalibrated_snps_raw_indels.vcf*

  bgzip -f gatk3-vqsr.vcf
  tabix -f -p vcf gatk3-vqsr.vcf.gz
}


################################################################################
#
# main
#
################################################################################

function main() {
  mkdir -p "${DATA_DIR}"
  mkdir -p "${OUTPUT_DIR}"
  mkdir -p "${LOG_DIR}"

  for arg in "$@"; do
    if [[ "${arg}" == "setup_software" ]]; then
      echo "setting up software"

      rm -rf "${BUILD_DIR}"
      mkdir -p "${BUILD_DIR}"
      pushd "${BUILD_DIR}"

      setup_common
      setup_samtools
      setup_htslib
      setup_bcftools
      setup_chmeval
      setup_bedtools
      setup_freebayes
      setup_16gt
      setup_strelka
      setup_picard
      setup_gatk3
      setup_happy

      popd
      rm -rf "${BUILD_DIR}"

    elif [[ "${arg}" == "setup_data" ]]; then
      echo "setting up data"
      prepare_wgs_sample
      prepare_giab
      prepare_ref
      fetch_deepvariant_historical_callsets
      fetch_gatk_resource_bundle
      fetch_dbsnp
      fetch_chm1_chm13
      soap_index_fasta
    elif [[ "${arg}" == "call_freebayes" ]]; then
      call_freebayes
    elif [[ "${arg}" == "call_samtools" ]]; then
      call_samtools
    elif [[ "${arg}" == "call_strelka" ]]; then
      call_strelka
    elif [[ "${arg}" == "call_16gt" ]]; then
      call_16gt
    elif [[ "${arg}" == "call_gatk3" ]]; then
      call_gatk3
    elif [[ "${arg}" == "call_all" ]]; then
      echo "Making all callsets"
      call_samtools
      call_gatk3
      call_freebayes
      call_strelka
      call_16gt
    elif [[ "${arg}" == "evaluate" ]]; then
      echo "Evaluating callsets"
      run_happy "deepvariant-0.4" "${DATA_DIR}/deepvariant-r0.4.vcf.gz"
      run_happy "deepvariant_pfda" "${DATA_DIR}/deepvariant-pfda.vcf.gz"
      run_happy "samtools-raw" "${OUTPUT_DIR}/samtools.raw.vcf.gz"
      run_happy "samtools-filtered" "${OUTPUT_DIR}/samtools.flt.vcf.gz"
      run_happy "gatk3-raw" "${OUTPUT_DIR}/gatk3-raw.vcf.gz"
      run_happy "gatk3-vqsr" "${OUTPUT_DIR}/gatk3-vqsr.vcf.gz"
      run_happy "gatk3-filtered" "${OUTPUT_DIR}/gatk3.flt.vcf.gz"
      run_happy "freebayes-raw" "${OUTPUT_DIR}/freebayes.raw.vcf.gz"
      run_happy "freebayes-filtered" "${OUTPUT_DIR}/freebayes.flt.vcf.gz"
      run_happy "strelka" "${OUTPUT_DIR}/strelka.vcf.gz"
      run_happy "16gt" ${OUTPUT_DIR}/16gt.filtered.vcf.gz
    fi
  done
}

main $@